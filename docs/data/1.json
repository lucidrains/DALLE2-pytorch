{
    "100": {
        "file_id": 3,
        "content": "| ------ | -------- | ------- | ----------- |\n| `unets` | Yes | N/A | A list of unets, using the configuration above |\n| `image_sizes` | Yes | N/A | The resolution of the image after each upsampling step. The length of this array should be the number of unets defined. |\n| `image_size` | Yes | N/A | Not used. Can be any number. |\n| `timesteps` | No | `1000` | The number of diffusion timesteps used for generation. |\n| `loss_type` | No | `l2` | The loss function. Options are `l1`, `huber`, or `l2`. |\n| `beta_schedule` | No | `cosine` | The noising schedule. Options are `cosine`, `linear`, `quadratic`, `jsd`, or `sigmoid`. |\n| `learned_variance` | No | `True` | Whether to learn the variance. |\n| `clip` | No | `None` | The clip model to use if embeddings are being generated on the fly. Takes keys `make` and `model` with defaults `openai` and `ViT-L/14`. |\nAny parameter from the `Decoder` constructor can also be given here.\n**<ins>Data</ins>:**\nSettings for creation of the dataloaders.\n| Option | Required | Default | Description |",
        "type": "code",
        "location": "/configs/README.md:25-40"
    },
    "101": {
        "file_id": 3,
        "content": "This code appears to be defining the configuration for a machine learning model, specifically one using U-Nets. The configuration includes options for the number of unets, image resolution, timesteps, loss function type, noise schedule, and learned variance. Additionally, there are settings for creating dataloaders for the model's data. The code also notes that any parameter from the `Decoder` constructor can be included in this configuration.",
        "type": "comment"
    },
    "102": {
        "file_id": 3,
        "content": "| ------ | -------- | ------- | ----------- |\n| `webdataset_base_url` | Yes | N/A | The url of a shard in the webdataset with the shard replaced with `{}`[^1]. |\n| `img_embeddings_url` | No | `None` | The url of the folder containing image embeddings shards. Not required if embeddings are in webdataset or clip is being used. |\n| `text_embeddings_url` | No | `None` | The url of the folder containing text embeddings shards. Not required if embeddings are in webdataset or clip is being used. |\n| `num_workers` | No | `4` | The number of workers used in the dataloader. |\n| `batch_size` | No | `64` | The batch size. |\n| `start_shard` | No | `0` | Defines the start of the shard range the dataset will recall. |\n| `end_shard` | No | `9999999` | Defines the end of the shard range the dataset will recall. |\n| `shard_width` | No | `6` | Defines the width of one webdataset shard number[^2]. |\n| `index_width` | No | `4` | Defines the width of the index of a file inside a shard[^3]. |\n| `splits` | No | `{ \"tra",
        "type": "code",
        "location": "/configs/README.md:41-51"
    },
    "103": {
        "file_id": 3,
        "content": "This code defines various configuration options for a dataloader, including webdataset and embeddings urls, worker numbers, batch size, shard range, and file indexing. The config allows flexibility in handling different types of datasets, with optional embeddings or use of the webdataset library.",
        "type": "comment"
    },
    "104": {
        "file_id": 3,
        "content": "in\": 0.75, \"val\": 0.15, \"test\": 0.1 }` | Defines the proportion of shards that will be allocated to the training, validation, and testing datasets. |\n| `shuffle_train` | No | `True` | Whether to shuffle the shards of the training dataset. |\n| `resample_train` | No | `False` | If true, shards will be randomly sampled with replacement from the datasets making the epoch length infinite if a limit is not set. Cannot be enabled if `shuffle_train` is enabled. |\n| `preprocessing` | No | `{ \"ToTensor\": True }` | Defines preprocessing applied to images from the datasets. |\n[^1]: If your shard files have the paths `protocol://path/to/shard/00104.tar`, then the base url would be `protocol://path/to/shard/{}.tar`. If you are using a protocol like `s3`, you need to pipe the tars. For example `pipe:s3cmd get s3://bucket/path/{}.tar -`.\n[^2]: This refers to the string length of the shard number for your webdataset shards. For instance, if your webdataset shard has the filename `00104.tar`, your shard length is 5.",
        "type": "code",
        "location": "/configs/README.md:51-58"
    },
    "105": {
        "file_id": 3,
        "content": "This code defines the proportion of shards allocated to training, validation, and testing datasets as well as whether to shuffle training dataset, preprocessing applied to images from datasets, and details for downloading shard files. It also provides information on how to use protocols like `s3` and calculating the shard length based on filename.",
        "type": "comment"
    },
    "106": {
        "file_id": 3,
        "content": "[^3]: Inside the webdataset `tar`, you have files named something like `001045945.jpg`. 5 of these characters refer to the shard, and 4 refer to the index of the file in the webdataset (shard is `001041` and index is `5945`). The `index_width` in this case is 4.\n**<ins>Train</ins>:**\nSettings for controlling the training hyperparameters.\n| Option | Required | Default | Description |\n| ------ | -------- | ------- | ----------- |\n| `epochs` | No | `20` | The number of epochs in the training run. |\n| `lr` | No | `1e-4` | The learning rate. |\n| `wd` | No | `0.01` | The weight decay. |\n| `max_grad_norm`| No | `0.5` | The grad norm clipping. |\n| `save_every_n_samples` | No | `100000` | Samples will be generated and a checkpoint will be saved every `save_every_n_samples` samples. |\n| `cond_scale` | No | `1.0` | Conditioning scale to use for sampling. Can also be an array of values, one for each unet. |\n| `device` | No | `cuda:0` | The device to train on. |\n| `epoch_samples` | No | `None` | Limits the num",
        "type": "code",
        "location": "/configs/README.md:60-74"
    },
    "107": {
        "file_id": 3,
        "content": "The code provides settings for controlling training hyperparameters, such as the number of epochs, learning rate, weight decay, and grad norm clipping. It also allows saving checkpoints at specific intervals and specifying the device to train on. The conditioning scale can be customized for each unet if desired.",
        "type": "comment"
    },
    "108": {
        "file_id": 3,
        "content": "ber of samples iterated through in each epoch. This must be set if resampling. None means no limit. |\n| `validation_samples` | No | `None` | The number of samples to use for validation. None mean the entire validation set. |\n| `use_ema` | No | `True` | Whether to use exponential moving average models for sampling. |\n| `ema_beta` | No | `0.99` | The ema coefficient. |\n| `unet_training_mask` | No | `None` | A boolean array of the same length as the number of unets. If false, the unet is frozen. A value of `None` trains all unets. |\n**<ins>Evaluate</ins>:**\nDefines which evaluation metrics will be used to test the model.\nEach metric can be enabled by setting its configuration. The configuration keys for each metric are defined by the torchmetrics constructors which will be linked.\n| Option | Required | Default | Description |\n| ------ | -------- | ------- | ----------- |\n| `n_evaluation_samples` | No | `1000` | The number of samples to generate to test the model. |\n| `FID` | No | `None` | Setting to",
        "type": "code",
        "location": "/configs/README.md:74-87"
    },
    "109": {
        "file_id": 3,
        "content": "The code snippet defines configurations for training a DALLE2 model in PyTorch. It includes settings such as the number of samples iterated through in each epoch, number of validation samples, whether to use exponential moving average models for sampling, and the ema coefficient. Additionally, it allows defining which evaluation metrics will be used to test the model by setting their configurations using torchmetrics constructors. The number of samples generated to test the model is also specified.",
        "type": "comment"
    },
    "110": {
        "file_id": 3,
        "content": " an object enables the [Frechet Inception Distance](https://torchmetrics.readthedocs.io/en/stable/image/frechet_inception_distance.html) metric. \n| `IS` | No | `None` | Setting to an object enables the [Inception Score](https://torchmetrics.readthedocs.io/en/stable/image/inception_score.html) metric.\n| `KID` | No | `None` | Setting to an object enables the [Kernel Inception Distance](https://torchmetrics.readthedocs.io/en/stable/image/kernel_inception_distance.html) metric. |\n| `LPIPS` | No | `None` | Setting to an object enables the [Learned Perceptual Image Patch Similarity](https://torchmetrics.readthedocs.io/en/stable/image/learned_perceptual_image_patch_similarity.html) metric. |\n**<ins>Tracker</ins>:**\nSelects how the experiment will be tracked.\n| Option | Required | Default | Description |\n| ------ | -------- | ------- | ----------- |\n| `data_path` | No | `./.tracker-data` | The path to the folder where temporary tracker data will be saved. |\n| `overwrite_data_path` | No | `False` | If true, the data path will be overwritten. Otherwise, you need to delete it yourself. |",
        "type": "code",
        "location": "/configs/README.md:87-98"
    },
    "111": {
        "file_id": 3,
        "content": "This code snippet is from the configs/README.md file of the DALLE2-pytorch project. It describes how to enable different image metrics and set up experiment tracking. The available metrics are Frechet Inception Distance, Inception Score, Kernel Inception Distance, and Learned Perceptual Image Patch Similarity. The tracker can be configured with data_path and overwrite_data_path options for storing temporary tracking data.",
        "type": "comment"
    },
    "112": {
        "file_id": 3,
        "content": "| `log` | Yes | N/A | Logging configuration. |\n| `load` | No | `None` | Checkpoint loading configuration. |\n| `save` | Yes | N/A | Checkpoint/Model saving configuration. |\nTracking is split up into three sections:\n* Log: Where to save run metadata and image output. Options are `console` or `wandb`.\n* Load: Where to load a checkpoint from. Options are `local`, `url`, or `wandb`.\n* Save: Where to save a checkpoint to. Options are `local`, `huggingface`, or `wandb`.\n**Logging:**\nAll loggers have the following keys:\n| Option | Required | Default | Description |\n| ------ | -------- | ------- | ----------- |\n| `log_type` | Yes | N/A | The type of logger class to use. |\n| `resume` | No | `False` | For loggers that have the option to resume an old run, resume it using maually input parameters. |\n| `auto_resume` | No | `False` | If true, the logger will attempt to resume an old run using parameters from that previous run. |\nIf using `console` there is no further configuration than setting `log_type` to `console`.",
        "type": "code",
        "location": "/configs/README.md:99-116"
    },
    "113": {
        "file_id": 3,
        "content": "The code defines configuration settings for logging, loading checkpoints, and saving checkpoints in a DALLE2-pytorch application. The logging section allows specifying where to save run metadata and image output (options: console or wandb). Loading can be from local, URL, or Wandb sources. Saving can be done locally, on HuggingFace, or via Wandb. Loggers have options for resume and auto-resume functions. If using console logging, only the log_type needs to be set as console.",
        "type": "comment"
    },
    "114": {
        "file_id": 3,
        "content": "| Option | Required | Default | Description |\n| ------ | -------- | ------- | ----------- |\n| `log_type` | Yes | N/A | Must be `console`. |\nIf using `wandb`\n| Option | Required | Default | Description |\n| ------ | -------- | ------- | ----------- |\n| `log_type` | Yes | N/A | Must be `wandb`. |\n| `wandb_entity` | Yes | N/A | The wandb entity to log to. |\n| `wandb_project` | Yes | N/A | The wandb project save the run to. |\n| `wandb_run_name` | No | `None` | The wandb run name. |\n| `wandb_run_id` | No | `None` | The wandb run id. Used if resuming an old run. |\n**Loading:**\nAll loaders have the following keys:\n| Option | Required | Default | Description |\n| ------ | -------- | ------- | ----------- |\n| `load_from` | Yes | N/A | The type of loader class to use. |\n| `only_auto_resume` | No | `False` | If true, the loader will only load the model if the run is being auto resumed. |\nIf using `local`\n| Option | Required | Default | Description |\n| ------ | -------- | ------- | ----------- |\n| `load_from` | Yes | N/A | Must be `local`. |",
        "type": "code",
        "location": "/configs/README.md:117-141"
    },
    "115": {
        "file_id": 3,
        "content": "This code is defining the configuration options for logging and loading in a DALLE2-pytorch application. The user has to specify the log type (console or wandb) along with other required and optional parameters depending on the selected logger. The loaders have options to specify the loader class type (e.g., local) and whether to only auto resume if the run is being resumed.",
        "type": "comment"
    },
    "116": {
        "file_id": 3,
        "content": "| `file_path` | Yes | N/A | The path to the checkpoint file. |\nIf using `url`\n| Option | Required | Default | Description |\n| ------ | -------- | ------- | ----------- |\n| `load_from` | Yes | N/A | Must be `url`. |\n| `url` | Yes | N/A | The url of the checkpoint file. |\nIf using `wandb`\n| Option | Required | Default | Description |\n| ------ | -------- | ------- | ----------- |\n| `load_from` | Yes | N/A | Must be `wandb`. |\n| `wandb_run_path` | No | `None` | The wandb run path. If `None`, uses the run that is being resumed. |\n| `wandb_file_path` | Yes | N/A | The path to the checkpoint file in the W&B file system. |\n**Saving:**\nUnlike `log` and `load`, `save` may be an array of options so that you can save to different locations in a run.\nAll save locations have these configuration options\n| Option | Required | Default | Description |\n| ------ | -------- | ------- | ----------- |\n| `save_to` | Yes | N/A | Must be `local`, `huggingface`, or `wandb`. |\n| `save_latest_to` | No | `None` | Sets the relative path to save the latest model to. |",
        "type": "code",
        "location": "/configs/README.md:142-164"
    },
    "117": {
        "file_id": 3,
        "content": "The code defines the options for loading and saving checkpoint files. It supports loading from a file path, URL or WandB run, with each option having specific required configurations. Saving to different locations is also supported through options like local, huggingface, or wandb, with additional configuration possibilities.",
        "type": "comment"
    },
    "118": {
        "file_id": 3,
        "content": "| `save_best_to` | No | `None` | Sets the relative path to save the best model to every time the model has a lower validation loss than all previous models. |\n| `save_meta_to` | No | `None` | The path to save metadata files in. This includes the config files used to start the training. |\n| `save_type` | No | `checkpoint` | The type of save. `checkpoint` saves a checkpoint, `model` saves a model without any fluff (Saves with ema if ema is enabled). |\nIf using `local`\n| Option | Required | Default | Description |\n| ------ | -------- | ------- | ----------- |\n| `save_to` | Yes | N/A | Must be `local`. |\nIf using `huggingface`\n| Option | Required | Default | Description |\n| ------ | -------- | ------- | ----------- |\n| `save_to` | Yes | N/A | Must be `huggingface`. |\n| `huggingface_repo` | Yes | N/A | The huggingface repository to save to. |\n| `token_path` | No | `None` | If logging in with the huggingface cli is not possible, point to a token file instead. |\nIf using `wandb`\n| Option | Required | Default | Description |",
        "type": "code",
        "location": "/configs/README.md:165-182"
    },
    "119": {
        "file_id": 3,
        "content": "This code sets options for saving models and metadata during training. It allows saving to local, huggingface or wandb storage with specific requirements for each option. The save type can be checkpoint or model, and there are additional options like saving best models, token file path, and repository paths.",
        "type": "comment"
    },
    "120": {
        "file_id": 3,
        "content": "| ------ | -------- | ------- | ----------- |\n| `save_to` | Yes | N/A | Must be `wandb`. |\n| `wandb_run_path` | No | `None` | The wandb run path. If `None`, uses the current run. You will almost always want this to be `None`. |",
        "type": "code",
        "location": "/configs/README.md:183-185"
    },
    "121": {
        "file_id": 3,
        "content": "The code defines configuration options for saving and interacting with the Weights & Biases (Wandb) run path. If `save_to` is set to `wandb`, the `wandb_run_path` should be `None`. Otherwise, it defaults to the current run if `wandb_run_path` is set to `None`.",
        "type": "comment"
    },
    "122": {
        "file_id": 4,
        "content": "/dalle2_pytorch/__init__.py",
        "type": "filepath"
    },
    "123": {
        "file_id": 4,
        "content": "This code is importing modules from the DALLE2-pytorch library, which includes the main DALLE2 class, diffusion prior network, unet, decoder, clip adapters, trainer for the decoder and diffusion prior, and VQGanVAE. The x_clip module also appears to be imported, but its purpose is not explicitly described in this chunk of code.",
        "type": "summary"
    },
    "124": {
        "file_id": 4,
        "content": "from dalle2_pytorch.version import __version__\nfrom dalle2_pytorch.dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, Unet, Decoder\nfrom dalle2_pytorch.dalle2_pytorch import OpenAIClipAdapter, OpenClipAdapter\nfrom dalle2_pytorch.trainer import DecoderTrainer, DiffusionPriorTrainer\nfrom dalle2_pytorch.vqgan_vae import VQGanVAE\nfrom x_clip import CLIP",
        "type": "code",
        "location": "/dalle2_pytorch/__init__.py:1-7"
    },
    "125": {
        "file_id": 4,
        "content": "This code is importing modules from the DALLE2-pytorch library, which includes the main DALLE2 class, diffusion prior network, unet, decoder, clip adapters, trainer for the decoder and diffusion prior, and VQGanVAE. The x_clip module also appears to be imported, but its purpose is not explicitly described in this chunk of code.",
        "type": "comment"
    },
    "126": {
        "file_id": 5,
        "content": "/dalle2_pytorch/cli.py",
        "type": "filepath"
    },
    "127": {
        "file_id": 5,
        "content": "This code imports libraries, defines functions, and parses command-line arguments for model path, conditioning scale, and input text. It loads a DALL-E2 model, generates an image based on the input text, saves it in PIL format, and returns the saved image.",
        "type": "summary"
    },
    "128": {
        "file_id": 5,
        "content": "import click\nimport torch\nimport torchvision.transforms as T\nfrom functools import reduce\nfrom pathlib import Path\nfrom dalle2_pytorch import DALLE2, Decoder, DiffusionPrior\ndef safeget(dictionary, keys, default = None):\n    return reduce(lambda d, key: d.get(key, default) if isinstance(d, dict) else default, keys.split('.'), dictionary)\ndef simple_slugify(text, max_length = 255):\n    return text.replace(\"-\", \"_\").replace(\",\", \"\").replace(\" \", \"_\").replace(\"|\", \"--\").strip('-_')[:max_length]\ndef get_pkg_version():\n    from pkg_resources import get_distribution\n    return get_distribution('dalle2_pytorch').version\ndef main():\n    pass\n@click.command()\n@click.option('--model', default = './dalle2.pt', help = 'path to trained DALL-E2 model')\n@click.option('--cond_scale', default = 2, help = 'conditioning scale (classifier free guidance) in decoder')\n@click.argument('text')\ndef dream(\n    model,\n    cond_scale,\n    text\n):\n    model_path = Path(model)\n    full_model_path = str(model_path.resolve())\n    assert model_path.exists(), f'model not found at {full_model_path}'",
        "type": "code",
        "location": "/dalle2_pytorch/cli.py:1-33"
    },
    "129": {
        "file_id": 5,
        "content": "This code imports necessary libraries, defines some utility functions and a main function. It also includes a command-line argument parser with options for model path, conditioning scale, and the text input. The assert statement ensures that the specified model file exists before proceeding.",
        "type": "comment"
    },
    "130": {
        "file_id": 5,
        "content": "    loaded = torch.load(str(model_path))\n    version = safeget(loaded, 'version')\n    print(f'loading DALL-E2 from {full_model_path}, saved at version {version} - current package version is {get_pkg_version()}')\n    prior_init_params = safeget(loaded, 'init_params.prior')\n    decoder_init_params = safeget(loaded, 'init_params.decoder')\n    model_params = safeget(loaded, 'model_params')\n    prior = DiffusionPrior(**prior_init_params)\n    decoder = Decoder(**decoder_init_params)\n    dalle2 = DALLE2(prior, decoder)\n    dalle2.load_state_dict(model_params)\n    image = dalle2(text, cond_scale = cond_scale)\n    pil_image = T.ToPILImage()(image)\n    return pil_image.save(f'./{simple_slugify(text)}.png')",
        "type": "code",
        "location": "/dalle2_pytorch/cli.py:34-52"
    },
    "131": {
        "file_id": 5,
        "content": "This code loads a saved DALL-E2 model from a specified path, checks the version, initializes the prior and decoder components, recreates the model using these components, loads its parameters, generates an image based on input text, converts it to PIL format, saves it with a file name derived from the input text, and returns the saved image.",
        "type": "comment"
    },
    "132": {
        "file_id": 6,
        "content": "/dalle2_pytorch/dalle2_pytorch.py",
        "type": "filepath"
    },
    "133": {
        "file_id": 6,
        "content": "The code uses VQGAN-VAE, CLIP, and CoCa libraries for image generation, and includes helper functions, PyTorch CLIP model, neural networks, DALL-E 2 architecture, self-attention layers with normalization and dropout regularization. It initializes efficient DALL-E 2 and Imagen models, utilizes diffusion models for denoising and inpainting images, and incorporates conditional sampling from DALLE2-pytorch model for low-resolution image generation.",
        "type": "summary"
    },
    "134": {
        "file_id": 6,
        "content": "import math\nimport random\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager\nfrom collections import namedtuple\nfrom pathlib import Path\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\nfrom torch import nn, einsum\nimport torchvision.transforms as T\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange\nfrom kornia.filters import gaussian_blur2d\nimport kornia.augmentation as K\nfrom dalle2_pytorch.tokenizer import tokenizer\nfrom dalle2_pytorch.vqgan_vae import NullVQGanVAE, VQGanVAE\nfrom resize_right import resize\n# rotary embeddings\nfrom rotary_embedding_torch import RotaryEmbedding\n# use x-clip\nfrom x_clip import CLIP\nfrom coca_pytorch import CoCa\n# constants\nNAT = 1. / math.log(2.)\nUnetOutput = namedtuple('UnetOutput', ['pred', 'var_interp_frac_unnormalized'])\n# helper functions\ndef exists(val):\n    return val is not None\ndef identity(t, *args, **kwargs):\n    return t\ndef first(arr, d = None):",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1-49"
    },
    "135": {
        "file_id": 6,
        "content": "This code imports various libraries and defines functions for data processing, including image resizing, Gaussian blurring, and rotary embeddings. It also utilizes the VQGAN-VAE, CLIP model, and CoCa. The code contains namedtuples, helper functions, and constants relevant to the tasks of image generation and language modeling.",
        "type": "comment"
    },
    "136": {
        "file_id": 6,
        "content": "    if len(arr) == 0:\n        return d\n    return arr[0]\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x, *args, **kwargs):\n        if not exists(x):\n            return x\n        return fn(x, *args, **kwargs)\n    return inner\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\ndef cast_tuple(val, length = None, validate = True):\n    if isinstance(val, list):\n        val = tuple(val)\n    out = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n    if exists(length) and validate:\n        assert len(out) == length\n    return out\ndef module_device(module):\n    if isinstance(module, nn.Identity):\n        return 'cpu' # It doesn't matter\n    return next(module.parameters()).device\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n@contextmanager\ndef null_context(*args, **kwargs):\n    yield\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:50-96"
    },
    "137": {
        "file_id": 6,
        "content": "Function 'if len(arr) == 0: return d' checks if the array is empty and returns the value 'd' if it is.\n'maybe(fn)' function creates a decorator that checks if the input exists, returning it if it does not.\n'default(val, d)' function returns the provided value 'val' if it exists; otherwise, it returns the default value 'd'.\n'cast_tuple(val, length=None, validate=True)' casts its argument to a tuple and optionally checks its length.\n'module_device(module)' retrieves the device of the module, defaulting to CPU for certain types like nn.Identity.\n'zero_init_(m)' initializes the weights and biases of the given module 'm' with zeros.\n'null_context(*args, **kwargs)' is a context manager that does nothing.\n'eval_decorator(fn)' wraps a function to evaluate the model before executing it.",
        "type": "comment"
    },
    "138": {
        "file_id": 6,
        "content": "        model.train(was_training)\n        return out\n    return inner\ndef is_float_dtype(dtype):\n    return any([dtype == float_dtype for float_dtype in (torch.float64, torch.float32, torch.float16, torch.bfloat16)])\ndef is_list_str(x):\n    if not isinstance(x, (list, tuple)):\n        return False\n    return all([type(el) == str for el in x])\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n# checkpointing helper function\ndef make_checkpointable(fn, **kwargs):\n    if isinstance(fn, nn.ModuleList):\n        return [maybe(make_checkpointable)(el, **kwargs) for el in fn]\n    condition = kwargs.pop('condition', None)\n    if exists(condition) and not condition(fn):\n        return fn\n    @wraps(fn)\n    def inner(*args):\n        input_needs_grad = any([isinstance(el, torch.Tensor) and el.requires_grad for el in args])\n        if not input_needs_grad:\n            return fn(*args)\n        return checkpoint(fn, *args)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:97-133"
    },
    "139": {
        "file_id": 6,
        "content": "This code defines several helper functions for processing lists of strings, padding tuples to a specific length, and creating checkpointable versions of Python functions. It also includes a function to determine if a given dtype is a floating point type, and a conditional wrapper for creating a checkpointable version of a function or module list.",
        "type": "comment"
    },
    "140": {
        "file_id": 6,
        "content": "    return inner\n# for controlling freezing of CLIP\ndef set_module_requires_grad_(module, requires_grad):\n    for param in module.parameters():\n        param.requires_grad = requires_grad\ndef freeze_all_layers_(module):\n    set_module_requires_grad_(module, False)\ndef unfreeze_all_layers_(module):\n    set_module_requires_grad_(module, True)\ndef freeze_model_and_make_eval_(model):\n    model.eval()\n    freeze_all_layers_(model)\n# tensor helpers\ndef log(t, eps = 1e-12):\n    return torch.log(t.clamp(min = eps))\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    nearest = False,\n    **kwargs\n):\n    orig_image_size = image.shape[-1]\n    if orig_image_size == target_image_size:\n        return image\n    if not nearest:\n        scale_factors = target_image_size / orig_image_size\n        out = resize(image, scale_factors = scale_factors, **kwargs)\n    else:\n        out = F.interpolate(image, target_image_size, mode = 'nearest')\n    if exists(clamp_range):",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:135-179"
    },
    "141": {
        "file_id": 6,
        "content": "The code defines functions for controlling the gradient flow in a module, freezing all layers in a model, and making it evaluate only. It also includes helper functions to log a tensor, normalize a tensor using L2 norm, and resize an image to the specified size with optional interpolation method.",
        "type": "comment"
    },
    "142": {
        "file_id": 6,
        "content": "        out = out.clamp(*clamp_range)\n    return out\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n# but CLIP may otherwise\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n# clip related adapters\nEmbeddedText = namedtuple('EmbedTextReturn', ['text_embed', 'text_encodings'])\nEmbeddedImage = namedtuple('EmbedImageReturn', ['image_embed', 'image_encodings'])\nclass BaseClipAdapter(nn.Module):\n    def __init__(self, clip, **kwargs):\n        super().__init__()\n        self.clip = clip\n        self.overrides = kwargs\n    def validate_and_resize_image(self, image):\n        image_size = image.shape[-1]\n        assert image_size >= self.image_size, f'you are passing in an image of size {image_size} but CLIP requires the image size to be at least {self.image_size}'\n        return resize_image_to(image, self.image_size)\n    @property\n    def dim_latent(self):\n        raise NotImplementedError\n    @property",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:180-214"
    },
    "143": {
        "file_id": 6,
        "content": "This code defines a function for normalizing an image to the range of -1 to 1, and another for unnormalizing it back to the 0 to 1 range. It also includes a namedtuple for returning embedded text and image data along with their encodings. The code further defines a base class for clip adapters that takes a CLIP model as an argument and provides methods for validating and resizing images to match CLIP's requirements.",
        "type": "comment"
    },
    "144": {
        "file_id": 6,
        "content": "    def image_size(self):\n        raise NotImplementedError\n    @property\n    def image_channels(self):\n        raise NotImplementedError\n    @property\n    def max_text_len(self):\n        raise NotImplementedError\n    def embed_text(self, text):\n        raise NotImplementedError\n    def embed_image(self, image):\n        raise NotImplementedError\nclass XClipAdapter(BaseClipAdapter):\n    @property\n    def dim_latent(self):\n        return self.clip.dim_latent\n    @property\n    def image_size(self):\n        return self.clip.image_size\n    @property\n    def image_channels(self):\n        return self.clip.image_channels\n    @property\n    def max_text_len(self):\n        return self.clip.text_seq_len\n    @torch.no_grad()\n    def embed_text(self, text):\n        text = text[..., :self.max_text_len]\n        text_mask = text != 0\n        encoder_output = self.clip.text_transformer(text)\n        encoder_output_is_cls = encoder_output.ndim == 3\n        text_cls, text_encodings = (encoder_output[:, 0], encoder_output[:, 1:]) if encoder_output_is_cls else (encoder_output, None)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:215-257"
    },
    "145": {
        "file_id": 6,
        "content": "This code defines a base class `BaseClipAdapter` with four methods that must be implemented by derived classes. The `XClipAdapter` class inherits from `BaseClipAdapter` and provides implementations for the properties of the underlying `clip` object, which is an instance of some clip model. The `embed_text` method takes a text input, truncates it to fit the maximum text length defined by `max_text_len`, applies a text transformer from the `clip` object, and returns the embeddings.",
        "type": "comment"
    },
    "146": {
        "file_id": 6,
        "content": "        text_embed = self.clip.to_text_latent(text_cls)\n        if exists(text_encodings):\n            text_encodings = text_encodings.masked_fill(~text_mask[..., None], 0.)\n        return EmbeddedText(l2norm(text_embed), text_encodings)\n    @torch.no_grad()\n    def embed_image(self, image):\n        image = self.validate_and_resize_image(image)\n        encoder_output = self.clip.visual_transformer(image)\n        image_cls, image_encodings = encoder_output[:, 0], encoder_output[:, 1:]\n        image_embed = self.clip.to_visual_latent(image_cls)\n        return EmbeddedImage(l2norm(image_embed), image_encodings)\nclass CoCaAdapter(BaseClipAdapter):\n    @property\n    def dim_latent(self):\n        return self.clip.dim\n    @property\n    def image_size(self):\n        assert 'image_size' in self.overrides\n        return self.overrides['image_size']\n    @property\n    def image_channels(self):\n        assert 'image_channels' in self.overrides\n        return self.overrides['image_channels']\n    @property\n    def max_text_len(self):",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:258-289"
    },
    "147": {
        "file_id": 6,
        "content": "This code snippet defines a class called CoCaAdapter, which is a base adapter for the DALL-E 2 PyTorch model. It contains methods to embed text and images, with optional overrides for image size and channels. The dim_latent property returns the dimension of the latent space, while max_text_len is used to set the maximum length for text inputs.",
        "type": "comment"
    },
    "148": {
        "file_id": 6,
        "content": "        assert 'max_text_len' in self.overrides\n        return self.overrides['max_text_len']\n    @torch.no_grad()\n    def embed_text(self, text):\n        text = text[..., :self.max_text_len]\n        text_mask = text != 0\n        text_embed, text_encodings = self.clip.embed_text(text)\n        text_encodings = text_encodings.masked_fill(~text_mask[..., None], 0.)\n        return EmbeddedText(text_embed, text_encodings)\n    @torch.no_grad()\n    def embed_image(self, image):\n        image = self.validate_and_resize_image(image)\n        image_embed, image_encodings = self.clip.embed_image(image)\n        return EmbeddedImage(image_embed, image_encodings)\nclass OpenAIClipAdapter(BaseClipAdapter):\n    def __init__(\n        self,\n        name = 'ViT-B/32'\n    ):\n        import clip\n        openai_clip, preprocess = clip.load(name)\n        super().__init__(openai_clip)\n        self.eos_id = 49407 # for handling 0 being also '!'\n        text_attention_final = self.find_layer('ln_final')\n        self.dim_latent_ = text_attention_final.weight.shape[0]",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:290-319"
    },
    "149": {
        "file_id": 6,
        "content": "This code is for a text-to-image model that uses CLIP as its base. It has functions to embed texts and images, with the ability to handle maximum text length. It initializes an OpenAIClipAdapter class using CLIP's 'ViT-B/32' model and finds the layer for text attention final output.",
        "type": "comment"
    },
    "150": {
        "file_id": 6,
        "content": "        self.handle = text_attention_final.register_forward_hook(self._hook)\n        self.clip_normalize = preprocess.transforms[-1]\n        self.cleared = False\n    def find_layer(self,  layer):\n        modules = dict([*self.clip.named_modules()])\n        return modules.get(layer, None)\n    def clear(self):\n        if self.cleared:\n            return\n        self.handle()\n    def _hook(self, _, inputs, outputs):\n        self.text_encodings = outputs\n    @property\n    def dim_latent(self):\n        return self.dim_latent_\n    @property\n    def image_size(self):\n        return self.clip.visual.input_resolution\n    @property\n    def image_channels(self):\n        return 3\n    @property\n    def max_text_len(self):\n        return self.clip.context_length\n    @torch.no_grad()\n    def embed_text(self, text):\n        text = text[..., :self.max_text_len]\n        is_eos_id = (text == self.eos_id)\n        text_mask_excluding_eos = is_eos_id.cumsum(dim = -1) == 0\n        text_mask = F.pad(text_mask_excluding_eos, (1, -1), value = True)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:320-360"
    },
    "151": {
        "file_id": 6,
        "content": "This code is part of a neural network model for text-to-image generation using PyTorch. It includes functions to handle text attention, clear the internal state, and embed input text. The class has properties such as `dim_latent`, `image_size`, `image_channels`, `max_text_len` which are used to define the network's structure and behavior.",
        "type": "comment"
    },
    "152": {
        "file_id": 6,
        "content": "        text_mask = text_mask & (text != 0)\n        assert not self.cleared\n        text_embed = self.clip.encode_text(text)\n        text_encodings = self.text_encodings\n        text_encodings = text_encodings.masked_fill(~text_mask[..., None], 0.)\n        del self.text_encodings\n        return EmbeddedText(l2norm(text_embed.float()), text_encodings.float())\n    @torch.no_grad()\n    def embed_image(self, image):\n        assert not self.cleared\n        image = self.validate_and_resize_image(image)\n        image = self.clip_normalize(image)\n        image_embed = self.clip.encode_image(image)\n        return EmbeddedImage(l2norm(image_embed.float()), None)\nclass OpenClipAdapter(BaseClipAdapter):\n    def __init__(\n        self,\n        name = 'ViT-B/32',\n        pretrained = 'laion400m_e32'\n    ):\n        import open_clip\n        clip, _, preprocess = open_clip.create_model_and_transforms(name, pretrained = pretrained)\n        super().__init__(clip)\n        self.eos_id = 49407\n        text_attention_final = self.find_layer('ln_final')",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:361-390"
    },
    "153": {
        "file_id": 6,
        "content": "Method to embed text using CLIP model by encoding the input text, applying a mask on text encodings, and returning EmbeddedText object with L2 normalized text embedding and float text encodings.",
        "type": "comment"
    },
    "154": {
        "file_id": 6,
        "content": "        self._dim_latent = text_attention_final.weight.shape[0]\n        self.handle = text_attention_final.register_forward_hook(self._hook)\n        self.clip_normalize = preprocess.transforms[-1]\n        self.cleared = False\n    def find_layer(self,  layer):\n        modules = dict([*self.clip.named_modules()])\n        return modules.get(layer, None)\n    def clear(self):\n        if self.cleared:\n            return\n        self.handle()\n    def _hook(self, _, inputs, outputs):\n        self.text_encodings = outputs\n    @property\n    def dim_latent(self):\n        return self._dim_latent\n    @property\n    def image_size(self):\n        image_size = self.clip.visual.image_size\n        if isinstance(image_size, tuple):\n            return max(image_size)\n        return image_size\n    @property\n    def image_channels(self):\n        return 3\n    @property\n    def max_text_len(self):\n        return self.clip.context_length\n    @torch.no_grad()\n    def embed_text(self, text):\n        text = text[..., :self.max_text_len]\n        is_eos_id = (text == self.eos_id)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:391-433"
    },
    "155": {
        "file_id": 6,
        "content": "The code represents a class that appears to be a part of a larger model. It has methods for embedding text, clearing internal state, finding layers in the network, and retrieving properties like latent dimension and maximum text length. The class relies on other components such as `preprocess`, `clip`, and `image_size`.",
        "type": "comment"
    },
    "156": {
        "file_id": 6,
        "content": "        text_mask_excluding_eos = is_eos_id.cumsum(dim = -1) == 0\n        text_mask = F.pad(text_mask_excluding_eos, (1, -1), value = True)\n        text_mask = text_mask & (text != 0)\n        assert not self.cleared\n        text_embed = self.clip.encode_text(text)\n        text_encodings = self.text_encodings\n        text_encodings = text_encodings.masked_fill(~text_mask[..., None], 0.)\n        del self.text_encodings\n        return EmbeddedText(l2norm(text_embed.float()), text_encodings.float())\n    @torch.no_grad()\n    def embed_image(self, image):\n        assert not self.cleared\n        image = self.validate_and_resize_image(image)\n        image = self.clip_normalize(image)\n        image_embed = self.clip.encode_image(image)\n        return EmbeddedImage(l2norm(image_embed.float()), None)\n# classifier free guidance functions\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:434-459"
    },
    "157": {
        "file_id": 6,
        "content": "This function takes in a text input and returns an EmbeddedText object containing the embedded text representation and a corresponding mask. It first creates a mask excluding the end of sentence (EOS) token, pads it, and applies the mask to the original mask. Then, it encodes the text using CLIP's encode_text function, and finally normalizes the resulting embeddings. The classifier free guidance functions return a probability mask based on the given probability value for a specific shape and device.",
        "type": "comment"
    },
    "158": {
        "file_id": 6,
        "content": "    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n# gaussian diffusion helper functions\ndef extract(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\ndef meanflat(x):\n    return x.mean(dim = tuple(range(1, len(x.shape))))\ndef normal_kl(mean1, logvar1, mean2, logvar2):\n    return 0.5 * (-1.0 + logvar2 - logvar1 + torch.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * torch.exp(-logvar2))\ndef approx_standard_normal_cdf(x):\n    return 0.5 * (1.0 + torch.tanh(((2.0 / math.pi) ** 0.5) * (x + 0.044715 * (x ** 3))))\ndef discretized_gaussian_log_likelihood(x, *, means, log_scales, thres = 0.999):\n    assert x.shape == means.shape == log_scales.shape\n    # attempting to correct nan gradients when learned variance is turned on\n    # in the setting of deepspeed fp16\n    eps = 1e-12 if x.dtype == torch.float32 else 1e-3\n    centered_x = x - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_x + 1. / 255.)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:460-488"
    },
    "159": {
        "file_id": 6,
        "content": "This code defines several helper functions used in the DALLE2-pytorch model. These functions are involved in tasks such as extracting values, calculating normal KL divergence, approximating the standard normal cumulative distribution function, and computing the discretized Gaussian log likelihood. The code also includes error handling for potential nan gradients when using deepspeed fp16.",
        "type": "comment"
    },
    "160": {
        "file_id": 6,
        "content": "    cdf_plus = approx_standard_normal_cdf(plus_in)\n    min_in = inv_stdv * (centered_x - 1. / 255.)\n    cdf_min = approx_standard_normal_cdf(min_in)\n    log_cdf_plus = log(cdf_plus, eps = eps)\n    log_one_minus_cdf_min = log(1. - cdf_min, eps = eps)\n    cdf_delta = cdf_plus - cdf_min\n    log_probs = torch.where(x < -thres,\n        log_cdf_plus,\n        torch.where(x > thres,\n            log_one_minus_cdf_min,\n            log(cdf_delta, eps = eps)))\n    return log_probs\ndef cosine_beta_schedule(timesteps, s = 0.008):\n    \"\"\"\n    cosine schedule\n    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n    \"\"\"\n    steps = timesteps + 1\n    x = torch.linspace(0, timesteps, steps, dtype = torch.float64)\n    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / first(alphas_cumprod)\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return torch.clip(betas, 0, 0.999)\ndef linear_beta_schedule(timesteps):\n    scale = 1000 / timesteps\n    beta_start = scale * 0.0001",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:489-519"
    },
    "161": {
        "file_id": 6,
        "content": "Function at line 488-518 calculates log probabilities for a given input x, using an adaptive quantile regression approach with a cosine or linear schedule. The cosine_beta_schedule function generates a sequence of beta values using a cosine schedule, and the linear_beta_schedule function generates a sequence of beta values linearly.",
        "type": "comment"
    },
    "162": {
        "file_id": 6,
        "content": "    beta_end = scale * 0.02\n    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\ndef quadratic_beta_schedule(timesteps):\n    scale = 1000 / timesteps\n    beta_start = scale * 0.0001\n    beta_end = scale * 0.02\n    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps, dtype = torch.float64) ** 2\ndef sigmoid_beta_schedule(timesteps):\n    scale = 1000 / timesteps\n    beta_start = scale * 0.0001\n    beta_end = scale * 0.02\n    betas = torch.linspace(-6, 6, timesteps, dtype = torch.float64)\n    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\nclass NoiseScheduler(nn.Module):\n    def __init__(self, *, beta_schedule, timesteps, loss_type, p2_loss_weight_gamma = 0., p2_loss_weight_k = 1):\n        super().__init__()\n        if beta_schedule == \"cosine\":\n            betas = cosine_beta_schedule(timesteps)\n        elif beta_schedule == \"linear\":\n            betas = linear_beta_schedule(timesteps)\n        elif beta_schedule == \"quadratic\":\n            betas = quadratic_beta_schedule(timesteps)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:520-548"
    },
    "163": {
        "file_id": 6,
        "content": "This code defines three beta scheduling functions (linear, quadratic, cosine) and a class for the NoiseScheduler. The scheduler initializes with a selected beta schedule and timesteps. The beta_schedule parameter determines which function to use for generating the betas, which represent noise scaling factors in the model's training process.",
        "type": "comment"
    },
    "164": {
        "file_id": 6,
        "content": "        elif beta_schedule == \"jsd\":\n            betas = 1.0 / torch.linspace(timesteps, 1, timesteps)\n        elif beta_schedule == \"sigmoid\":\n            betas = sigmoid_beta_schedule(timesteps)\n        else:\n            raise NotImplementedError()\n        alphas = 1. - betas\n        alphas_cumprod = torch.cumprod(alphas, axis = 0)\n        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n        # register buffer helper function to cast double back to float\n        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n        register_buffer('betas', betas)\n        register_buffer('alphas_cumprod', alphas_cumprod)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:549-580"
    },
    "165": {
        "file_id": 6,
        "content": "This code sets the beta schedule and alpha values based on user input, then selects a loss function according to the specified type. The code also registers buffer helper functions for 'betas' and 'alphas_cumprod'.",
        "type": "comment"
    },
    "166": {
        "file_id": 6,
        "content": "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        register_buffer('posterior_variance', posterior_variance)\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:581-601"
    },
    "167": {
        "file_id": 6,
        "content": "The code is registering various buffers for computations related to diffusion. It calculates the posterior variance and clips the log of the posterior variance to avoid numerical instability at the beginning of the diffusion chain.",
        "type": "comment"
    },
    "168": {
        "file_id": 6,
        "content": "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n        # p2 loss reweighting\n        self.has_p2_loss_reweighting = p2_loss_weight_gamma > 0.\n        register_buffer('p2_loss_weight', (p2_loss_weight_k + alphas_cumprod / (1 - alphas_cumprod)) ** -p2_loss_weight_gamma)\n    def sample_random_times(self, batch):\n        return torch.randint(0, self.num_timesteps, (batch,), device = self.betas.device, dtype = torch.long)\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:602-620"
    },
    "169": {
        "file_id": 6,
        "content": "In this code segment, the author is computing posterior means for a model, performing loss reweighting, generating random times, and calculating posterior values. The posterior means are calculated based on betas and alphas, while the loss reweighting considers p2_loss_weight_gamma. Random times are sampled for a batch of inputs using torch.randint. The q_posterior function calculates posterior mean, variance, and log-variance clipped from these computed values.",
        "type": "comment"
    },
    "170": {
        "file_id": 6,
        "content": "    def q_sample(self, x_start, t, noise = None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n    def calculate_v(self, x_start, t, noise = None):\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n        )\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape = x_from.shape\n        noise = default(noise, lambda: torch.randn_like(x_from))\n        alpha = extract(self.sqrt_alphas_cumprod, from_t, shape)\n        sigma = extract(self.sqrt_one_minus_alphas_cumprod, from_t, shape)\n        alpha_next = extract(self.sqrt_alphas_cumprod, to_t, shape)\n        sigma_next = extract(self.sqrt_one_minus_alphas_cumprod, to_t, shape)\n        return x_from * (alpha_next / alpha) + noise * (sigma_next * alpha - sigma * alpha_next) / alpha",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:622-645"
    },
    "171": {
        "file_id": 6,
        "content": "The code defines three functions: `q_sample`, `calculate_v`, and `q_sample_from_to`. These functions are part of a neural network for generating images. `q_sample` combines alpha and noise values to generate a sample, while `calculate_v` calculates the difference between an alpha-blended noise and a one minus alpha-blended image start. The `q_sample_from_to` function samples from one timestep to another by interpolating alphas and sigmas.",
        "type": "comment"
    },
    "172": {
        "file_id": 6,
        "content": "    def predict_start_from_v(self, x_t, t, v):\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n        )\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n    def predict_noise_from_start(self, x_t, t, x0):\n        return (\n            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n        )\n    def p2_reweigh_loss(self, loss, times):\n        if not self.has_p2_loss_reweighting:\n            return loss\n        return loss * extract(self.p2_loss_weight, times, loss.shape)\n# rearrange image to sequence\nclass RearrangeToSequence(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x):",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:647-677"
    },
    "173": {
        "file_id": 6,
        "content": "The code defines three methods for predicting values from different inputs, including v and noise. It also includes a method to reweight loss using p2_loss_weight and a class to rearrange images into sequences.",
        "type": "comment"
    },
    "174": {
        "file_id": 6,
        "content": "        x = rearrange(x, 'b c ... -> b ... c')\n        x, ps = pack([x], 'b * c')\n        x = self.fn(x)\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b ... c -> b c ...')\n        return x\n# diffusion prior\nclass LayerNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-5, fp16_eps = 1e-3, stable = False):\n        super().__init__()\n        self.eps = eps\n        self.fp16_eps = fp16_eps\n        self.stable = stable\n        self.g = nn.Parameter(torch.ones(dim))\n    def forward(self, x):\n        eps = self.eps if x.dtype == torch.float32 else self.fp16_eps\n        if self.stable:\n            x = x / x.amax(dim = -1, keepdim = True).detach()\n        var = torch.var(x, dim = -1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = -1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\nclass ChanLayerNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-5, fp16_eps = 1e-3, stable = False):\n        super().__init__()\n        self.eps = eps\n        self.fp16_eps = fp16_eps",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:678-711"
    },
    "175": {
        "file_id": 6,
        "content": "This function is applying layer normalization to input tensor 'x' and returning the normalized output. The 'LayerNorm' class is a type of layer normalization, while 'ChanLayerNorm' is a channel-wise version. The code includes settings for epsilon, float precision, and stability options.",
        "type": "comment"
    },
    "176": {
        "file_id": 6,
        "content": "        self.stable = stable\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n    def forward(self, x):\n        eps = self.eps if x.dtype == torch.float32 else self.fp16_eps\n        if self.stable:\n            x = x / x.amax(dim = 1, keepdim = True).detach()\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n# mlp\nclass MLP(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        *,\n        expansion_factor = 2.,\n        depth = 2,\n        norm = False,\n    ):\n        super().__init__()\n        hidden_dim = int(expansion_factor * dim_out)\n        norm_fn = lambda: nn.LayerNorm(hidden_dim) if norm else nn.Identity()\n        layers = [nn.Sequential(\n            nn.Linear(dim_in, hidden_dim),",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:712-750"
    },
    "177": {
        "file_id": 6,
        "content": "This code defines a Residual class that wraps a function and adds it to the input. It also contains an MLP (Multi-Layer Perceptron) class with optional normalization and activation functions, followed by a series of fully connected layers. The forward method in DALLE2_PyTorch performs normalization, calculates mean and variance, then applies element-wise transformations before returning the output.",
        "type": "comment"
    },
    "178": {
        "file_id": 6,
        "content": "            nn.SiLU(),\n            norm_fn()\n        )]\n        for _ in range(depth - 1):\n            layers.append(nn.Sequential(\n                nn.Linear(hidden_dim, hidden_dim),\n                nn.SiLU(),\n                norm_fn()\n            ))\n        layers.append(nn.Linear(hidden_dim, dim_out))\n        self.net = nn.Sequential(*layers)\n    def forward(self, x):\n        return self.net(x.float())\n# relative positional bias for causal transformer\nclass RelPosBias(nn.Module):\n    def __init__(\n        self,\n        heads = 8,\n        num_buckets = 32,\n        max_distance = 128,\n    ):\n        super().__init__()\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n    @staticmethod\n    def _relative_position_bucket(\n        relative_position,\n        num_buckets = 32,\n        max_distance = 128\n    ):\n        n = -relative_position\n        n = torch.max(n, torch.zeros_like(n))\n        max_exact = num_buckets // 2\n        is_small = n < max_exact",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:751-792"
    },
    "179": {
        "file_id": 6,
        "content": "This code defines a neural network architecture for the DALL-E 2 model. It includes a sequential layer with multiple linear layers, SiLU activation function, and normalization. The forward method performs inference on input data. Another class is defined for relative positional bias in causal transformer. The RelPosBias class initializes an embedding layer to calculate the relative position between elements for attention mechanism. It uses the concept of buckets, where each bucket represents a range of distances between two elements, and computes the relative position bucket based on input data.",
        "type": "comment"
    },
    "180": {
        "file_id": 6,
        "content": "        val_if_large = max_exact + (torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)).long()\n        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n        return torch.where(is_small, n, val_if_large)\n    def forward(self, i, j, *, device):\n        q_pos = torch.arange(i, dtype = torch.long, device = device)\n        k_pos = torch.arange(j, dtype = torch.long, device = device)\n        rel_pos = rearrange(k_pos, 'j -> 1 j') - rearrange(q_pos, 'i -> i 1')\n        rp_bucket = self._relative_position_bucket(rel_pos, num_buckets = self.num_buckets, max_distance = self.max_distance)\n        values = self.relative_attention_bias(rp_bucket)\n        return rearrange(values, 'i j h -> h i j')\n# feedforward\nclass SwiGLU(nn.Module):\n    \"\"\" used successfully in https://arxiv.org/abs/2204.0231 \"\"\"\n    def forward(self, x):\n        x, gate = x.chunk(2, dim = -1)\n        return x * F.silu(gate)\ndef FeedForward(\n    dim,\n    mult = 4,",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:794-816"
    },
    "181": {
        "file_id": 6,
        "content": "This code snippet defines a class for DALLE2-pytorch, containing a method to calculate relative position buckets and an attention layer. The attention layer uses the SwiGLU activation function in its FeedForward module. The purpose of this code is to facilitate the calculation and application of positional embeddings in a transformer model.",
        "type": "comment"
    },
    "182": {
        "file_id": 6,
        "content": "    dropout = 0.,\n    post_activation_norm = False\n):\n    \"\"\" post-activation norm https://arxiv.org/abs/2110.09456 \"\"\"\n    inner_dim = int(mult * dim)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, inner_dim * 2, bias = False),\n        SwiGLU(),\n        LayerNorm(inner_dim) if post_activation_norm else nn.Identity(),\n        nn.Dropout(dropout),\n        nn.Linear(inner_dim, dim, bias = False)\n    )\n# attention\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        dropout = 0.,\n        causal = False,\n        rotary_emb = None,\n        cosine_sim = True,\n        cosine_sim_scale = 16\n    ):\n        super().__init__()\n        self.scale = cosine_sim_scale if cosine_sim else (dim_head ** -0.5)\n        self.cosine_sim = cosine_sim\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.causal = causal\n        self.norm = LayerNorm(dim)\n        self.dropout = nn.Dropout(dropout)\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:817-858"
    },
    "183": {
        "file_id": 6,
        "content": "The code defines a module that applies post-activation normalization. It also includes a nested Attention class that performs multi-head attention with optional causal masking and rotary embedding. The main components include layer normalization, dropout regularization, and linear transformations for dimensionality adjustments. The cosine similarity calculation is utilized if specified.",
        "type": "comment"
    },
    "184": {
        "file_id": 6,
        "content": "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n        self.rotary_emb = rotary_emb\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n    def forward(self, x, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n        x = self.norm(x)\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n        q = q * self.scale\n        # rotary embeddings\n        if exists(self.rotary_emb):\n            q, k = map(self.rotary_emb.rotate_queries_or_keys, (q, k))\n        # add null key / value for classifier free guidance in prior net\n        nk, nv = map(lambda t: repeat(t, 'd -> b 1 d', b = b), self.null_kv.unbind(dim = -2))\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n        # whether to use cosine sim\n        if self.cosine_sim:",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:859-891"
    },
    "185": {
        "file_id": 6,
        "content": "This code defines a self-attention layer for DALL·E 2, initializing linear layers and including the option to use rotary embeddings. It also allows for classifier free guidance by adding null key/value pairs and using cosine similarity if enabled.",
        "type": "comment"
    },
    "186": {
        "file_id": 6,
        "content": "            q, k = map(l2norm, (q, k))\n        q, k = map(lambda t: t * math.sqrt(self.scale), (q, k))\n        # calculate query / key similarities\n        sim = einsum('b h i d, b j d -> b h i j', q, k)\n        # relative positional encoding (T5 style)\n        if exists(attn_bias):\n            sim = sim + attn_bias\n        # masking\n        max_neg_value = -torch.finfo(sim.dtype).max\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n        if self.causal:\n            i, j = sim.shape[-2:]\n            causal_mask = torch.ones((i, j), dtype = torch.bool, device = device).triu(j - i + 1)\n            sim = sim.masked_fill(causal_mask, max_neg_value)\n        # attention\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.type(sim.dtype)\n        attn = self.dropout(attn)\n        # aggregate values\n        out = einsum('b h i j, b j d -> b h i d', attn, v)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:892-928"
    },
    "187": {
        "file_id": 6,
        "content": "This code snippet performs multi-head attention by first normalizing the query and key tensors, calculating their similarities, adding relative positional encoding if available, masking irrelevant values based on a given mask, applying causal masking if specified, and finally computing the attention weights and aggregating the corresponding values.",
        "type": "comment"
    },
    "188": {
        "file_id": 6,
        "content": "        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\nclass CausalTransformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        ff_mult = 4,\n        norm_in = False,\n        norm_out = True,\n        attn_dropout = 0.,\n        ff_dropout = 0.,\n        final_proj = True,\n        normformer = False,\n        rotary_emb = True\n    ):\n        super().__init__()\n        self.init_norm = LayerNorm(dim) if norm_in else nn.Identity() # from latest BLOOM model and Yandex's YaLM\n        self.rel_pos_bias = RelPosBias(heads = heads)\n        rotary_emb = RotaryEmbedding(dim = min(32, dim_head)) if rotary_emb else None\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, causal = True, dim_head = dim_head, heads = heads, dropout = attn_dropout, rotary_emb = rotary_emb),\n                FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout, post_activation_norm = normformer)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:930-961"
    },
    "189": {
        "file_id": 6,
        "content": "This code defines a `CausalTransformer` class for natural language processing tasks. The class initializes several modules such as LayerNorm, RelPosBias, RotaryEmbedding, and Attention. It also includes a FeedForward layer with configurable parameters like `dim`, `depth`, `dim_head`, `heads`, `ff_mult`, `attn_dropout`, `ff_dropout`, `norm_in`, `norm_out`, `final_proj`, and `rotary_emb`. The code snippet you provided is responsible for rearranging the tensor dimensions and returning it after processing by the `CausalTransformer` model.",
        "type": "comment"
    },
    "190": {
        "file_id": 6,
        "content": "            ]))\n        self.norm = LayerNorm(dim, stable = True) if norm_out else nn.Identity()  # unclear in paper whether they projected after the classic layer norm for the final denoised image embedding, or just had the transformer output it directly: plan on offering both options\n        self.project_out = nn.Linear(dim, dim, bias = False) if final_proj else nn.Identity()\n    def forward(self, x):\n        n, device = x.shape[1], x.device\n        x = self.init_norm(x)\n        attn_bias = self.rel_pos_bias(n, n + 1, device = device)\n        for attn, ff in self.layers:\n            x = attn(x, attn_bias = attn_bias) + x\n            x = ff(x) + x\n        out = self.norm(x)\n        return self.project_out(out)\nclass DiffusionPriorNetwork(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_timesteps = None,\n        num_time_embeds = 1,\n        num_image_embeds = 1,\n        num_text_embeds = 1,\n        max_text_len = 256,\n        self_cond = False,\n        **kwargs\n    ):\n        super().__init__()",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:962-993"
    },
    "191": {
        "file_id": 6,
        "content": "The code initializes a DiffusionPriorNetwork model with multiple layers, including attention and feed-forward modules. It also includes layer normalization and the option to project the output. The network takes in input of varying dimensions and can condition on time, image, and/or text embeddings. The self_cond parameter determines whether or not to use self-conditioning.",
        "type": "comment"
    },
    "192": {
        "file_id": 6,
        "content": "        self.dim = dim\n        self.num_time_embeds = num_time_embeds\n        self.num_image_embeds = num_image_embeds\n        self.num_text_embeds = num_text_embeds\n        self.to_text_embeds = nn.Sequential(\n            nn.Linear(dim, dim * num_text_embeds) if num_text_embeds > 1 else nn.Identity(),\n            Rearrange('b (n d) -> b n d', n = num_text_embeds)\n        )\n        self.continuous_embedded_time = not exists(num_timesteps)\n        self.to_time_embeds = nn.Sequential(\n            nn.Embedding(num_timesteps, dim * num_time_embeds) if exists(num_timesteps) else nn.Sequential(SinusoidalPosEmb(dim), MLP(dim, dim * num_time_embeds)), # also offer a continuous version of timestep embeddings, with a 2 layer MLP\n            Rearrange('b (n d) -> b n d', n = num_time_embeds)\n        )\n        self.to_image_embeds = nn.Sequential(\n            nn.Linear(dim, dim * num_image_embeds) if num_image_embeds > 1 else nn.Identity(),\n            Rearrange('b (n d) -> b n d', n = num_image_embeds)\n        )\n        self.learned_query = nn.Parameter(torch.randn(dim))",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:994-1017"
    },
    "193": {
        "file_id": 6,
        "content": "This code defines a class with parameters for dimensionality, number of time, image, and text embeddings. It initializes layers to transform input into text, time, and image embeddings. The \"learned_query\" is a learned parameter for the model.",
        "type": "comment"
    },
    "194": {
        "file_id": 6,
        "content": "        self.causal_transformer = CausalTransformer(dim = dim, **kwargs)\n        # dalle1 learned padding strategy\n        self.max_text_len = max_text_len\n        self.null_text_encodings = nn.Parameter(torch.randn(1, max_text_len, dim))\n        self.null_text_embeds = nn.Parameter(torch.randn(1, num_text_embeds, dim))\n        self.null_image_embed = nn.Parameter(torch.randn(1, dim))\n        # whether to use self conditioning, Hinton's group's new ddpm technique\n        self.self_cond = self_cond\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n        if cond_scale == 1:\n            return logits\n        null_logits = self.forward(*args, text_cond_drop_prob = 1., image_cond_drop_prob = 1, **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n    def forward(\n        self,\n        image_embed,\n        diffusion_timesteps,\n        *,\n        text_embed,\n        text_encodings = None,",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1018-1052"
    },
    "195": {
        "file_id": 6,
        "content": "The code defines a model with a causal transformer and includes parameters for padding strategy, self-conditioning, and a function to perform forward calculations. The `forward_with_cond_scale` method takes conditional scaling as input and returns the scaled logits by combining original logits with null logits at 100% condition drop probabilities.",
        "type": "comment"
    },
    "196": {
        "file_id": 6,
        "content": "        self_cond = None,\n        text_cond_drop_prob = 0.,\n        image_cond_drop_prob = 0.\n    ):\n        batch, dim, device, dtype = *image_embed.shape, image_embed.device, image_embed.dtype\n        num_time_embeds, num_image_embeds, num_text_embeds = self.num_time_embeds, self.num_image_embeds, self.num_text_embeds\n        # setup self conditioning\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros(batch, self.dim, device = device, dtype = dtype))\n            self_cond = rearrange(self_cond, 'b d -> b 1 d')\n        # in section 2.2, last paragraph\n        # \"... consisting of encoded text, CLIP text embedding, diffusion timestep embedding, noised CLIP image embedding, final embedding for prediction\"\n        text_embed = self.to_text_embeds(text_embed)\n        image_embed = self.to_image_embeds(image_embed)\n        # classifier free guidance masks\n        text_keep_mask = prob_mask_like((batch,), 1 - text_cond_drop_prob, device = device)\n        text_keep_mask = rearrange(text_keep_mask, 'b -> b 1 1')",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1053-1076"
    },
    "197": {
        "file_id": 6,
        "content": "This code initializes a model's parameters based on the given image_embed. It sets up self-conditioning if necessary, converts text and image embeddings to the appropriate format, and creates classifier free guidance masks for both text and image inputs. The model will use these embeddings and masks for prediction.",
        "type": "comment"
    },
    "198": {
        "file_id": 6,
        "content": "        image_keep_mask = prob_mask_like((batch,), 1 - image_cond_drop_prob, device = device)\n        image_keep_mask = rearrange(image_keep_mask, 'b -> b 1 1')\n        # make text encodings optional\n        # although the paper seems to suggest it is present <--\n        if not exists(text_encodings):\n            text_encodings = torch.empty((batch, 0, dim), device = device, dtype = dtype)\n        mask = torch.any(text_encodings != 0., dim = -1)\n        # replace any padding in the text encodings with learned padding tokens unique across position\n        text_encodings = text_encodings[:, :self.max_text_len]\n        mask = mask[:, :self.max_text_len]\n        text_len = text_encodings.shape[-2]\n        remainder = self.max_text_len - text_len\n        if remainder > 0:\n            text_encodings = F.pad(text_encodings, (0, 0, 0, remainder), value = 0.)\n            mask = F.pad(mask, (0, remainder), value = False)\n        # mask out text encodings with null encodings\n        null_text_encodings = self.null_text_encodings.to(text_encodings.dtype)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1078-1103"
    },
    "199": {
        "file_id": 6,
        "content": "This code snippet is preparing the input data for a DALL-E 2 model by handling text encodings. It creates an image_keep_mask, makes text encodings optional based on their existence, applies masking to remove padding or null encodings, and ensures that the length of text_encodings matches the expected maximum length.",
        "type": "comment"
    }
}