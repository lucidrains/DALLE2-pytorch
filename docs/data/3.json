{
    "300": {
        "file_id": 6,
        "content": "        lowres_downsample_first = True,             # cascading ddpm - resizes to lower resolution, then to next conditional resolution + blur\n        blur_prob = 0.5,                            # cascading ddpm - when training, the gaussian blur is only applied 50% of the time\n        blur_sigma = 0.6,                           # cascading ddpm - blur sigma\n        blur_kernel_size = 3,                       # cascading ddpm - blur kernel size\n        lowres_noise_sample_level = 0.2,            # in imagen paper, they use a 0.2 noise level at sample time for low resolution conditioning\n        clip_denoised = True,\n        clip_x_start = True,\n        clip_adapter_overrides = dict(),\n        learned_variance = True,\n        learned_variance_constrain_frac = False,\n        vb_loss_weight = 0.001,\n        unconditional = False,                      # set to True for generating images without conditioning\n        auto_normalize_img = True,                  # whether to take care of normalizing the i",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2497-2509"
    },
    "301": {
        "file_id": 6,
        "content": "This code snippet is responsible for configuring the settings for a denoising diffusion probabilistic model (DDPM) in the DALLE2-pytorch project. The settings include cascading DDPM parameters, noise level at sample time, clip options, learned variance configuration, and unconditional image generation toggles.",
        "type": "comment"
    },
    "302": {
        "file_id": 6,
        "content": "mage from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        use_dynamic_thres = False,                  # from the Imagen paper\n        dynamic_thres_percentile = 0.95,\n        p2_loss_weight_gamma = 0.,                  # p2 loss weight, from https://arxiv.org/abs/2204.00227 - 0 is equivalent to weight of 1 across time - 1. is recommended\n        p2_loss_weight_k = 1,\n        ddim_sampling_eta = 0.                      # can be set to 0. for deterministic sampling afaict\n    ):\n        super().__init__()\n        # clip\n        self.clip = None\n        if exists(clip):\n            assert not unconditional, 'clip must not be given if doing unconditional image training'\n            assert channels == clip.image_channels, f'channels of image ({channels}) should be equal to the channels that CLIP accepts ({clip.image_channels})'\n            if isinstance(clip, CLIP):\n                clip = XClipAdapter(clip, **clip_adapter_overrides)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2509-2526"
    },
    "303": {
        "file_id": 6,
        "content": "The code initializes an object with various parameters such as use_dynamic_thres, dynamic_thres_percentile, p2_loss_weight_gamma, p2_loss_weight_k, ddim_sampling_eta, and clip. It also checks if the 'clip' parameter is given and performs necessary assertions. If 'clip' exists and unconditional image training is not being done, it ensures the channels match with CLIP's accepted channels. It also uses XClipAdapter for compatibility with additional overrides.",
        "type": "comment"
    },
    "304": {
        "file_id": 6,
        "content": "            elif isinstance(clip, CoCa):\n                clip = CoCaAdapter(clip, **clip_adapter_overrides)\n            freeze_model_and_make_eval_(clip)\n            assert isinstance(clip, BaseClipAdapter)\n            self.clip = clip\n        # determine image size, with image_size and image_sizes taking precedence\n        if exists(image_size) or exists(image_sizes):\n            assert exists(image_size) ^ exists(image_sizes), 'only one of image_size or image_sizes must be given'\n            image_size = default(image_size, lambda: image_sizes[-1])\n        elif exists(clip):\n            image_size = clip.image_size\n        else:\n            raise Error('either image_size, image_sizes, or clip must be given to decoder')\n        # channels\n        self.channels = channels\n        # normalize and unnormalize image functions\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        # verify conditioning method",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2527-2555"
    },
    "305": {
        "file_id": 6,
        "content": "The code checks the input 'clip' type and applies the CoCaAdapter if it's an instance of CoCa. It then freezes the model for evaluation, ensures 'clip' is a BaseClipAdapter instance, and assigns it to self.clip. The image_size is determined from either 'image_size', 'image_sizes', or 'clip'. It sets the 'channels', 'normalize_img', and 'unnormalize_img' based on given parameters.",
        "type": "comment"
    },
    "306": {
        "file_id": 6,
        "content": "        unets = cast_tuple(unet)\n        num_unets = len(unets)\n        self.num_unets = num_unets\n        self.unconditional = unconditional\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n        vaes = pad_tuple_to_length(cast_tuple(vae), len(unets), fillvalue = NullVQGanVAE(channels = self.channels))\n        # whether to use learned variance, defaults to True for the first unet in the cascade, as in paper\n        learned_variance = pad_tuple_to_length(cast_tuple(learned_variance), len(unets), fillvalue = False)\n        self.learned_variance = learned_variance\n        self.learned_variance_constrain_frac = learned_variance_constrain_frac # whether to constrain the output of the network (the interpolation fraction) from 0 to 1\n        self.vb_loss_weight = vb_loss_weight\n        # default and validate conditioning parameters\n        use_noise_for_lowres_cond = cast_tuple(use_noise_for_lowres_cond, num_unets - 1, validate = False)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2557-2577"
    },
    "307": {
        "file_id": 6,
        "content": "This code initializes the U-Nets and VAEs for a DALL-E 2 model. It sets the number of unets, whether they are unconditional or conditioned on previous unets, and their learned variance. It also sets default parameters for conditioning with noise and constrains the output of the network from 0 to 1.",
        "type": "comment"
    },
    "308": {
        "file_id": 6,
        "content": "        use_blur_for_lowres_cond = cast_tuple(use_blur_for_lowres_cond, num_unets - 1, validate = False)\n        if len(use_noise_for_lowres_cond) < num_unets:\n            use_noise_for_lowres_cond = (False, *use_noise_for_lowres_cond)\n        if len(use_blur_for_lowres_cond) < num_unets:\n            use_blur_for_lowres_cond = (False, *use_blur_for_lowres_cond)\n        assert not use_noise_for_lowres_cond[0], 'first unet will never need low res noise conditioning'\n        assert not use_blur_for_lowres_cond[0], 'first unet will never need low res blur conditioning'\n        assert num_unets == 1 or all((use_noise or use_blur) for use_noise, use_blur in zip(use_noise_for_lowres_cond[1:], use_blur_for_lowres_cond[1:]))\n        # construct unets and vaes\n        self.unets = nn.ModuleList([])\n        self.vaes = nn.ModuleList([])\n        for ind, (one_unet, one_vae, one_unet_learned_var, lowres_noise_cond) in enumerate(zip(unets, vaes, learned_variance, use_noise_for_lowres_cond)):\n            assert isinstance(one_unet, Unet)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2578-2597"
    },
    "309": {
        "file_id": 6,
        "content": "This code is setting up Unets and Vaes for a model. It ensures that the lists of noise conditions and blur conditions are long enough to correspond to each Unet, adds the Unets and Vaes to module lists, and asserts that at least one Unet will not need low res noise or blur conditioning.",
        "type": "comment"
    },
    "310": {
        "file_id": 6,
        "content": "            assert isinstance(one_vae, (VQGanVAE, NullVQGanVAE))\n            is_first = ind == 0\n            latent_dim = one_vae.encoded_dim if exists(one_vae) else None\n            unet_channels = default(latent_dim, self.channels)\n            unet_channels_out = unet_channels * (1 if not one_unet_learned_var else 2)\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                lowres_noise_cond = lowres_noise_cond,\n                cond_on_image_embeds = not unconditional and is_first,\n                cond_on_text_encodings = not unconditional and one_unet.cond_on_text_encodings,\n                channels = unet_channels,\n                channels_out = unet_channels_out\n            )\n            self.unets.append(one_unet)\n            self.vaes.append(one_vae.copy_for_eval())\n        # sampling timesteps, defaults to non-ddim with full timesteps sampling\n        self.sample_timesteps = cast_tuple(sample_timesteps, num_unets)\n        self.ddim_sampling_eta = ddim_sampling_eta",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2598-2621"
    },
    "311": {
        "file_id": 6,
        "content": "This code block appends a new VAE instance to the list of VAEs and a copied evaluation version of that VAE to the VAEs list. The code also sets the sampling timesteps and ddim_sampling_eta based on the input parameters.",
        "type": "comment"
    },
    "312": {
        "file_id": 6,
        "content": "        # create noise schedulers per unet\n        if not exists(beta_schedule):\n            beta_schedule = ('cosine', *(('cosine',) * max(num_unets - 2, 0)), *(('linear',) * int(num_unets > 1)))\n        beta_schedule = cast_tuple(beta_schedule, num_unets)\n        p2_loss_weight_gamma = cast_tuple(p2_loss_weight_gamma, num_unets)\n        self.noise_schedulers = nn.ModuleList([])\n        for ind, (unet_beta_schedule, unet_p2_loss_weight_gamma, sample_timesteps) in enumerate(zip(beta_schedule, p2_loss_weight_gamma, self.sample_timesteps)):\n            assert not exists(sample_timesteps) or sample_timesteps <= timesteps, f'sampling timesteps {sample_timesteps} must be less than or equal to the number of training timesteps {timesteps} for unet {ind + 1}'\n            noise_scheduler = NoiseScheduler(\n                beta_schedule = unet_beta_schedule,\n                timesteps = timesteps,\n                loss_type = loss_type,\n                p2_loss_weight_gamma = unet_p2_loss_weight_gamma,\n                p2_loss_weight_k = p2_loss_weight_k",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2623-2641"
    },
    "313": {
        "file_id": 6,
        "content": "This code creates noise schedulers for each unet, based on the provided beta schedule and loss weight gamma. It asserts that sampling timesteps must be less than or equal to the number of training timesteps, and initializes a NoiseScheduler object with the specified parameters for each unet.",
        "type": "comment"
    },
    "314": {
        "file_id": 6,
        "content": "            )\n            self.noise_schedulers.append(noise_scheduler)\n        # unet image sizes\n        image_sizes = default(image_sizes, (image_size,))\n        image_sizes = tuple(sorted(set(image_sizes)))\n        assert self.num_unets == len(image_sizes), f'you did not supply the correct number of u-nets ({self.num_unets}) for resolutions {image_sizes}'\n        self.image_sizes = image_sizes\n        self.sample_channels = cast_tuple(self.channels, len(image_sizes))\n        # random crop sizes (for super-resoluting unets at the end of cascade?)\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, len(image_sizes))\n        assert not exists(self.random_crop_sizes[0]), 'you would not need to randomly crop the image for the base unet'\n        # predict x0 config\n        self.predict_x_start = cast_tuple(predict_x_start, len(unets)) if not predict_x_start_for_latent_diffusion else tuple(map(lambda t: isinstance(t, VQGanVAE), self.vaes))\n        # predict v\n        self.predict_v = cast_tuple(predict_v, len(unets))",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2642-2666"
    },
    "315": {
        "file_id": 6,
        "content": "This code is setting up the parameters for a model. It creates noise schedulers, defines image sizes and crop sizes for different resolutions, and configures predicting x0 and v values. These settings will be used to train or use the model. The code also performs assertions to ensure that the correct number of unets and vaes are provided for each resolution.",
        "type": "comment"
    },
    "316": {
        "file_id": 6,
        "content": "        # input image range\n        self.input_image_range = (-1. if not auto_normalize_img else 0., 1.)\n        # cascading ddpm related stuff\n        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n        self.lowres_conds = nn.ModuleList([])\n        for unet_index, use_noise, use_blur in zip(range(num_unets), use_noise_for_lowres_cond, use_blur_for_lowres_cond):\n            if unet_index == 0:\n                self.lowres_conds.append(None)\n                continue\n            lowres_cond = LowresConditioner(\n                downsample_first = lowres_downsample_first,\n                use_blur = use_blur,\n                use_noise = use_noise,\n                blur_prob = blur_prob,\n                blur_sigma = blur_sigma,\n                blur_kernel_size = blur_kernel_size,\n                input_image_range = self.input_image_range,",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2668-2691"
    },
    "317": {
        "file_id": 6,
        "content": "The code initializes the input image range and handles lowres_cond for each unet in the model. It ensures that the first unet is unconditioned, while the rest have `lowres_cond` set to True. The `LowresConditioner` class is used with specified parameters for downsampling, blurring, and input image range.",
        "type": "comment"
    },
    "318": {
        "file_id": 6,
        "content": "                normalize_img_fn = self.normalize_img,\n                unnormalize_img_fn = self.unnormalize_img\n            )\n            self.lowres_conds.append(lowres_cond)\n        self.lowres_noise_sample_level = lowres_noise_sample_level\n        # classifier free guidance\n        self.image_cond_drop_prob = image_cond_drop_prob\n        self.text_cond_drop_prob = text_cond_drop_prob\n        self.can_classifier_guidance = image_cond_drop_prob > 0. or text_cond_drop_prob > 0.\n        # whether to clip when sampling\n        self.clip_denoised = clip_denoised\n        self.clip_x_start = clip_x_start\n        # dynamic thresholding settings, if clipping denoised during sampling\n        self.use_dynamic_thres = use_dynamic_thres\n        self.dynamic_thres_percentile = dynamic_thres_percentile\n        # device tracker\n        self.register_buffer('_dummy', torch.Tensor([True]), persistent = False)\n    @property\n    def device(self):\n        return self._dummy.device\n    @property\n    def condition_on_text_encodings(self):",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2692-2725"
    },
    "319": {
        "file_id": 6,
        "content": "This code is setting up parameters and functions for an image generation model. It includes normalization and unnormalization functions, lowres noise sample level, classifier free guidance settings, clipping options during sampling, dynamic thresholding settings, and device management. The model can condition on text encodings and uses a device tracker to keep track of device information.",
        "type": "comment"
    },
    "320": {
        "file_id": 6,
        "content": "        return any([unet.cond_on_text_encodings for unet in self.unets if isinstance(unet, Unet)])\n    def get_unet(self, unet_number):\n        assert 0 < unet_number <= self.num_unets\n        index = unet_number - 1\n        return self.unets[index]\n    def parse_unet_output(self, learned_variance, output):\n        var_interp_frac_unnormalized = None\n        if learned_variance:\n            output, var_interp_frac_unnormalized = output.chunk(2, dim = 1)\n        return UnetOutput(output, var_interp_frac_unnormalized)\n    @contextmanager\n    def one_unet_in_gpu(self, unet_number = None, unet = None):\n        assert exists(unet_number) ^ exists(unet)\n        if exists(unet_number):\n            unet = self.get_unet(unet_number)\n        # devices\n        cuda, cpu = torch.device('cuda'), torch.device('cpu')\n        self.cuda()\n        devices = [module_device(unet) for unet in self.unets]\n        self.unets.to(cpu)\n        unet.to(cuda)\n        yield\n        for unet, device in zip(self.unets, devices):\n            unet.to(device)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2726-2762"
    },
    "321": {
        "file_id": 6,
        "content": "This code defines methods for working with a collection of UNET models. The `get_unet` method retrieves a specific UNET based on its number, ensuring it is within the valid range. `parse_unet_output` parses the output of a UNET, interpreting learned variance if present. The `one_unet_in_gpu` context manager allows running inference for one UNET on the GPU while keeping other UNETs on the CPU.",
        "type": "comment"
    },
    "322": {
        "file_id": 6,
        "content": "    def dynamic_threshold(self, x):\n        \"\"\" proposed in https://arxiv.org/abs/2205.11487 as an improved clamping in the setting of classifier free guidance \"\"\"\n        # s is the threshold amount\n        # static thresholding would just be s = 1\n        s = 1.\n        if self.use_dynamic_thres:\n            s = torch.quantile(\n                rearrange(x, 'b ... -> b (...)').abs(),\n                self.dynamic_thres_percentile,\n                dim = -1\n            )\n            s.clamp_(min = 1.)\n            s = s.view(-1, *((1,) * (x.ndim - 1)))\n        # clip by threshold, depending on whether static or dynamic\n        x = x.clamp(-s, s) / s\n        return x\n    def p_mean_variance(self, unet, x, t, image_embed, noise_scheduler, text_encodings = None, lowres_cond_img = None, self_cond = None, clip_denoised = True, predict_x_start = False, predict_v = False, learned_variance = False, cond_scale = 1., model_output = None, lowres_noise_level = None):\n        assert not (cond_scale != 1. and not self.",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2764-2785"
    },
    "323": {
        "file_id": 6,
        "content": "This code snippet defines a function `dynamic_threshold` and `p_mean_variance`. The `dynamic_threshold` function adjusts the threshold for clamping based on the input's quantile values. It uses static thresholding (s=1) by default, but can be set to dynamic thresholding if `self.use_dynamic_thres` is true. The `p_mean_variance` function performs classifier-free guidance for image generation and includes options for mean/variance prediction, conditioning, noise scheduling, and more.",
        "type": "comment"
    },
    "324": {
        "file_id": 6,
        "content": "can_classifier_guidance), 'the decoder was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n        model_output = default(model_output, lambda: unet.forward_with_cond_scale(x, t, image_embed = image_embed, text_encodings = text_encodings, cond_scale = cond_scale, lowres_cond_img = lowres_cond_img, self_cond = self_cond, lowres_noise_level = lowres_noise_level))\n        pred, var_interp_frac_unnormalized = self.parse_unet_output(learned_variance, model_output)\n        if predict_v:\n            x_start = noise_scheduler.predict_start_from_v(x, t = t, v = pred)\n        elif predict_x_start:\n            x_start = pred\n        else:\n            x_start = noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n        if clip_denoised:\n            x_start = self.dynamic_threshold(x_start)\n        model_mean, posterior_variance, posterior_log_variance = noise_scheduler.q_posterior(x_start=x_start, x_t=x, t=t)\n        if learned_variance:",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2785-2803"
    },
    "325": {
        "file_id": 6,
        "content": "This code block is responsible for decoding an input image using a pre-trained unet model. It applies classifier free guidance if enabled, and then calculates the mean and variance of the posterior distribution to perform denoising diffusion probability.",
        "type": "comment"
    },
    "326": {
        "file_id": 6,
        "content": "            # if learned variance, posterio variance and posterior log variance are predicted by the network\n            # by an interpolation of the max and min log beta values\n            # eq 15 - https://arxiv.org/abs/2102.09672\n            min_log = extract(noise_scheduler.posterior_log_variance_clipped, t, x.shape)\n            max_log = extract(torch.log(noise_scheduler.betas), t, x.shape)\n            var_interp_frac = unnormalize_zero_to_one(var_interp_frac_unnormalized)\n            if self.learned_variance_constrain_frac:\n                var_interp_frac = var_interp_frac.sigmoid()\n            posterior_log_variance = var_interp_frac * max_log + (1 - var_interp_frac) * min_log\n            posterior_variance = posterior_log_variance.exp()\n        return model_mean, posterior_variance, posterior_log_variance, x_start\n    @torch.no_grad()\n    def p_sample(self, unet, x, t, image_embed, noise_scheduler, text_encodings = None, cond_scale = 1., lowres_cond_img = None, self_cond = None, predict_x_",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2804-2820"
    },
    "327": {
        "file_id": 6,
        "content": "This code calculates the posterior variance and log variance for a model based on the maximum and minimum log beta values, as described in Equation 15 from arXiv paper. It also applies a learned constraint factor and uses sigmoid activation if required. The function returns the model mean, posterior variance, posterior log variance, and x_start.",
        "type": "comment"
    },
    "328": {
        "file_id": 6,
        "content": "start = False, predict_v = False, learned_variance = False, clip_denoised = True, lowres_noise_level = None):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance, x_start = self.p_mean_variance(unet, x = x, t = t, image_embed = image_embed, text_encodings = text_encodings, cond_scale = cond_scale, lowres_cond_img = lowres_cond_img, self_cond = self_cond, clip_denoised = clip_denoised, predict_x_start = predict_x_start, predict_v = predict_v, noise_scheduler = noise_scheduler, learned_variance = learned_variance, lowres_noise_level = lowres_noise_level)\n        noise = torch.randn_like(x)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n        return pred, x_start\n    @torch.no_grad()\n    def p_sample_loop_ddpm(\n        self,\n        unet,\n        shape,\n        image_embed,\n        noise_scheduler,\n        predict_x_start = False,",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2820-2836"
    },
    "329": {
        "file_id": 6,
        "content": "This function takes input x and returns the predicted values pred and x_start. It uses a p_mean_variance method from self to calculate model_mean, model_log_variance, and x_start. Noise is added to the input x, except when t == 0. The result is the sum of model_mean and nonzero_mask * (0.5 * model_log_variance).exp() * noise. This is a part of the DDPM (Denoising Diffusion Probabilistic Models) framework for generating images.",
        "type": "comment"
    },
    "330": {
        "file_id": 6,
        "content": "        predict_v = False,\n        learned_variance = False,\n        clip_denoised = True,\n        lowres_cond_img = None,\n        text_encodings = None,\n        cond_scale = 1,\n        is_latent_diffusion = False,\n        lowres_noise_level = None,\n        inpaint_image = None,\n        inpaint_mask = None,\n        inpaint_resample_times = 5\n    ):\n        device = self.device\n        b = shape[0]\n        img = torch.randn(shape, device = device)\n        x_start = None # for self-conditioning\n        is_inpaint = exists(inpaint_image)\n        resample_times = inpaint_resample_times if is_inpaint else 1\n        if is_inpaint:\n            inpaint_image = self.normalize_img(inpaint_image)\n            inpaint_image = resize_image_to(inpaint_image, shape[-1], nearest = True)\n            inpaint_mask = rearrange(inpaint_mask, 'b h w -> b 1 h w').float()\n            inpaint_mask = resize_image_to(inpaint_mask, shape[-1], nearest = True)\n            inpaint_mask = inpaint_mask.bool()\n        if not is_latent_diffusion:",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2837-2866"
    },
    "331": {
        "file_id": 6,
        "content": "This function initializes image and related variables. If inpainting is present, it normalizes and resizes the image, mask, and sets their dimensions accordingly. The function also determines if the model is performing latent diffusion by checking for provided parameters. It then proceeds to an if-not condition where it assumes that the model is not performing latent diffusion.",
        "type": "comment"
    },
    "332": {
        "file_id": 6,
        "content": "            lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n        for time in tqdm(reversed(range(0, noise_scheduler.num_timesteps)), desc = 'sampling loop time step', total = noise_scheduler.num_timesteps):\n            is_last_timestep = time == 0\n            for r in reversed(range(0, resample_times)):\n                is_last_resample_step = r == 0\n                times = torch.full((b,), time, device = device, dtype = torch.long)\n                if is_inpaint:\n                    # following the repaint paper\n                    # https://arxiv.org/abs/2201.09865\n                    noised_inpaint_image = noise_scheduler.q_sample(inpaint_image, t = times)\n                    img = (img * ~inpaint_mask) + (noised_inpaint_image * inpaint_mask)\n                self_cond = x_start if unet.self_cond else None\n                img, x_start = self.p_sample(\n                    unet,\n                    img,\n                    times,\n                    image_embed = image_embed,\n                    text_encodings = text_encodings,",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2867-2890"
    },
    "333": {
        "file_id": 6,
        "content": "This code performs progressive growing of an image using a diffusion model, such as DALLE 2. It iterates over timesteps in reverse order and resamples each timestep to produce a final output image. It also includes the option for inpainting by following the Repaint paper's approach. The self-conditioning and U-Net are utilized within the p_sample function, which takes care of the actual sampling process.",
        "type": "comment"
    },
    "334": {
        "file_id": 6,
        "content": "                    cond_scale = cond_scale,\n                    self_cond = self_cond,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_level = lowres_noise_level,\n                    predict_x_start = predict_x_start,\n                    predict_v = predict_v,\n                    noise_scheduler = noise_scheduler,\n                    learned_variance = learned_variance,\n                    clip_denoised = clip_denoised\n                )\n                if is_inpaint and not (is_last_timestep or is_last_resample_step):\n                    # in repaint, you renoise and resample up to 10 times every step\n                    img = noise_scheduler.q_sample_from_to(img, times - 1, times)\n        if is_inpaint:\n            img = (img * ~inpaint_mask) + (inpaint_image * inpaint_mask)\n        unnormalize_img = self.unnormalize_img(img)\n        return unnormalize_img\n    @torch.no_grad()\n    def p_sample_loop_ddim(\n        self,\n        unet,\n        shape,\n        image_embed,",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2891-2917"
    },
    "335": {
        "file_id": 6,
        "content": "This code is part of a model that performs image denoising using diffusion models. It samples images at different timesteps, applies noise scheduling for resampling, and handles inpainting by combining input mask and image embeddings. The output is then unnormalized for the final result.",
        "type": "comment"
    },
    "336": {
        "file_id": 6,
        "content": "        noise_scheduler,\n        timesteps,\n        eta = 1.,\n        predict_x_start = False,\n        predict_v = False,\n        learned_variance = False,\n        clip_denoised = True,\n        lowres_cond_img = None,\n        text_encodings = None,\n        cond_scale = 1,\n        is_latent_diffusion = False,\n        lowres_noise_level = None,\n        inpaint_image = None,\n        inpaint_mask = None,\n        inpaint_resample_times = 5\n    ):\n        batch, device, total_timesteps, alphas, eta = shape[0], self.device, noise_scheduler.num_timesteps, noise_scheduler.alphas_cumprod, self.ddim_sampling_eta\n        times = torch.linspace(0., total_timesteps, steps = timesteps + 2)[:-1]\n        times = list(reversed(times.int().tolist()))\n        time_pairs = list(zip(times[:-1], times[1:]))\n        time_pairs = list(filter(lambda t: t[0] > t[1], time_pairs))\n        is_inpaint = exists(inpaint_image)\n        resample_times = inpaint_resample_times if is_inpaint else 1\n        if is_inpaint:\n            inpaint_image = self.normalize_img(inpaint_image)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2918-2946"
    },
    "337": {
        "file_id": 6,
        "content": "This function takes multiple parameters including noise_scheduler, timesteps, eta, and more. It extracts necessary information like batch size, device, total timesteps, alphas, and other parameters to perform DDIM sampling. It also checks if inpainting is required and resamples times accordingly.",
        "type": "comment"
    },
    "338": {
        "file_id": 6,
        "content": "            inpaint_image = resize_image_to(inpaint_image, shape[-1], nearest = True)\n            inpaint_mask = rearrange(inpaint_mask, 'b h w -> b 1 h w').float()\n            inpaint_mask = resize_image_to(inpaint_mask, shape[-1], nearest = True)\n            inpaint_mask = inpaint_mask.bool()\n        img = torch.randn(shape, device = device)\n        x_start = None # for self-conditioning\n        if not is_latent_diffusion:\n            lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n            is_last_timestep = time_next == 0\n            for r in reversed(range(0, resample_times)):\n                is_last_resample_step = r == 0\n                alpha = alphas[time]\n                alpha_next = alphas[time_next]\n                time_cond = torch.full((batch,), time, device = device, dtype = torch.long)\n                if is_inpaint:\n                    # following the repaint paper\n                    # https://arxiv.org/abs/2201.09865",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2947-2972"
    },
    "339": {
        "file_id": 6,
        "content": "The code is sampling from a diffusion model and applying inpainting. It resizes images, prepares masks for inpainting, sets up variables for time steps, and conditions the model based on inpainting or not. The code follows the process described in the Repaint paper (https://arxiv.org/abs/2201.09865).",
        "type": "comment"
    },
    "340": {
        "file_id": 6,
        "content": "                    noised_inpaint_image = noise_scheduler.q_sample(inpaint_image, t = time_cond)\n                    img = (img * ~inpaint_mask) + (noised_inpaint_image * inpaint_mask)\n                self_cond = x_start if unet.self_cond else None\n                unet_output = unet.forward_with_cond_scale(img, time_cond, image_embed = image_embed, text_encodings = text_encodings, cond_scale = cond_scale, self_cond = self_cond, lowres_cond_img = lowres_cond_img, lowres_noise_level = lowres_noise_level)\n                pred, _ = self.parse_unet_output(learned_variance, unet_output)\n                # predict x0\n                if predict_v:\n                    x_start = noise_scheduler.predict_start_from_v(img, t = time_cond, v = pred)\n                elif predict_x_start:\n                    x_start = pred\n                else:\n                    x_start = noise_scheduler.predict_start_from_noise(img, t = time_cond, noise = pred)\n                # maybe clip x0\n                if clip_denoised:\n                    x_start = self.dynamic_threshold(x_start)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2973-2994"
    },
    "341": {
        "file_id": 6,
        "content": "This code is using a conditional image generation model to generate an output image based on the input image, conditioning factors (time_cond, image_embed, text_encodings), and possibly predicting x0 values for further processing or clipping.",
        "type": "comment"
    },
    "342": {
        "file_id": 6,
        "content": "                # predict noise\n                pred_noise = noise_scheduler.predict_noise_from_start(img, t = time_cond, x0 = x_start)\n                c1 = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n                c2 = ((1 - alpha_next) - torch.square(c1)).sqrt()\n                noise = torch.randn_like(img) if not is_last_timestep else 0.\n                img = x_start * alpha_next.sqrt() + \\\n                      c1 * noise + \\\n                      c2 * pred_noise\n                if is_inpaint and not (is_last_timestep or is_last_resample_step):\n                    # in repaint, you renoise and resample up to 10 times every step\n                    time_next_cond = torch.full((batch,), time_next, device = device, dtype = torch.long)\n                    img = noise_scheduler.q_sample_from_to(img, time_next_cond, time_cond)\n        if exists(inpaint_image):\n            img = (img * ~inpaint_mask) + (inpaint_image * inpaint_mask)\n        img = self.unnormalize_img(img)\n        return img",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:2996-3017"
    },
    "343": {
        "file_id": 6,
        "content": "Predicts noise based on the current state and time, applies coefficients to noise and image, performs inpainting if necessary, and unnormalizes the image.",
        "type": "comment"
    },
    "344": {
        "file_id": 6,
        "content": "    @torch.no_grad()\n    def p_sample_loop(self, *args, noise_scheduler, timesteps = None, **kwargs):\n        num_timesteps = noise_scheduler.num_timesteps\n        timesteps = default(timesteps, num_timesteps)\n        assert timesteps <= num_timesteps\n        is_ddim = timesteps < num_timesteps\n        if not is_ddim:\n            return self.p_sample_loop_ddpm(*args, noise_scheduler = noise_scheduler, **kwargs)\n        return self.p_sample_loop_ddim(*args, noise_scheduler = noise_scheduler, timesteps = timesteps, **kwargs)\n    def p_losses(self, unet, x_start, times, *, image_embed, noise_scheduler, lowres_cond_img = None, text_encodings = None, predict_x_start = False, predict_v = False, noise = None, learned_variance = False, clip_denoised = False, is_latent_diffusion = False, lowres_noise_level = None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        # normalize to [-1, 1]\n        if not is_latent_diffusion:\n            x_start = self.normalize_img(x_start)\n            lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3019-3039"
    },
    "345": {
        "file_id": 6,
        "content": "Function `p_sample_loop` takes in arguments, determines if DDPM or DDIM should be used for sampling, and calls respective function.\nIn `p_losses`, noise is defaulted if not provided, and images are normalized before processing if not latent diffusion.",
        "type": "comment"
    },
    "346": {
        "file_id": 6,
        "content": "        # get x_t\n        x_noisy = noise_scheduler.q_sample(x_start = x_start, t = times, noise = noise)\n        # unet kwargs\n        unet_kwargs = dict(\n            image_embed = image_embed,\n            text_encodings = text_encodings,\n            lowres_cond_img = lowres_cond_img,\n            lowres_noise_level = lowres_noise_level,\n        )\n        # self conditioning\n        self_cond = None\n        if unet.self_cond and random.random() < 0.5:\n            with torch.no_grad():\n                unet_output = unet(x_noisy, times, **unet_kwargs)\n                self_cond, _ = self.parse_unet_output(learned_variance, unet_output)\n                self_cond = self_cond.detach()\n        # forward to get model prediction\n        unet_output = unet(\n            x_noisy,\n            times,\n            **unet_kwargs,\n            self_cond = self_cond,\n            image_cond_drop_prob = self.image_cond_drop_prob,\n            text_cond_drop_prob = self.text_cond_drop_prob,\n        )\n        pred, _ = self.parse_unet_output(learned_variance, unet_output)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3041-3075"
    },
    "347": {
        "file_id": 6,
        "content": "Code snippet is from the DALLE2-pytorch model. It samples noisy images and uses them to conditionally generate unet outputs for self-conditioning and prediction, with optional dropout probabilities for image and text conditions.",
        "type": "comment"
    },
    "348": {
        "file_id": 6,
        "content": "        if predict_v:\n            target = noise_scheduler.calculate_v(x_start, times, noise)\n        elif predict_x_start:\n            target = x_start\n        else:\n            target = noise\n        loss = noise_scheduler.loss_fn(pred, target, reduction = 'none')\n        loss = reduce(loss, 'b ... -> b (...)', 'mean')\n        loss = noise_scheduler.p2_reweigh_loss(loss, times)\n        loss = loss.mean()\n        if not learned_variance:\n            # return simple loss if not using learned variance\n            return loss\n        # most of the code below is transcribed from\n        # https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/diffusion_utils_2.py\n        # the Improved DDPM paper then further modified it so that the mean is detached (shown a couple lines before), and weighted to be smaller than the l1 or l2 \"simple\" loss\n        # it is questionable whether this is really needed, looking at some of the figures in the paper, but may as well stay faithful to their implementation",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3077-3098"
    },
    "349": {
        "file_id": 6,
        "content": "The code calculates the loss in a specific manner depending on the input parameters. If predict_v is true, it calculates the target value for v. If predict_x_start is true, it uses x_start as the target. Otherwise, it uses noise as the target. Then, it applies the loss function, reduces the loss, reweighs the loss based on times, and finally calculates the mean of the loss. If learned_variance is not used, it returns the simple loss.",
        "type": "comment"
    },
    "350": {
        "file_id": 6,
        "content": "        # if learning the variance, also include the extra weight kl loss\n        true_mean, _, true_log_variance_clipped = noise_scheduler.q_posterior(x_start = x_start, x_t = x_noisy, t = times)\n        model_mean, _, model_log_variance, _ = self.p_mean_variance(unet, x = x_noisy, t = times, image_embed = image_embed, noise_scheduler = noise_scheduler, clip_denoised = clip_denoised, learned_variance = True, model_output = unet_output)\n        # kl loss with detached model predicted mean, for stability reasons as in paper\n        detached_model_mean = model_mean.detach()\n        kl = normal_kl(true_mean, true_log_variance_clipped, detached_model_mean, model_log_variance)\n        kl = meanflat(kl) * NAT\n        decoder_nll = -discretized_gaussian_log_likelihood(x_start, means = detached_model_mean, log_scales = 0.5 * model_log_variance)\n        decoder_nll = meanflat(decoder_nll) * NAT\n        # at the first timestep return the decoder NLL, otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3100-3115"
    },
    "351": {
        "file_id": 6,
        "content": "This code calculates the KL divergence between true and model predicted posterior distributions, and decoder negative log likelihood. It uses detached model predictions for stability reasons as per the paper. The loss at the first timestep is the decoder NLL, otherwise it's the KL divergence.",
        "type": "comment"
    },
    "352": {
        "file_id": 6,
        "content": "        vb_losses = torch.where(times == 0, decoder_nll, kl)\n        # weight the vb loss smaller, for stability, as in the paper (recommended 0.001)\n        vb_loss = vb_losses.mean() * self.vb_loss_weight\n        return loss + vb_loss\n    @torch.no_grad()\n    @eval_decorator\n    def sample(\n        self,\n        image = None,\n        image_embed = None,\n        text = None,\n        text_encodings = None,\n        batch_size = 1,\n        cond_scale = 1.,\n        start_at_unet_number = 1,\n        stop_at_unet_number = None,\n        distributed = False,\n        inpaint_image = None,\n        inpaint_mask = None,\n        inpaint_resample_times = 5,\n        one_unet_in_gpu_at_time = True\n    ):\n        assert self.unconditional or exists(image_embed), 'image embed must be present on sampling from decoder unless if trained unconditionally'\n        if not self.unconditional:\n            batch_size = image_embed.shape[0]\n        if exists(text) and not exists(text_encodings) and not self.unconditional:\n            assert exists(self.clip)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3117-3149"
    },
    "353": {
        "file_id": 6,
        "content": "This function calculates the variational Bayes loss and adds it to the main loss. It then samples from the model given input parameters such as image, text, batch size, etc., with option for conditional or unconditional sampling. The function also performs some assertions on the inputs to ensure proper usage.",
        "type": "comment"
    },
    "354": {
        "file_id": 6,
        "content": "            _, text_encodings = self.clip.embed_text(text)\n        assert not (self.condition_on_text_encodings and not exists(text_encodings)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text_encodings and exists(text_encodings)), 'decoder specified not to be conditioned on text, yet it is presented'\n        assert not (exists(inpaint_image) ^ exists(inpaint_mask)), 'inpaint_image and inpaint_mask (boolean mask of [batch, height, width]) must be both given for inpainting'\n        img = None\n        if start_at_unet_number > 1:\n            # Then we are not generating the first image and one must have been passed in\n            assert exists(image), 'image must be passed in if starting at unet number > 1'\n            assert image.shape[0] == batch_size, 'image must have batch size of {} if starting at unet number > 1'.format(batch_size)\n            prev_unet_output_size = self.image_sizes[start_at_unet_number - 2]\n            img = resize_image_to(image, prev_unet_output_size, nearest = True)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3150-3163"
    },
    "355": {
        "file_id": 6,
        "content": "This code checks for valid inputs and asserts whether text, text encodings, or inpaint_image and mask are present based on the condition specified. It also ensures that the image input has the correct batch size when starting at a specific unet number. If necessary, it resizes the image using nearest-neighbor interpolation.",
        "type": "comment"
    },
    "356": {
        "file_id": 6,
        "content": "        is_cuda = next(self.parameters()).is_cuda\n        num_unets = self.num_unets\n        cond_scale = cast_tuple(cond_scale, num_unets)\n        for unet_number, unet, vae, channel, image_size, predict_x_start, predict_v, learned_variance, noise_scheduler, lowres_cond, sample_timesteps, unet_cond_scale in tqdm(zip(range(1, num_unets + 1), self.unets, self.vaes, self.sample_channels, self.image_sizes, self.predict_x_start, self.predict_v, self.learned_variance, self.noise_schedulers, self.lowres_conds, self.sample_timesteps, cond_scale)):\n            if unet_number < start_at_unet_number:\n                continue  # It's the easiest way to do it\n            context = self.one_unet_in_gpu(unet = unet) if is_cuda and one_unet_in_gpu_at_time else null_context()\n            with context:\n                # prepare low resolution conditioning for upsamplers\n                lowres_cond_img = lowres_noise_level = None\n                shape = (batch_size, channel, image_size, image_size)\n                if unet.lowres_cond:",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3165-3182"
    },
    "357": {
        "file_id": 6,
        "content": "This code is iterating through each unet in the model, skipping the first X unets based on a given parameter. It checks if the current unet should be processed based on its position, and then prepares low resolution conditioning for upsamplers if required. The code also handles CUDA processing and uses context managers to ensure efficient resource usage.",
        "type": "comment"
    },
    "358": {
        "file_id": 6,
        "content": "                    lowres_cond_img = resize_image_to(img, target_image_size = image_size, clamp_range = self.input_image_range, nearest = True)\n                    if lowres_cond.use_noise:\n                        lowres_noise_level = torch.full((batch_size,), int(self.lowres_noise_sample_level * 1000), dtype = torch.long, device = self.device)\n                        lowres_cond_img, _ = lowres_cond.noise_image(lowres_cond_img, lowres_noise_level)\n                # latent diffusion\n                is_latent_diffusion = isinstance(vae, VQGanVAE)\n                image_size = vae.get_encoded_fmap_size(image_size)\n                shape = (batch_size, vae.encoded_dim, image_size, image_size)\n                lowres_cond_img = maybe(vae.encode)(lowres_cond_img)\n                # denoising loop for image\n                img = self.p_sample_loop(\n                    unet,\n                    shape,\n                    image_embed = image_embed,\n                    text_encodings = text_encodings,\n                    cond_scale = unet_cond_scale,",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3183-3204"
    },
    "359": {
        "file_id": 6,
        "content": "This code is part of a denoising diffusion model. It first resizes the input image to a target size and applies noise if needed. Then, it checks if the VAE (Variational Autoencoder) is used for latent diffusion and adjusts the image size accordingly. Finally, it encodes the low-resolution image using the VAE and enters a denoising loop with a UNet model to generate the final output image.",
        "type": "comment"
    },
    "360": {
        "file_id": 6,
        "content": "                    predict_x_start = predict_x_start,\n                    predict_v = predict_v,\n                    learned_variance = learned_variance,\n                    clip_denoised = not is_latent_diffusion,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_level = lowres_noise_level,\n                    is_latent_diffusion = is_latent_diffusion,\n                    noise_scheduler = noise_scheduler,\n                    timesteps = sample_timesteps,\n                    inpaint_image = inpaint_image,\n                    inpaint_mask = inpaint_mask,\n                    inpaint_resample_times = inpaint_resample_times\n                )\n                img = vae.decode(img)\n            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n                break\n        return img\n    def forward(\n        self,\n        image,\n        text = None,\n        image_embed = None,\n        text_encodings = None,\n        unet_number = None,\n        return_lowres_",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3205-3233"
    },
    "361": {
        "file_id": 6,
        "content": "The function takes an image and optionally text, generates images at different UNet resolutions based on input parameters, and returns the generated image. It includes options for low-resolution output, inpainting, and stopping at a specific UNet resolution.",
        "type": "comment"
    },
    "362": {
        "file_id": 6,
        "content": "cond_image = False # whether to return the low resolution conditioning images, for debugging upsampler purposes\n    ):\n        assert not (self.num_unets > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {self.num_unets}, if you are training cascading DDPM (multiple unets)'\n        unet_number = default(unet_number, 1)\n        unet_index = unet_number - 1\n        unet = self.get_unet(unet_number)\n        vae                 = self.vaes[unet_index]\n        noise_scheduler     = self.noise_schedulers[unet_index]\n        lowres_conditioner  = self.lowres_conds[unet_index]\n        target_image_size   = self.image_sizes[unet_index]\n        predict_x_start     = self.predict_x_start[unet_index]\n        predict_v           = self.predict_v[unet_index]\n        random_crop_size    = self.random_crop_sizes[unet_index]\n        learned_variance    = self.learned_variance[unet_index]\n        b, c, h, w, device, = *image.shape, image.device\n        assert image.shape[1] == self.channels",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3233-3251"
    },
    "363": {
        "file_id": 6,
        "content": "This function is initializing variables for a specific U-Net in the model, based on the provided unet_number. It assigns the corresponding U-Net, VAE, noise scheduler, lowres conditioner, target image size, predict x start, predict v, random crop size, and learned variance from predefined lists for that U-Net index. It also ensures the image shape aligns with the expected number of channels.",
        "type": "comment"
    },
    "364": {
        "file_id": 6,
        "content": "        assert h >= target_image_size and w >= target_image_size\n        times = torch.randint(0, noise_scheduler.num_timesteps, (b,), device = device, dtype = torch.long)\n        if not exists(image_embed) and not self.unconditional:\n            assert exists(self.clip), 'if you want to derive CLIP image embeddings automatically, you must supply `clip` to the decoder on init'\n            image_embed, _ = self.clip.embed_image(image)\n        if exists(text) and not exists(text_encodings) and not self.unconditional:\n            assert exists(self.clip), 'if you are passing in raw text, you need to supply `clip` to the decoder'\n            _, text_encodings = self.clip.embed_text(text)\n        assert not (self.condition_on_text_encodings and not exists(text_encodings)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text_encodings and exists(text_encodings)), 'decoder specified not to be conditioned on text, yet it is presented'\n        ",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3252-3267"
    },
    "365": {
        "file_id": 6,
        "content": "The code checks if the image and/or text inputs exist, ensuring that either the CLIP model or the necessary inputs are present. It asserts that if the decoder is supposed to be conditioned on text encodings, then the text encodings must be provided, and vice versa. This helps prevent errors in the input data for generating image embeddings.",
        "type": "comment"
    },
    "366": {
        "file_id": 6,
        "content": "lowres_cond_img, lowres_noise_level = lowres_conditioner(image, target_image_size = target_image_size, downsample_image_size = self.image_sizes[unet_index - 1]) if exists(lowres_conditioner) else (None, None)\n        image = resize_image_to(image, target_image_size, nearest = True)\n        if exists(random_crop_size):\n            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n            # make sure low res conditioner and image both get augmented the same way\n            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n            image = aug(image)\n            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n        is_latent_diffusion = not isinstance(vae, NullVQGanVAE)\n        vae.eval()\n        with torch.no_grad():\n            image = vae.encode(image)\n            lowres_cond_img = maybe(vae.encode)(lowres_cond_img)\n        losses = self.p_losses(unet, image, times, image_embed = image",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3267-3285"
    },
    "367": {
        "file_id": 6,
        "content": "This code snippet is conditioning a low-resolution image using the lowres_conditioner and performing data augmentation via Kornia's RandomCrop. It also encodes both the image and the conditioned image using a VAE (Variational Autoencoder) and calculates loss from p_losses for further processing in the U-net model.",
        "type": "comment"
    },
    "368": {
        "file_id": 6,
        "content": "_embed, text_encodings = text_encodings, lowres_cond_img = lowres_cond_img, predict_x_start = predict_x_start, predict_v = predict_v, learned_variance = learned_variance, is_latent_diffusion = is_latent_diffusion, noise_scheduler = noise_scheduler, lowres_noise_level = lowres_noise_level)\n        if not return_lowres_cond_image:\n            return losses\n        return losses, lowres_cond_img\n# main class\nclass DALLE2(nn.Module):\n    def __init__(\n        self,\n        *,\n        prior,\n        decoder,\n        prior_num_samples = 2\n    ):\n        super().__init__()\n        assert isinstance(prior, DiffusionPrior)\n        assert isinstance(decoder, Decoder)\n        self.prior = prior\n        self.decoder = decoder\n        self.prior_num_samples = prior_num_samples\n        self.decoder_need_text_cond = self.decoder.condition_on_text_encodings\n        self.to_pil = T.ToPILImage()\n    @torch.no_grad()\n    @eval_decorator\n    def forward(\n        self,\n        text,\n        cond_scale = 1.,\n        prior_cond_scale = 1.,",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3285-3319"
    },
    "369": {
        "file_id": 6,
        "content": "This code defines a DALLE2 class with prior and decoder modules. It takes text input, performs diffusion, and returns losses or lowres_cond_img based on the return flag. If not returning the lowres conditional image, it returns only losses.",
        "type": "comment"
    },
    "370": {
        "file_id": 6,
        "content": "        return_pil_images = False\n    ):\n        device = module_device(self)\n        one_text = isinstance(text, str) or (not is_list_str(text) and text.shape[0] == 1)\n        if isinstance(text, str) or is_list_str(text):\n            text = [text] if not isinstance(text, (list, tuple)) else text\n            text = tokenizer.tokenize(text).to(device)\n        image_embed = self.prior.sample(text, num_samples_per_batch = self.prior_num_samples, cond_scale = prior_cond_scale)\n        text_cond = text if self.decoder_need_text_cond else None\n        images = self.decoder.sample(image_embed = image_embed, text = text_cond, cond_scale = cond_scale)\n        if return_pil_images:\n            images = list(map(self.to_pil, images.unbind(dim = 0)))\n        if one_text:\n            return first(images)\n        return images",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:3320-3340"
    },
    "371": {
        "file_id": 6,
        "content": "This function takes text as input, tokenizes it if necessary, and uses a prior model to generate image embeddings. It then passes these embeddings along with the text (if required) to a decoder model to generate images. Optionally, it converts the images to PIL format and returns them. If only one text is given, it returns the first generated image.",
        "type": "comment"
    },
    "372": {
        "file_id": 7,
        "content": "/dalle2_pytorch/dataloaders/README.md",
        "type": "filepath"
    },
    "373": {
        "file_id": 7,
        "content": "The code creates a dataloader for image embedding datasets and sets up training, evaluation, and testing splits for three ranks using the provided config TRAIN_ARGS. It uses img2dataset, clip-retrieval, and embedding-dataset-reordering tools to load images and embeddings without resampling.",
        "type": "summary"
    },
    "374": {
        "file_id": 7,
        "content": "## Dataloaders\nIn order to make loading data simple and efficient, we include some general dataloaders that can be used to train portions of the network.\n### Decoder: Image Embedding Dataset\nWhen training the decoder (and up samplers if training together) in isolation, you will need to load images and corresponding image embeddings. This dataset can read two similar types of datasets. First, it can read a [webdataset](https://github.com/webdataset/webdataset) that contains `.jpg` and `.npy` files in the `.tar`s that contain the images and associated image embeddings respectively. Alternatively, you can also specify a source for the embeddings outside of the webdataset. In this case, the path to the embeddings should contain `.npy` files with the same shard numbers as the webdataset and there should be a correspondence between the filename of the `.jpg` and the index of the embedding in the `.npy`. So, for example, `0001.tar` from the webdataset with image `00010509.jpg` (the first 4 digit",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/README.md:1-5"
    },
    "375": {
        "file_id": 7,
        "content": "This code snippet describes the usage of general dataloaders for efficient data loading and training portions of the network, particularly focusing on the decoder. It supports two types of datasets: a webdataset containing .jpg and .npy files in .tar formats or an external source where .npy files correspond to .jpg filenames from the webdataset.",
        "type": "comment"
    },
    "376": {
        "file_id": 7,
        "content": "s are the shard number and the last 4 are the index) in it should be paralleled by a `img_emb_0001.npy` which contains a NumPy array with the embedding at index 509.\nGenerating a dataset of this type:\n1. Use [img2dataset](https://github.com/rom1504/img2dataset) to generate a webdataset.\n2. Use [clip-retrieval](https://github.com/rom1504/clip-retrieval) to convert the images to embeddings.\n3. Use [embedding-dataset-reordering](https://github.com/Veldrovive/embedding-dataset-reordering) to reorder the embeddings into the expected format.\nUsage:\n```python\nfrom dalle2_pytorch.dataloaders import ImageEmbeddingDataset, create_image_embedding_dataloader\n# Create a dataloader directly.\ndataloader = create_image_embedding_dataloader(\n    tar_url=\"/path/or/url/to/webdataset/{0000..9999}.tar\", # Uses bracket expanding notation. This specifies to read all tars from 0000.tar to 9999.tar\n    embeddings_url=\"path/or/url/to/embeddings/folder\",     # Included if .npy files are not in webdataset. Left out or set to None otherwise",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/README.md:5-19"
    },
    "377": {
        "file_id": 7,
        "content": "This code demonstrates how to create a dataloader for an image embedding dataset. It utilizes three separate tools: img2dataset, clip-retrieval, and embedding-dataset-reordering. The user must provide the appropriate URLs for the webdataset and embeddings folder in order to generate the dataloader. The code snippet also highlights the usage of create_image_embedding_dataloader function which takes in URL parameters and returns a dataloader object.",
        "type": "comment"
    },
    "378": {
        "file_id": 7,
        "content": "    num_workers=4,\n    batch_size=32,\n    shard_width=4,                                         # If a file in the webdataset shard 3 is named 0003039.jpg, we know the shard width is 4 and the last three digits are the index\n    shuffle_num=200,                                       # Does a shuffle of the data with a buffer size of 200\n    shuffle_shards=True,                                   # Shuffle the order the shards are read in\n    resample_shards=False,                                 # Sample shards with replacement. If true, an epoch will be infinite unless stopped manually\n)\nfor img, emb in dataloader:\n    print(img.shape)  # torch.Size([32, 3, 256, 256])\n    print(emb.shape)  # torch.Size([32, 512])\n    # Train decoder only as shown above\n# Or create a dataset without a loader so you can configure it manually\ndataset = ImageEmbeddingDataset(\n    urls=\"/path/or/url/to/webdataset/{0000..9999}.tar\",\n    embedding_folder_url=\"path/or/url/to/embeddings/folder\",\n    shard_width=4,\n    shuffle_shards=True,",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/README.md:20-37"
    },
    "379": {
        "file_id": 7,
        "content": "This code initializes a dataloader with parameters such as number of workers, batch size, shard width, and shuffle settings. It loads images and their corresponding embeddings from webdataset files. The images' shapes are printed for a single epoch. An ImageEmbeddingDataset is also created without a loader for manual configuration.",
        "type": "comment"
    },
    "380": {
        "file_id": 7,
        "content": "    resample=False\n)\n```\n### Diffusion Prior: Prior Embedding Dataset\nWhen training the prior it is much more efficient to work with pre-computed embeddings. The `PriorEmbeddingDataset` class enables you to leverage the same script (with minimal modification) for both embedding-only and text-conditioned prior training. This saves you from having to worry about a lot of the boilerplate code.\nTo utilize the `PriorEmbeddingDataset`, all you need to do is make a single call to `get_reader()` which will create `EmbeddingReader` object(s) for you. Afterwards, you can utilize `make_splits()` to cleanly create DataLoader objects from for your training run.\nIf you are training in a distributed manner, `make_splits()` accepts `rank` and `world_size` arguments to properly distribute to each process. The defaults for these values are `rank=0` and `world_size=1`, so single-process training can safely ignore these parameters.\nUsage:\n```python\nfrom dalle2_pytorch.dataloaders import get_reader, make_splits\n# grab embeddings from some specified location",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/README.md:38-53"
    },
    "381": {
        "file_id": 7,
        "content": "The `resample=False` argument is used to disable resampling when processing the embeddings in the Prior Embedding Dataset. This ensures that the embeddings are not recomputed and can be efficiently used for both embedding-only and text-conditioned prior training.",
        "type": "comment"
    },
    "382": {
        "file_id": 7,
        "content": "IMG_URL = \"data/img_emb/\"\nMETA_URL = \"data/meta/\"\nreader = get_reader(text_conditioned=True, img_url=IMG_URL, meta_url=META_URL)\n# some config for training\nTRAIN_ARGS = {\n    \"world_size\": 3,\n    \"text_conditioned\": True,\n    \"start\": 0,\n    \"num_data_points\": 10000,\n    \"batch_size\": 2,\n    \"train_split\": 0.5,\n    \"eval_split\": 0.25,\n    \"image_reader\": reader,\n}\n# specifying a rank will handle allocation internally\nrank0_train, rank0_eval, rank0_test = make_splits(rank=0, **TRAIN_ARGS)\nrank1_train, rank1_eval, rank1_test = make_splits(rank=1, **TRAIN_ARGS)\nrank2_train, rank2_eval, rank2_test = make_splits(rank=2, **TRAIN_ARGS)\n```",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/README.md:54-75"
    },
    "383": {
        "file_id": 7,
        "content": "The code sets up training, evaluation, and testing splits for three different ranks (0, 1, 2) using the provided config TRAIN_ARGS. It uses the get_reader function to load image and metadata from specified URLs, and the make_splits function to divide the data into train, eval, and test sets for distributed training.",
        "type": "comment"
    },
    "384": {
        "file_id": 8,
        "content": "/dalle2_pytorch/dataloaders/__init__.py",
        "type": "filepath"
    },
    "385": {
        "file_id": 8,
        "content": "This code imports necessary classes for ImageEmbeddingDataset and PriorEmbeddingDataset from their respective modules in the DALLE2-pytorch library. These datasets are used to load data for the model's training and inference.",
        "type": "summary"
    },
    "386": {
        "file_id": 8,
        "content": "from dalle2_pytorch.dataloaders.decoder_loader import ImageEmbeddingDataset, create_image_embedding_dataloader\nfrom dalle2_pytorch.dataloaders.prior_loader import make_splits, get_reader, PriorEmbeddingDataset",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/__init__.py:1-2"
    },
    "387": {
        "file_id": 8,
        "content": "This code imports necessary classes for ImageEmbeddingDataset and PriorEmbeddingDataset from their respective modules in the DALLE2-pytorch library. These datasets are used to load data for the model's training and inference.",
        "type": "comment"
    },
    "388": {
        "file_id": 9,
        "content": "/dalle2_pytorch/dataloaders/decoder_loader.py",
        "type": "filepath"
    },
    "389": {
        "file_id": 9,
        "content": "The code defines functions for retrieving embeddings, combining image and text embeddings, creating image embedding datasets, and handling exceptions in webdataset tar files. It also includes support for preprocessing, resampling, shuffling, package checks, and dataloaders.",
        "type": "summary"
    },
    "390": {
        "file_id": 9,
        "content": "import os\nimport webdataset as wds\nimport torch\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport fsspec\nimport shutil\ndef get_shard(filename):\n    \"\"\"\n    Filenames with shards in them have a consistent structure that we can take advantage of\n    Standard structure: path/to/file/prefix_string_00001.ext\n    \"\"\"\n    try:\n        return filename.split(\"_\")[-1].split(\".\")[0]\n    except ValueError:\n        raise RuntimeError(f\"Could not find shard for filename {filename}\")\ndef get_example_file(fs, path, file_format):\n    \"\"\"\n    Given a file system and a file extension, return the example file\n    \"\"\"\n    return fs.glob(os.path.join(path, f\"*.{file_format}\"))[0]\ndef embedding_inserter(samples, embeddings_url, index_width, sample_key='npy', handler=wds.handlers.reraise_exception):\n    \"\"\"Given a datum of {\"__key__\": str, \"__url__\": str, ...} adds the cooresponding embedding and yields\"\"\"\n    previous_tar_url = None\n    current_embeddings = None\n    # Get a reference to an abstract file system where the embeddings are stored",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:1-29"
    },
    "391": {
        "file_id": 9,
        "content": "This code defines three functions: `get_shard`, `get_example_file`, and `embedding_inserter`. The first function extracts the shard number from a filename. The second function returns an example file given a file system and a file format. Lastly, the third function inserts embeddings into a dataset, given samples, embedding URL, index width, sample key, and a handler to handle exceptions.",
        "type": "comment"
    },
    "392": {
        "file_id": 9,
        "content": "    embeddings_fs, embeddings_path = fsspec.core.url_to_fs(embeddings_url)\n    example_embedding_file = get_example_file(embeddings_fs, embeddings_path, \"npy\")\n    example_embedding_shard = get_shard(example_embedding_file)\n    emb_shard_width = len(example_embedding_shard)\n    # Easier to get the basename without the shard once than search through for the correct file every time\n    embedding_file_basename = '_'.join(example_embedding_file.split(\"_\")[:-1]) + \"_\"\n    def load_corresponding_embeds(tar_url):\n      \"\"\"Finds and reads the npy files that contains embeddings for the given webdataset tar\"\"\"\n      shard = int(tar_url.split(\"/\")[-1].split(\".\")[0])\n      embedding_url = embedding_file_basename + str(shard).zfill(emb_shard_width) + '.npy'\n      with embeddings_fs.open(embedding_url) as f:\n        data = np.load(f)\n      return torch.from_numpy(data)\n    for sample in samples:\n        try:\n            tar_url = sample[\"__url__\"]\n            key = sample[\"__key__\"]\n            if tar_url != previous_tar_url:",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:30-49"
    },
    "393": {
        "file_id": 9,
        "content": "This code segment retrieves and loads embeddings from a webdataset tar file using the given URL. It identifies the correct npy file containing the embeddings by extracting the shard number from the URL, then opens and loads the data into a torch tensor.",
        "type": "comment"
    },
    "394": {
        "file_id": 9,
        "content": "                # If the tar changed, we need to download new embeddings\n                # This means if we shuffle before inserting it will load many more files than we expect and be very inefficient.\n                previous_tar_url = tar_url\n                current_embeddings = load_corresponding_embeds(tar_url)\n            embedding_index = int(key[-index_width:])\n            embedding = current_embeddings[embedding_index]\n            # We need to check if this sample is nonzero. If it is, this embedding is not valid and we should continue to the next loop\n            if torch.count_nonzero(embedding) == 0:\n                raise RuntimeError(f\"Webdataset had a sample, but no embedding was found. ImgShard: {key[:-index_width]} - Index: {key[-index_width:]}\")\n            sample[sample_key] = embedding\n            yield sample\n        except Exception as exn:  # From wds implementation\n            if handler(exn):\n                continue\n            else:\n                break\ninsert_embedding = wds.filters.pipelinefilter(embedding_inserter)",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:50-67"
    },
    "395": {
        "file_id": 9,
        "content": "The code checks if a tar file changed and loads corresponding embeddings. If the sample has no embedding, it raises an error. The insert_embedding variable is assigned a pipeline filter with the embedding inserter function.",
        "type": "comment"
    },
    "396": {
        "file_id": 9,
        "content": "def unassociated_shard_skipper(tarfiles, embeddings_url, handler=wds.handlers.reraise_exception):\n    \"\"\"Finds if the is a corresponding embedding for the tarfile at { url: [URL] }\"\"\"\n    embeddings_fs, embeddings_path = fsspec.core.url_to_fs(embeddings_url)\n    embedding_files = embeddings_fs.ls(embeddings_path)\n    get_embedding_shard = lambda embedding_file: int(embedding_file.split(\"_\")[-1].split(\".\")[0])\n    embedding_shards = set([get_embedding_shard(filename) for filename in embedding_files])  # Sets have O(1) check for member\n    get_tar_shard = lambda tar_file: int(tar_file.split(\"/\")[-1].split(\".\")[0])\n    for tarfile in tarfiles:\n        try:\n            webdataset_shard = get_tar_shard(tarfile[\"url\"])\n            # If this shard has an associated embeddings file, we pass it through. Otherwise we iterate until we do have one\n            if webdataset_shard in embedding_shards:\n                yield tarfile\n        except Exception as exn:  # From wds implementation\n            if handler(exn):",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:69-84"
    },
    "397": {
        "file_id": 9,
        "content": "This function checks if there are corresponding embeddings for the given tarfiles. It first retrieves a set of embedding shards from the embeddings_url, then iterates through the tarfiles. If a tarfile's shard is in the set of embedding shards, it yields the tarfile. Otherwise, it will continue to iterate until it finds a matching shard. Exceptions are handled using the provided handler function.",
        "type": "comment"
    },
    "398": {
        "file_id": 9,
        "content": "                continue\n            else:\n                break\nskip_unassociated_shards = wds.filters.pipelinefilter(unassociated_shard_skipper)\ndef join_embeddings(samples, handler=wds.handlers.reraise_exception):\n    \"\"\"\n    Takes the img_emb and text_emb keys and turns them into one key \"emb\": { \"text\": text_emb, \"img\": img_emb }\n    either or both of text_emb and img_emb may not be in the sample so we only add the ones that exist\n    \"\"\"\n    for sample in samples:\n        try:\n            sample['emb'] = {}\n            if 'text_emb' in sample:\n                sample['emb']['text'] = sample['text_emb']\n            if 'img_emb' in sample:\n                sample['emb']['img'] = sample['img_emb']\n            yield sample\n        except Exception as exn:  # From wds implementation\n            if handler(exn):\n                continue\n            else:\n                break\ndef verify_keys(samples, required_keys, handler=wds.handlers.reraise_exception):\n    \"\"\"\n    Requires that both the image and embedding are present in the sample",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:85-111"
    },
    "399": {
        "file_id": 9,
        "content": "The code defines two functions: `join_embeddings()` and `verify_keys()`. The first function combines the `img_emb` and `text_emb` keys into a single \"emb\" key in each sample, only including existing embeddings. The second function ensures that both image and embedding are present in each sample. If not, it either continues or breaks depending on the exception handler.",
        "type": "comment"
    }
}