{
    "400": {
        "file_id": 9,
        "content": "    This is important to do as a user may forget they do not have embeddings in their webdataset and neglect to add them using the embedding_folder_url parameter.\n    \"\"\"\n    for sample in samples:\n        try:\n            for key in required_keys:\n                assert key in sample, f\"Sample {sample['__key__']} missing {key}. Has keys {sample.keys()}\"\n            yield sample\n        except Exception as exn:  # From wds implementation\n            if handler(exn):\n                continue\n            else:\n                break\nkey_verifier = wds.filters.pipelinefilter(verify_keys)\nclass ImageEmbeddingDataset(wds.DataPipeline, wds.compat.FluidInterface):\n    \"\"\"\n    A fluid interface wrapper for DataPipline that returns image embedding pairs\n    Reads embeddings as npy files from the webdataset if they exist. If embedding_folder_url is set, they will be inserted in from the alternate source.\n    \"\"\"\n    def __init__(\n            self,\n            urls,\n            img_embedding_folder_url=None,\n            text_embedding_folder_url=None,",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:112-136"
    },
    "401": {
        "file_id": 9,
        "content": "This code checks if required keys are present in each sample, asserts if missing and yields the sample. It uses a key_verifier filter and a fluid interface for DataPipeline to return image embedding pairs. Embeddings can be read from webdataset or inserted from an alternate source based on embedding_folder_url.",
        "type": "comment"
    },
    "402": {
        "file_id": 9,
        "content": "            index_width=None,\n            img_preproc=None,\n            extra_keys=[],\n            handler=wds.handlers.reraise_exception,\n            resample=False,\n            shuffle_shards=True\n    ):\n        \"\"\"\n        Modeled directly off of the WebDataset constructor\n        :param urls: A url pointing to the tar files of the webdataset formatted as /path/to/webdataset/{0000..9999}.tar\n        :param embedding_folder_url: Required if webdataset does not contain embeddings. A url pointing to the npy files of the embeddings. Should have the same number of shards as the webdataset.\n            Webdataset image keys should align with the index of the embedding. This means missing image indices must have a corresponding embedding of all zeros.\n        :param index_width: The number of digits in the index. This is used to align the embedding index with the image index.\n            For example, if a file in the webdataset shard 3 is named 0003039.jpg, we know the shard is 4 digits and the last 3 digits are the index_width.",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:137-151"
    },
    "403": {
        "file_id": 9,
        "content": "The code defines a function to load data from webdatasets and embeddings for a model. It takes URLs as input, where each URL points to tar files of the webdataset. If embeddings are not included in the dataset, an embedding_folder_URL is required. The index width specifies the number of digits in the index, used to align image and embedding indices. The handler handles exceptions, while resample can be set for resampling data. The shuffle_shards flag determines whether to shuffle shards during loading.",
        "type": "comment"
    },
    "404": {
        "file_id": 9,
        "content": "        :param img_preproc: This function is run on the img before it is batched and returned. Useful for data augmentation or converting to torch tensor.\n        :param handler: A webdataset handler.\n        :param resample: If true, resample webdataset shards with replacement. You need to set your own epoch size if this is true since it will resample infinitely.\n        :param shuffle_shards: If true, shuffle the shards before resampling. This cannot be true if resample is true.\n        \"\"\"\n        super().__init__()\n        keys = [\"jpg\", \"emb\"] + extra_keys\n        # if img_embedding_folder_url is not None:\n        #     keys.append(\"img_emb\")\n        # if text_embedding_folder_url is not None:\n        #     keys.append(\"text_emb\")\n        # keys.extend(extra_keys)\n        self.key_map = {key: i for i, key in enumerate(keys)}\n        self.resampling = resample\n        self.img_preproc = img_preproc\n        # If s3, check if s3fs is installed and s3cmd is installed and check if the data is piped instead of straight up",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:152-169"
    },
    "405": {
        "file_id": 9,
        "content": "This function is a webdataset handler that takes parameters for img_preproc, resample, and shuffle_shards. It initializes the keys for data loading and maps them to their respective indices. If img_embedding_folder_url or text_embedding_folder_url is not None, \"img_emb\" and \"text_emb\" will be added as keys. The function also checks if s3fs and s3cmd are installed, and handles data piping.",
        "type": "comment"
    },
    "406": {
        "file_id": 9,
        "content": "        if (isinstance(urls, str) and \"s3:\" in urls) or (isinstance(urls, list) and any([\"s3:\" in url for url in urls])):\n            # Then this has an s3 link for the webdataset and we need extra packages\n            if shutil.which(\"s3cmd\") is None:\n                raise RuntimeError(\"s3cmd is required for s3 webdataset\")\n        if (img_embedding_folder_url is not None and \"s3:\" in img_embedding_folder_url) or (text_embedding_folder_url is not None and \"s3:\" in text_embedding_folder_url):\n            # Then the embeddings are being loaded from s3 and fsspec requires s3fs\n            try:\n                import s3fs\n            except ImportError:\n                raise RuntimeError(\"s3fs is required to load embeddings from s3\")\n        # Add the shardList and randomize or resample if requested\n        if resample:\n            assert not shuffle_shards, \"Cannot both resample and shuffle\"\n            self.append(wds.ResampledShards(urls))\n        else:\n            self.append(wds.SimpleShardList(urls))",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:170-185"
    },
    "407": {
        "file_id": 9,
        "content": "Code checks if the URLs provided for webdataset contain \"s3:\" indicating S3 links. If so, it requires 's3cmd' and 's3fs' packages to be installed or raises an error. It also adds shardList and allows resampling or shuffling of shards based on user input.",
        "type": "comment"
    },
    "408": {
        "file_id": 9,
        "content": "            if shuffle_shards:\n                self.append(wds.filters.shuffle(1000))\n        if img_embedding_folder_url is not None:\n            # There may be webdataset shards that do not have a embedding shard associated with it. If we do not skip these, they would cause issues.\n            self.append(skip_unassociated_shards(embeddings_url=img_embedding_folder_url, handler=handler))\n        if text_embedding_folder_url is not None:\n            self.append(skip_unassociated_shards(embeddings_url=text_embedding_folder_url, handler=handler))\n        self.append(wds.tarfile_to_samples(handler=handler))\n        self.append(wds.decode(\"pilrgb\", handler=handler))\n        if img_embedding_folder_url is not None:\n            # Then we are loading image embeddings for a remote source\n            assert index_width is not None, \"Reading embeddings separately requires index width length to be given\"\n            self.append(insert_embedding(embeddings_url=img_embedding_folder_url, index_width=index_width, sample_key='img_emb', handler=handler))",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:186-200"
    },
    "409": {
        "file_id": 9,
        "content": "The code configures a decoder loader for DALLE2-pytorch. It shuffles 1000 filters and skips unassociated shards if necessary, loads embeddings from URLs, converts to samples, and decodes images as PILRGB.",
        "type": "comment"
    },
    "410": {
        "file_id": 9,
        "content": "        if text_embedding_folder_url is not None:\n            # Then we are loading image embeddings for a remote source\n            assert index_width is not None, \"Reading embeddings separately requires index width length to be given\"\n            self.append(insert_embedding(embeddings_url=text_embedding_folder_url, index_width=index_width, sample_key='text_emb', handler=handler))\n        self.append(join_embeddings)\n        self.append(key_verifier(required_keys=keys, handler=handler))\n        # Apply preprocessing\n        self.append(wds.map(self.preproc))\n        self.append(wds.to_tuple(*keys))\n    def preproc(self, sample):\n        \"\"\"Applies the preprocessing for images\"\"\"\n        if self.img_preproc is not None:\n            sample[\"jpg\"] = self.img_preproc(sample[\"jpg\"])\n        return sample\ndef create_image_embedding_dataloader(\n    tar_url,\n    num_workers,\n    batch_size,\n    img_embeddings_url=None,\n    text_embeddings_url=None,\n    index_width=None,\n    shuffle_num = None,\n    shuffle_shards = True,",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:201-225"
    },
    "411": {
        "file_id": 9,
        "content": "This code creates an image embedding dataloader. If a text embedding folder URL is provided, it loads image embeddings for remote sources based on the given index width. It then applies preprocessing and joins the embeddings before returning the tuple of keys. The preproc function applies image preprocessing if available.",
        "type": "comment"
    },
    "412": {
        "file_id": 9,
        "content": "    resample_shards = False, \n    img_preproc=None,\n    extra_keys=[],\n    handler=wds.handlers.reraise_exception#warn_and_continue\n):\n    \"\"\"\n    Convenience function to create an image embedding dataseta and dataloader in one line\n    :param tar_url: A url pointing to the tar files of the webdataset formatted as /path/to/webdataset/{0000..9999}.tar\n    :param num_workers: The number of workers to use for the dataloader\n    :param batch_size: The batch size to use for the dataloader\n    :param embeddings_url: Required if webdataset does not contain embeddings. A url pointing to the npy files of the embeddings. Should have the same number of shards as the webdataset.\n        Webdataset image keys should align with the index of the embedding. This means missing image indices must have a corresponding embedding of all zeros.\n    :param index_width: The number of digits in the index. This is used to align the embedding index with the image index.\n            For example, if a file in the webdataset sh",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:226-240"
    },
    "413": {
        "file_id": 9,
        "content": "This code creates an image embedding dataset and dataloader in one line, accepting parameters such as tar_url, num_workers, batch_size, embeddings_url, and index_width. The function is designed for webdataset format and requires the same number of shards for both the webdataset images and their corresponding embeddings. It also supports handling exceptions using a specified handler.",
        "type": "comment"
    },
    "414": {
        "file_id": 9,
        "content": "ard 3 is named 0003039.jpg, we know the shard is 4 digits and the last 3 digits are the index_width.\n    :param shuffle_num: If not None, shuffle the dataset with this size buffer after sampling.\n    :param shuffle_shards: If true, shuffle the shards before sampling. This cannot be true if resample is true.\n    :param resample_shards: If true, resample webdataset shards with replacement. You need to set your own epoch size if this is true since it will resample infinitely.\n    :param handler: A webdataset handler.\n    \"\"\"\n    ds = ImageEmbeddingDataset(\n        tar_url,\n        img_embedding_folder_url=img_embeddings_url,\n        text_embedding_folder_url=text_embeddings_url,\n        index_width=index_width,\n        shuffle_shards=shuffle_shards,\n        resample=resample_shards,\n        extra_keys=extra_keys,\n        img_preproc=img_preproc,\n        handler=handler\n    )\n    if shuffle_num is not None and shuffle_num > 0:\n        ds.shuffle(1000)\n    return DataLoader(\n        ds,\n        num_workers=num_workers,",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:240-261"
    },
    "415": {
        "file_id": 9,
        "content": "This code defines a function that takes in parameters like tar_url, img_embedding_folder_url, text_embeddings_url, index_width, extra_keys, img_preproc, and handler. It creates an ImageEmbeddingDataset and optionally shuffles it based on the given shuffle_num. Then, it returns a DataLoader for further processing.",
        "type": "comment"
    },
    "416": {
        "file_id": 9,
        "content": "        batch_size=batch_size,\n        prefetch_factor=2,  # This might be good to have high so the next npy file is prefetched\n        pin_memory=True,\n        shuffle=False\n    )",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/decoder_loader.py:262-266"
    },
    "417": {
        "file_id": 9,
        "content": "This code creates a data loader for the decoder model. It sets batch size, prefetch factor (for efficient loading), pin memory (for faster GPU transfers), and disables shuffling.",
        "type": "comment"
    },
    "418": {
        "file_id": 10,
        "content": "/dalle2_pytorch/dataloaders/prior_loader.py",
        "type": "filepath"
    },
    "419": {
        "file_id": 10,
        "content": "This code offers efficient data retrieval classes for DALL-E 2, supports text conditioning and MPI distribution. It divides embedding reader objects into training, evaluation, and test sets using PyTorch Dataloaders, without specifying batch sizes.",
        "type": "summary"
    },
    "420": {
        "file_id": 10,
        "content": "from math import ceil\nfrom clip import tokenize\nfrom embedding_reader import EmbeddingReader\nfrom torch import from_numpy\nfrom torch.utils.data import IterableDataset, DataLoader\nclass PriorEmbeddingDataset(IterableDataset):\n    \"\"\"\n    PriorEmbeddingDataset is a wrapper of EmbeddingReader.\n    It enables one to simplify the logic necessary to yield samples from\n    the different EmbeddingReader configurations available.\n    \"\"\"\n    def __init__(\n        self,\n        text_conditioned: bool,\n        batch_size: int,\n        start: int,\n        stop: int,\n        image_reader,\n        text_reader: EmbeddingReader = None,\n    ) -> None:\n        super(PriorEmbeddingDataset).__init__()\n        self.text_conditioned = text_conditioned\n        if not self.text_conditioned:\n            self.text_reader = text_reader\n        self.image_reader = image_reader\n        self.start = start\n        self.stop = stop\n        self.batch_size = batch_size\n    def __len__(self):\n        return self.stop - self.start\n    def __iter__(self):",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/prior_loader.py:1-40"
    },
    "421": {
        "file_id": 10,
        "content": "The code defines a class called PriorEmbeddingDataset that wraps the EmbeddingReader class. It allows for simplified sample retrieval from various configurations of EmbeddingReader by enabling batch-based access to prior data, where text_conditioned and batch_size are parameters, along with start and stop indices for the range of data to be loaded.",
        "type": "comment"
    },
    "422": {
        "file_id": 10,
        "content": "        # D.R.Y loader args\n        loader_args = dict(\n            batch_size=self.batch_size,\n            start=self.start,\n            end=self.stop,\n            show_progress=False,\n        )\n        # if the data requested is text conditioned, only load images\n        if self.text_conditioned:\n            self.loader = self.image_reader(**loader_args)\n        # otherwise, include text embeddings and bypass metadata\n        else:\n            self.loader = zip(\n                self.image_reader(**loader_args), self.text_reader(**loader_args)\n            )\n        # return the data loader in its formatted state\n        return self\n    def __next__(self):\n        try:\n            return self.get_sample()\n        except StopIteration:\n            raise StopIteration\n    def __str__(self):\n        return f\"<PriorEmbeddingDataset: start: {self.start}, stop: {self.stop}, len: {self.__len__()}>\"\n    def set_start(self, start):\n        \"\"\"\n        Adjust the starting point within the reader, useful for resuming an epoch",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/prior_loader.py:41-72"
    },
    "423": {
        "file_id": 10,
        "content": "The code defines a PriorEmbeddingDataset class for data loading in DALLE2-pytorch. It uses an image_reader and text_reader to load data in a batch, with optional text conditioning. It includes a __next__ method for iterating through the dataset and a set_start method for adjusting the starting point within the reader.",
        "type": "comment"
    },
    "424": {
        "file_id": 10,
        "content": "        \"\"\"\n        self.start = start\n    def get_start(self):\n        return self.start\n    def get_sample(self):\n        \"\"\"\n        pre-proocess data from either reader into a common format\n        \"\"\"\n        if self.text_conditioned:\n            image_embedding, caption = next(self.loader)\n            image_embedding = from_numpy(image_embedding)\n            tokenized_caption = tokenize(caption[\"caption\"].to_list(), truncate=True)\n            return image_embedding, tokenized_caption\n        else:\n            (image_embedding, _), (text_embedding, _) = next(self.loader)\n            image_embedding = from_numpy(image_embedding)\n            text_embedding = from_numpy(text_embedding)\n            return image_embedding, text_embedding\n# helper functions\ndef distribute_to_rank(start, stop, rank, world_size):\n    \"\"\"\n    Distribute data to each rank given the world size.\n    Return:\n        - New start and stop points for this rank.\n    \"\"\"\n    num_samples = int(stop - start)\n    per_rank = int(ceil((num_samples) / float(world_size)))",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/prior_loader.py:73-112"
    },
    "425": {
        "file_id": 10,
        "content": "This code defines a class with methods to manage data loading and distribution for the DALL-E 2 model. It supports text-conditioned or unconditioned data, preprocesses input into a common format, and distributes data across multiple ranks using MPI.",
        "type": "comment"
    },
    "426": {
        "file_id": 10,
        "content": "    assert (\n        per_rank > 0\n    ), f\"Number of samples per rank must be larger than 0, (found: {per_rank})\"\n    rank_start = start + rank * per_rank\n    rank_stop = min(rank_start + per_rank, stop)\n    new_length = rank_stop - rank_start\n    assert (\n        new_length > 0\n    ), \"Calculated start and stop points result in a length of zero for this rank.\"\n    return rank_start, rank_stop\ndef get_reader(\n    text_conditioned: bool, img_url: str, meta_url: str = None, txt_url: str = None\n):\n    \"\"\"\n    Create an EmbeddingReader object from the specified URLs\n    get_reader() will always expect a url to image embeddings.\n    If text-conditioned, it will also expect a meta_url for the captions.\n    Otherwise, it will need txt_url for the matching text embeddings.\n    Returns an image_reader object if text-conditioned.\n    Otherwise it returns both an image_reader and a text_reader\n    \"\"\"\n    assert img_url is not None, \"Must supply a image url\"\n    if text_conditioned:\n        assert meta_url is not None, \"Must supply meta url if text-conditioned\"",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/prior_loader.py:114-149"
    },
    "427": {
        "file_id": 10,
        "content": "The code is defining functions that calculate the start and stop points for a given rank, and another function to create an EmbeddingReader object based on URLs. It asserts that certain inputs are not None before proceeding, ensuring necessary information is provided.",
        "type": "comment"
    },
    "428": {
        "file_id": 10,
        "content": "        image_reader = EmbeddingReader(\n            embeddings_folder=img_url,\n            file_format=\"parquet_npy\",\n            # will assume the caption column exists and is the only one requested\n            meta_columns=[\"caption\"],\n            metadata_folder=meta_url,\n        )\n        return image_reader\n    # otherwise we will require text embeddings as well and return two readers\n    assert (\n        txt_url is not None\n    ), \"Must supply text embedding url if not text-conditioning\"\n    image_reader = EmbeddingReader(img_url, file_format=\"npy\")\n    text_reader = EmbeddingReader(txt_url, file_format=\"npy\")\n    return image_reader, text_reader\ndef make_splits(\n    text_conditioned: bool,\n    batch_size: int,\n    num_data_points: int,\n    train_split: float,\n    eval_split: float,\n    image_reader: EmbeddingReader,\n    text_reader: EmbeddingReader = None,\n    start=0,\n    rank=0,\n    world_size=1,\n):\n    \"\"\"\n    Split an embedding reader object as needed.\n    NOTE: make_splits() will infer the test set size from your train and eval.",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/prior_loader.py:151-187"
    },
    "429": {
        "file_id": 10,
        "content": "This code defines a function to split an embedding reader object into training, evaluation, and optional test sets. It takes in the text conditioned flag, batch size, number of data points, and train/eval splits as input parameters. If text-conditioning is not enabled, it requires text embedding URLs as well and returns two readers.",
        "type": "comment"
    },
    "430": {
        "file_id": 10,
        "content": "    Input:\n        - text_conditioned: whether to prepare text-conditioned training data\n        - batch_size: the batch size for a single gpu\n        - num_data_points: the total number of data points you wish to train on\n        - train_split: the percentage of data you wish to train on\n        - eval_split: the percentage of data you wish to validate on\n        - image_reader: the image_reader you wish to split\n        - text_reader: the text_reader you want to split (if !text_conditioned)\n        - start: the starting point within your dataset\n        - rank: the rank of your worker\n        - world_size: the total world size of your distributed training run\n    Returns:\n        - PyTorch Dataloaders that yield tuples of (img, txt) data.\n    \"\"\"\n    assert start < image_reader.count, \"start position cannot exceed reader count.\"\n    # verify that the num_data_points does not exceed the max points\n    if num_data_points > (image_reader.count - start):\n        print(\n            \"Specified count is larger than what's available...defaulting to reader's count.\"",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/prior_loader.py:189-210"
    },
    "431": {
        "file_id": 10,
        "content": "This function takes various inputs like batch size, train and eval splits, readers, and starting point to create PyTorch Dataloaders for image-text pairs. It ensures the start position is within the reader's count, and if the specified data points count exceeds the available ones, it defaults to the remaining count.",
        "type": "comment"
    },
    "432": {
        "file_id": 10,
        "content": "        )\n        num_data_points = image_reader.count\n    # compute split points\n    train_set_size = int(train_split * num_data_points)\n    eval_set_size = int(eval_split * num_data_points)\n    eval_start = train_set_size\n    eval_stop = int(eval_start + eval_set_size)\n    assert (\n        train_split + eval_split\n    ) < 1.0, \"Specified train and eval split is too large to infer a test split.\"\n    # distribute to rank\n    rank_train_start, rank_train_stop = distribute_to_rank(\n        start, train_set_size, rank, world_size\n    )\n    rank_eval_start, rank_eval_stop = distribute_to_rank(\n        train_set_size, eval_stop, rank, world_size\n    )\n    rank_test_start, rank_test_stop = distribute_to_rank(\n        eval_stop, num_data_points, rank, world_size\n    )\n    # wrap up splits into a dict\n    train_split_args = dict(\n        start=rank_train_start, stop=rank_train_stop, batch_size=batch_size\n    )\n    eval_split_args = dict(\n        start=rank_eval_start, stop=rank_eval_stop, batch_size=batch_size\n    )\n    test_split_args = dict(",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/prior_loader.py:211-242"
    },
    "433": {
        "file_id": 10,
        "content": "Computing split points for training and evaluation data sets based on the specified splits. Distributing the data to ranks according to the world size. Wrapping up the splits into a dictionary with start, stop, and batch_size parameters.",
        "type": "comment"
    },
    "434": {
        "file_id": 10,
        "content": "        start=rank_test_start, stop=rank_test_stop, batch_size=batch_size\n    )\n    if text_conditioned:\n        # add the text-conditioned args to a unified dict\n        reader_args = dict(\n            text_conditioned=text_conditioned,\n            image_reader=image_reader,\n        )\n        train_split_args = dict(**reader_args, **train_split_args)\n        eval_split_args = dict(**reader_args, **eval_split_args)\n        test_split_args = dict(**reader_args, **test_split_args)\n        train = PriorEmbeddingDataset(**train_split_args)\n        val = PriorEmbeddingDataset(**eval_split_args)\n        test = PriorEmbeddingDataset(**test_split_args)\n    else:\n        # add the non-conditioned args to a unified dict\n        reader_args = dict(\n            text_conditioned=text_conditioned,\n            image_reader=image_reader,\n            text_reader=text_reader,\n        )\n        train_split_args = dict(**reader_args, **train_split_args)\n        eval_split_args = dict(**reader_args, **eval_split_args)\n        test_split_args = dict(**reader_args, **test_split_args)",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/prior_loader.py:243-271"
    },
    "435": {
        "file_id": 10,
        "content": "Code is creating a PriorEmbeddingDataset for train, validation, and test datasets based on given arguments. If text_conditioned, it creates separate dictionaries for each dataset and passes them to the PriorEmbeddingDataset class; otherwise, it adds additional non-conditioned arguments for the same process.",
        "type": "comment"
    },
    "436": {
        "file_id": 10,
        "content": "        train = PriorEmbeddingDataset(**train_split_args)\n        val = PriorEmbeddingDataset(**eval_split_args)\n        test = PriorEmbeddingDataset(**test_split_args)\n    # true batch size is specifed in the PriorEmbeddingDataset\n    train_loader = DataLoader(train, batch_size=None)\n    eval_loader = DataLoader(val, batch_size=None)\n    test_loader = DataLoader(test, batch_size=None)\n    return train_loader, eval_loader, test_loader",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/prior_loader.py:273-282"
    },
    "437": {
        "file_id": 10,
        "content": "This code creates train, val, and test datasets using PriorEmbeddingDataset with specific args. DataLoaders are created without specifying batch sizes, so the true batch size is determined in PriorEmbeddingDataset. The loaders and datasets are returned for further processing.",
        "type": "comment"
    },
    "438": {
        "file_id": 11,
        "content": "/dalle2_pytorch/dataloaders/simple_image_only_dataloader.py",
        "type": "filepath"
    },
    "439": {
        "file_id": 11,
        "content": "This code defines a Dataset class and get_images_dataloader function for loading image data. The Dataset class initializes with a folder path, image size, and extensions to consider. The get_images_dataloader function returns a DataLoader object for the specified folder with optional parameters like batch size, shuffle, cycle_dl, and pin_memory.",
        "type": "summary"
    },
    "440": {
        "file_id": 11,
        "content": "from pathlib import Path\nimport torch\nfrom torch.utils import data\nfrom torchvision import transforms, utils\nfrom PIL import Image\n# helpers functions\ndef cycle(dl):\n    while True:\n        for data in dl:\n            yield data\n# dataset and dataloader\nclass Dataset(data.Dataset):\n    def __init__(\n        self,\n        folder,\n        image_size,\n        exts = ['jpg', 'jpeg', 'png']\n    ):\n        super().__init__()\n        self.folder = folder\n        self.image_size = image_size\n        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n        self.transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.RandomHorizontalFlip(),\n            transforms.CenterCrop(image_size),\n            transforms.ToTensor()\n        ])\n    def __len__(self):\n        return len(self.paths)\n    def __getitem__(self, index):\n        path = self.paths[index]\n        img = Image.open(path)\n        return self.transform(img)\ndef get_images_dataloader(\n    folder,\n    *,",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/simple_image_only_dataloader.py:1-47"
    },
    "441": {
        "file_id": 11,
        "content": "This code defines a Dataset class and get_images_dataloader function for loading image data. The Dataset class initializes with a folder path, image size, and extensions to consider. It uses transforms to apply resizing, horizontal flipping, centercropping, and converting images to tensors. The get_images_dataloader function returns a data loader object for the specified folder.",
        "type": "comment"
    },
    "442": {
        "file_id": 11,
        "content": "    batch_size,\n    image_size,\n    shuffle = True,\n    cycle_dl = True,\n    pin_memory = True\n):\n    ds = Dataset(folder, image_size)\n    dl = data.DataLoader(ds, batch_size = batch_size, shuffle = shuffle, pin_memory = pin_memory)\n    if cycle_dl:\n        dl = cycle(dl)\n    return dl",
        "type": "code",
        "location": "/dalle2_pytorch/dataloaders/simple_image_only_dataloader.py:48-59"
    },
    "443": {
        "file_id": 11,
        "content": "This function takes parameters such as folder, batch size, image size, shuffle, cycle_dl, and pin_memory. It creates a dataset from the provided folder using a given image size. Then, it uses DataLoader to create a data loader with the specified batch size, shuffle, and pin memory settings. If cycle_dl is True, it applies cyclic permutations to the data loader. Finally, it returns the data loader.",
        "type": "comment"
    },
    "444": {
        "file_id": 12,
        "content": "/dalle2_pytorch/optimizer.py",
        "type": "filepath"
    },
    "445": {
        "file_id": 12,
        "content": "This code defines two functions, `separate_weight_decayable_params` and `get_optimizer`. The `get_optimizer` function takes parameters, learning rate, weight decay, and other options to create an optimizer object. It filters the parameters based on `requires_grad`, separates weight-decayable parameters, and uses either Adam or AdamW optimizer depending on the weight decay value.",
        "type": "summary"
    },
    "446": {
        "file_id": 12,
        "content": "from torch.optim import AdamW, Adam\ndef separate_weight_decayable_params(params):\n    wd_params, no_wd_params = [], []\n    for param in params:\n        param_list = no_wd_params if param.ndim < 2 else wd_params\n        param_list.append(param)\n    return wd_params, no_wd_params\ndef get_optimizer(\n    params,\n    lr = 1e-4,\n    wd = 1e-2,\n    betas = (0.9, 0.99),\n    eps = 1e-8,\n    filter_by_requires_grad = False,\n    group_wd_params = True,\n    **kwargs\n):\n    if filter_by_requires_grad:\n        params = list(filter(lambda t: t.requires_grad, params))\n    if wd == 0:\n        return Adam(params, lr = lr, betas = betas, eps = eps)\n    if group_wd_params:\n        wd_params, no_wd_params = separate_weight_decayable_params(params)\n        params = [\n            {'params': wd_params},\n            {'params': no_wd_params, 'weight_decay': 0},\n        ]\n    return AdamW(params, lr = lr, weight_decay = wd, betas = betas, eps = eps)",
        "type": "code",
        "location": "/dalle2_pytorch/optimizer.py:1-34"
    },
    "447": {
        "file_id": 12,
        "content": "This code defines two functions, `separate_weight_decayable_params` and `get_optimizer`. The `get_optimizer` function takes parameters, learning rate, weight decay, and other options to create an optimizer object. It filters the parameters based on `requires_grad`, separates weight-decayable parameters, and uses either Adam or AdamW optimizer depending on the weight decay value.",
        "type": "comment"
    },
    "448": {
        "file_id": 13,
        "content": "/dalle2_pytorch/tokenizer.py",
        "type": "filepath"
    },
    "449": {
        "file_id": 13,
        "content": "The code simplifies DALL-E2 text tokenization by offering a PyTorch BPE tokenizer implementation with features for whitespace cleanup, formatting fixes, human-readable conversion, and handling context length limitations.",
        "type": "summary"
    },
    "450": {
        "file_id": 13,
        "content": "# take from https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py\n# to give users a quick easy start to training DALL-E without doing BPE\nimport torch\nimport html\nimport os\nimport ftfy\nimport regex as re\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom dalle2_pytorch.utils import import_or_print_error\n# OpenAI simple tokenizer\n@lru_cache()\ndef default_bpe():\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data/bpe_simple_vocab_16e6.txt\")\n@lru_cache()\ndef bytes_to_unicode():\n    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n    cs = bs[:]\n    n = 0\n    for b in range(2 ** 8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2 ** 8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\ndef get_pairs(word):\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\ndef basic_clean(text):",
        "type": "code",
        "location": "/dalle2_pytorch/tokenizer.py:1-42"
    },
    "451": {
        "file_id": 13,
        "content": "This code imports necessary libraries and defines functions for tokenization, specifically for the DALL-E2 model. It uses OpenAI's simple tokenizer, a byte-to-unicode conversion, and a function to generate character pairs from a given word. The code is meant to provide users with an easy way to start training DALL-E without implementing BPE (Byte Pair Encoding).",
        "type": "comment"
    },
    "452": {
        "file_id": 13,
        "content": "    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\ndef whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path = default_bpe()):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = Path(bpe_path).read_text(encoding='utf8').split('\\n')\n        merges = merges[1:49152 - 256 - 2 + 1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v + '</w>' for v in vocab]\n        for merge in merges:\n            vocab.append(''.join(merge))\n        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n        self.vocab_size = 49408\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))",
        "type": "code",
        "location": "/dalle2_pytorch/tokenizer.py:43-69"
    },
    "453": {
        "file_id": 13,
        "content": "This code is a Python class for a tokenizer that utilizes byte encoding and decoding, along with byte-pair encoding (BPE) to convert text into tokens. The class also includes methods for cleaning whitespace and fixing text formatting issues. The BPE merges are loaded from a specified file path, and the vocabulary is expanded by adding special tokens like \"<|startoftext|>\" and \"<|endoftext|>\".",
        "type": "comment"
    },
    "454": {
        "file_id": 13,
        "content": "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n        self.pat = re.compile(\n            r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\",\n            re.IGNORECASE)\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + (token[-1] + '</w>',)\n        pairs = get_pairs(word)\n        if not pairs:\n            return token + '</w>'\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break",
        "type": "code",
        "location": "/dalle2_pytorch/tokenizer.py:70-98"
    },
    "455": {
        "file_id": 13,
        "content": "The code defines a tokenizer that uses byte-pair encoding (BPE) for text. It compiles a regular expression pattern to match words and special tokens like \"<|startoftext|>\" and \"<|endoftext|>\". The `bpe` method takes a token, checks if it's in the cache, and if not, processes it using BPE by splitting it into smaller parts until no more splits are possible.",
        "type": "comment"
    },
    "456": {
        "file_id": 13,
        "content": "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = ' '.join(word)\n        self.cache[token] = word\n        return word\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n        return bpe_tokens\n    def decode(self, tokens, remove_start_end = True, pad_tokens = set()):\n        if torch.is_tensor(tokens):\n            tokens = tokens.tolist()",
        "type": "code",
        "location": "/dalle2_pytorch/tokenizer.py:100-126"
    },
    "457": {
        "file_id": 13,
        "content": "Code snippet is from a byte-pair encoding (BPE) tokenizer implementation in PyTorch. The code encodes input text into BPE tokens, performs wordpiece tokenization, and caches the mapping between tokens and words for decoding. The encode() function processes the input text by applying preprocessing steps, performing BPE, and extending tokens list with BPE tokens. The decode() function allows decoding of encoded tokens back to words using cached mappings.",
        "type": "comment"
    },
    "458": {
        "file_id": 13,
        "content": "        if remove_start_end:\n            tokens = [token for token in tokens if token not in (49406, 40407, 0)]\n        text = ''.join([self.decoder[token] for token in tokens if token not in pad_tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n        return text\n    def tokenize(self, texts, context_length = 256, truncate_text = False):\n        if isinstance(texts, str):\n            texts = [texts]\n        all_tokens = [self.encode(text) for text in texts]\n        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n        for i, tokens in enumerate(all_tokens):\n            if len(tokens) > context_length:\n                if truncate_text:\n                    tokens = tokens[:context_length]\n                else:\n                    raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n            result[i, :len(tokens)] = torch.tensor(tokens)\n        return result\ntokenizer = SimpleTokenizer()",
        "type": "code",
        "location": "/dalle2_pytorch/tokenizer.py:128-151"
    },
    "459": {
        "file_id": 13,
        "content": "The code defines a SimpleTokenizer class that tokenizes input texts using an encoding scheme and provides a method to convert encoded tokens into human-readable text. It also includes a tokenize function to process multiple input texts, considering context length limitations and handling truncation. The provided code snippet focuses on the process of converting encoded tokens into text.",
        "type": "comment"
    },
    "460": {
        "file_id": 13,
        "content": "# YTTM tokenizer\nclass YttmTokenizer:\n    def __init__(self, bpe_path = None):\n        bpe_path = Path(bpe_path)\n        assert bpe_path.exists(), f'BPE json path {str(bpe_path)} does not exist'\n        self.yttm = import_or_print_error('youtokentome', 'you need to install youtokentome by `pip install youtokentome`')\n        tokenizer = self.yttm.BPE(model = str(bpe_path))\n        self.tokenizer = tokenizer\n        self.vocab_size = tokenizer.vocab_size()\n    def decode(self, tokens, pad_tokens = set()):\n        if torch.is_tensor(tokens):\n            tokens = tokens.tolist()\n        return self.tokenizer.decode(tokens, ignore_ids = pad_tokens.union({0}))\n    def encode(self, texts):\n        encoded = self.tokenizer.encode(texts, output_type = self.yttm.OutputType.ID)\n        return list(map(torch.tensor, encoded))\n    def tokenize(self, texts, context_length = 256, truncate_text = False):\n        if isinstance(texts, str):\n            texts = [texts]\n        all_tokens = self.encode(texts)\n        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)",
        "type": "code",
        "location": "/dalle2_pytorch/tokenizer.py:153-182"
    },
    "461": {
        "file_id": 13,
        "content": "This code defines a YTTM tokenizer class in PyTorch. The constructor loads the BPE model from the specified path and initializes the tokenizer instance, which can decode and encode text sequences. The decode function converts tokenized lists to human-readable strings, while the encode function transforms input texts into tokenized lists. The tokenize method takes a list of texts, encodes them, and returns a tensor of shape (number_of_texts, context_length) for further processing.",
        "type": "comment"
    },
    "462": {
        "file_id": 13,
        "content": "        for i, tokens in enumerate(all_tokens):\n            if len(tokens) > context_length:\n                if truncate_text:\n                    tokens = tokens[:context_length]\n                else:\n                    raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n            result[i, :len(tokens)] = torch.tensor(tokens)\n        return result",
        "type": "code",
        "location": "/dalle2_pytorch/tokenizer.py:183-191"
    },
    "463": {
        "file_id": 13,
        "content": "This code segment iterates through all tokens in a list, truncating any token sequence longer than the specified context length. If truncation is not allowed and an input text is too long, it raises a RuntimeError. The truncated or original tokens are then converted to torch tensors and stored in a result array.",
        "type": "comment"
    },
    "464": {
        "file_id": 14,
        "content": "/dalle2_pytorch/trackers.py",
        "type": "filepath"
    },
    "465": {
        "file_id": 14,
        "content": "The code initializes trackers and loggers, provides methods for logging data, saving configurations, and metadata. It saves states and models, manages loading/saving checkpoints, and handles errors with a \"recall()\" function.",
        "type": "summary"
    },
    "466": {
        "file_id": 14,
        "content": "import urllib.request\nimport os\nimport json\nfrom pathlib import Path\nimport shutil\nfrom itertools import zip_longest\nfrom typing import Any, Optional, List, Union\nfrom pydantic import BaseModel\nimport torch\nfrom dalle2_pytorch.dalle2_pytorch import Decoder, DiffusionPrior\nfrom dalle2_pytorch.utils import import_or_print_error\nfrom dalle2_pytorch.trainer import DecoderTrainer, DiffusionPriorTrainer\nfrom dalle2_pytorch.version import __version__\nfrom packaging import version\n# constants\nDEFAULT_DATA_PATH = './.tracker-data'\n# helper functions\ndef exists(val):\n    return val is not None\nclass BaseLogger:\n    \"\"\"\n    An abstract class representing an object that can log data.\n    Parameters:\n        data_path (str): A file path for storing temporary data.\n        verbose (bool): Whether of not to always print logs to the console.\n    \"\"\"\n    def __init__(self, data_path: str, resume: bool = False, auto_resume: bool = False, verbose: bool = False, **kwargs):\n        self.data_path = Path(data_path)\n        self.resume = resume",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:1-35"
    },
    "467": {
        "file_id": 14,
        "content": "This code is from the \"trackers.py\" file in the DALLE2-pytorch library, containing a class for base logger objects that can log data with optional data storage path and verbosity control. The class initializes with specified parameters like data_path, resume, auto_resume, and verbose. It uses Pathlib for path manipulation and supports temporary data storage.",
        "type": "comment"
    },
    "468": {
        "file_id": 14,
        "content": "        self.auto_resume = auto_resume\n        self.verbose = verbose\n    def init(self, full_config: BaseModel, extra_config: dict, **kwargs) -> None:\n        \"\"\"\n        Initializes the logger.\n        Errors if the logger is invalid.\n        full_config is the config file dict while extra_config is anything else from the script that is not defined the config file.\n        \"\"\"\n        raise NotImplementedError\n    def log(self, log, **kwargs) -> None:\n        raise NotImplementedError\n    def log_images(self, images, captions=[], image_section=\"images\", **kwargs) -> None:\n        raise NotImplementedError\n    def log_file(self, file_path, **kwargs) -> None:\n        raise NotImplementedError\n    def log_error(self, error_string, **kwargs) -> None:\n        raise NotImplementedError\n    def get_resume_data(self, **kwargs) -> dict:\n        \"\"\"\n        Sets tracker attributes that along with { \"resume\": True } will be used to resume training.\n        It is assumed that after init is called this data will be complete.",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:36-62"
    },
    "469": {
        "file_id": 14,
        "content": "The code defines a logger class with methods for logging different types of data, and an initialization method to set up the logger. The logger raises a NotImplementedError for each method, which means they need to be implemented in child classes. The get_resume_data method sets tracker attributes used to resume training if needed.",
        "type": "comment"
    },
    "470": {
        "file_id": 14,
        "content": "        If the logger does not have any resume functionality, it should return an empty dict.\n        \"\"\"\n        raise NotImplementedError\nclass ConsoleLogger(BaseLogger):\n    def init(self, full_config: BaseModel, extra_config: dict, **kwargs) -> None:\n        print(\"Logging to console\")\n    def log(self, log, **kwargs) -> None:\n        print(log)\n    def log_images(self, images, captions=[], image_section=\"images\", **kwargs) -> None:\n        pass\n    def log_file(self, file_path, **kwargs) -> None:\n        pass\n    def log_error(self, error_string, **kwargs) -> None:\n        print(error_string)\n    def get_resume_data(self, **kwargs) -> dict:\n        return {}\nclass WandbLogger(BaseLogger):\n    \"\"\"\n    Logs to a wandb run.\n    Parameters:\n        data_path (str): A file path for storing temporary data.\n        wandb_entity (str): The wandb entity to log to.\n        wandb_project (str): The wandb project to log to.\n        wandb_run_id (str): The wandb run id to resume.\n        wandb_run_name (str): The wandb run name to use.",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:63-94"
    },
    "471": {
        "file_id": 14,
        "content": "This code defines two logger classes, ConsoleLogger and WandbLogger, which inherit from the BaseLogger class. The ConsoleLogger logs to the console while the WandbLogger logs data to a Weights & Biases (WandB) run. Both loggers have methods for logging different types of data such as logs, images, files, and errors. The ConsoleLogger returns an empty dictionary if resuming is not supported, whereas the WandbLogger requires additional parameters like wandb_entity, wandb_project, wandb_run_id, and wandb_run_name for proper functioning.",
        "type": "comment"
    },
    "472": {
        "file_id": 14,
        "content": "    \"\"\"\n    def __init__(self,\n        data_path: str,\n        wandb_entity: str,\n        wandb_project: str,\n        wandb_run_id: Optional[str] = None,\n        wandb_run_name: Optional[str] = None,\n        **kwargs\n    ):\n        super().__init__(data_path, **kwargs)\n        self.entity = wandb_entity\n        self.project = wandb_project\n        self.run_id = wandb_run_id\n        self.run_name = wandb_run_name\n    def init(self, full_config: BaseModel, extra_config: dict, **kwargs) -> None:\n        assert self.entity is not None, \"wandb_entity must be specified for wandb logger\"\n        assert self.project is not None, \"wandb_project must be specified for wandb logger\"\n        self.wandb = import_or_print_error('wandb', '`pip install wandb` to use the wandb logger')\n        os.environ[\"WANDB_SILENT\"] = \"true\"\n        # Initializes the wandb run\n        init_object = {\n            \"entity\": self.entity,\n            \"project\": self.project,\n            \"config\": {**full_config.dict(), **extra_config}\n        }",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:95-120"
    },
    "473": {
        "file_id": 14,
        "content": "This code is a Python class for creating and initializing a WandB logger. It requires a data path, WandB entity, and project parameters. The class also supports additional configuration options. If the WandB entity or project are not specified, an error will be raised.",
        "type": "comment"
    },
    "474": {
        "file_id": 14,
        "content": "        if self.run_name is not None:\n            init_object['name'] = self.run_name\n        if self.resume:\n            assert self.run_id is not None, '`wandb_run_id` must be provided if `wandb_resume` is True'\n            if self.run_name is not None:\n                print(\"You are renaming a run. I hope that is what you intended.\")\n            init_object['resume'] = 'must'\n            init_object['id'] = self.run_id\n        self.wandb.init(**init_object)\n        print(f\"Logging to wandb run {self.wandb.run.path}-{self.wandb.run.name}\")\n    def log(self, log, **kwargs) -> None:\n        if self.verbose:\n            print(log)\n        self.wandb.log(log, **kwargs)\n    def log_images(self, images, captions=[], image_section=\"images\", **kwargs) -> None:\n        \"\"\"\n        Takes a tensor of images and a list of captions and logs them to wandb.\n        \"\"\"\n        wandb_images = [self.wandb.Image(image, caption=caption) for image, caption in zip_longest(images, captions)]\n        self.wandb.log({ image_section: wandb_images }, **kwargs)",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:121-143"
    },
    "475": {
        "file_id": 14,
        "content": "This code initializes a Wandb tracker, allowing for easy logging of data to a specific run. If `run_id` is provided and `wandb_resume` is True, the run is resumed with a warning about renaming. The code then logs various types of data including logs, images with captions, using the Wandb API. Verbose output is also supported for logs.",
        "type": "comment"
    },
    "476": {
        "file_id": 14,
        "content": "    def log_file(self, file_path, base_path: Optional[str] = None, **kwargs) -> None:\n        if base_path is None:\n            # Then we take the basepath as the parent of the file_path\n            base_path = Path(file_path).parent\n        self.wandb.save(str(file_path), base_path = str(base_path))\n    def log_error(self, error_string, step=None, **kwargs) -> None:\n        if self.verbose:\n            print(error_string)\n        self.wandb.log({\"error\": error_string, **kwargs}, step=step)\n    def get_resume_data(self, **kwargs) -> dict:\n        # In order to resume, we need wandb_entity, wandb_project, and wandb_run_id\n        return {\n            \"entity\": self.entity,\n            \"project\": self.project,\n            \"run_id\": self.wandb.run.id\n        }\nlogger_type_map = {\n    'console': ConsoleLogger,\n    'wandb': WandbLogger,\n}\ndef create_logger(logger_type: str, data_path: str, **kwargs) -> BaseLogger:\n    if logger_type == 'custom':\n        raise NotImplementedError('Custom loggers are not supported yet. Please use a different logger type.')",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:145-170"
    },
    "477": {
        "file_id": 14,
        "content": "The code defines a class with three methods: `log_file`, `log_error`, and `get_resume_data`. The `log_file` method logs a file path, `log_error` logs an error string, and `get_resume_data` returns a dictionary containing essential resume information. Additionally, there is a function `create_logger` which creates a logger of type 'console' or 'wandb'. For now, custom loggers are not supported.",
        "type": "comment"
    },
    "478": {
        "file_id": 14,
        "content": "    try:\n        logger_class = logger_type_map[logger_type]\n    except KeyError:\n        raise ValueError(f'Unknown logger type: {logger_type}. Must be one of {list(logger_type_map.keys())}')\n    return logger_class(data_path, **kwargs)\nclass BaseLoader:\n    \"\"\"\n    An abstract class representing an object that can load a model checkpoint.\n    Parameters:\n        data_path (str): A file path for storing temporary data.\n    \"\"\"\n    def __init__(self, data_path: str, only_auto_resume: bool = False, **kwargs):\n        self.data_path = Path(data_path)\n        self.only_auto_resume = only_auto_resume\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        raise NotImplementedError\n    def recall() -> dict:\n        raise NotImplementedError\nclass UrlLoader(BaseLoader):\n    \"\"\"\n    A loader that downloads the file from a url and loads it\n    Parameters:\n        data_path (str): A file path for storing temporary data.\n        url (str): The url to download the file from.\n    \"\"\"\n    def __init__(self, data_path: str, url: str, **kwargs):",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:171-200"
    },
    "479": {
        "file_id": 14,
        "content": "Function tries to create an instance of a logger class based on the given type, otherwise it raises a ValueError. BaseLoader is an abstract class that can be used to load model checkpoints with data_path and optionally other parameters. UrlLoader extends BaseLoader by allowing loading files from URLs instead of local file paths.",
        "type": "comment"
    },
    "480": {
        "file_id": 14,
        "content": "        super().__init__(data_path, **kwargs)\n        self.url = url\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        # Makes sure the file exists to be downloaded\n        pass  # TODO: Actually implement that\n    def recall(self) -> dict:\n        # Download the file\n        save_path = self.data_path / 'loaded_checkpoint.pth'\n        urllib.request.urlretrieve(self.url, str(save_path))\n        # Load the file\n        return torch.load(str(save_path), map_location='cpu')\nclass LocalLoader(BaseLoader):\n    \"\"\"\n    A loader that loads a file from a local path\n    Parameters:\n        data_path (str): A file path for storing temporary data.\n        file_path (str): The path to the file to load.\n    \"\"\"\n    def __init__(self, data_path: str, file_path: str, **kwargs):\n        super().__init__(data_path, **kwargs)\n        self.file_path = Path(file_path)\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        # Makes sure the file exists to be loaded\n        if not self.file_path.exists() and not self.only_auto_resume:",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:201-229"
    },
    "481": {
        "file_id": 14,
        "content": "The code defines a base class, \"BaseLoader\", which is responsible for loading files from a given data path. It initializes the class by setting the URL and has an init method to check if the file exists. The \"recall\" method downloads the file and loads it into memory. Additionally, there is a subclass called \"LocalLoader\" that loads files from local paths, checking if the file exists before loading it.",
        "type": "comment"
    },
    "482": {
        "file_id": 14,
        "content": "            raise FileNotFoundError(f'Model not found at {self.file_path}')\n    def recall(self) -> dict:\n        # Load the file\n        return torch.load(str(self.file_path), map_location='cpu')\nclass WandbLoader(BaseLoader):\n    \"\"\"\n    A loader that loads a model from an existing wandb run\n    \"\"\"\n    def __init__(self, data_path: str, wandb_file_path: str, wandb_run_path: Optional[str] = None, **kwargs):\n        super().__init__(data_path, **kwargs)\n        self.run_path = wandb_run_path\n        self.file_path = wandb_file_path\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        self.wandb = import_or_print_error('wandb', '`pip install wandb` to use the wandb recall function')\n        # Make sure the file can be downloaded\n        if self.wandb.run is not None and self.run_path is None:\n            self.run_path = self.wandb.run.path\n            assert self.run_path is not None, 'wandb run was not found to load from. If not using the wandb logger must specify the `wandb_run_path`.'\n        assert self.run_path is not None, '`wandb_run_path` must be provided for the wandb loader'",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:230-251"
    },
    "483": {
        "file_id": 14,
        "content": "This code defines a class `WandbLoader` that loads a model from an existing W&B (Weights & Biases) run. It requires a data path, a file path within the W&B run, and optionally a W&B run path. The `__init__` method initializes the object, the `init` method ensures the file can be downloaded, and the `recall` method loads the model using `torch.load`. If a W&B run is available but the run path is not specified, it sets the run path to the current run's path. The code also imports the 'wandb' library if it is missing.",
        "type": "comment"
    },
    "484": {
        "file_id": 14,
        "content": "        assert self.file_path is not None, '`wandb_file_path` must be provided for the wandb loader'\n        os.environ[\"WANDB_SILENT\"] = \"true\"\n        pass  # TODO: Actually implement that\n    def recall(self) -> dict:\n        file_reference = self.wandb.restore(self.file_path, run_path=self.run_path)\n        return torch.load(file_reference.name, map_location='cpu')\nloader_type_map = {\n    'url': UrlLoader,\n    'local': LocalLoader,\n    'wandb': WandbLoader,\n}\ndef create_loader(loader_type: str, data_path: str, **kwargs) -> BaseLoader:\n    if loader_type == 'custom':\n        raise NotImplementedError('Custom loaders are not supported yet. Please use a different loader type.')\n    try:\n        loader_class = loader_type_map[loader_type]\n    except KeyError:\n        raise ValueError(f'Unknown loader type: {loader_type}. Must be one of {list(loader_type_map.keys())}')\n    return loader_class(data_path, **kwargs)\nclass BaseSaver:\n    def __init__(self,\n        data_path: str,\n        save_latest_to: Optional[Union[str, bool]] = None,",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:252-278"
    },
    "485": {
        "file_id": 14,
        "content": "This code defines a `BaseSaver` class with an optional parameter for saving the latest data to a specified location. It also includes a function `create_loader()` that creates different types of loaders (url, local, wandb) based on the provided loader type and data path. The WandbLoader is used to restore data from a specified file path using Weights & Biases environment.",
        "type": "comment"
    },
    "486": {
        "file_id": 14,
        "content": "        save_best_to: Optional[Union[str, bool]] = None,\n        save_meta_to: Optional[str] = None,\n        save_type: str = 'checkpoint',\n        **kwargs\n    ):\n        self.data_path = Path(data_path)\n        self.save_latest_to = save_latest_to\n        self.saving_latest = save_latest_to is not None and save_latest_to is not False\n        self.save_best_to = save_best_to\n        self.saving_best = save_best_to is not None and save_best_to is not False\n        self.save_meta_to = save_meta_to\n        self.saving_meta = save_meta_to is not None\n        self.save_type = save_type\n        assert save_type in ['checkpoint', 'model'], '`save_type` must be one of `checkpoint` or `model`'\n        assert self.saving_latest or self.saving_best or self.saving_meta, 'At least one saving option must be specified'\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        raise NotImplementedError\n    def save_file(self, local_path: Path, save_path: str, is_best=False, is_latest=False, **kwargs) -> None:\n        \"\"\"",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:279-299"
    },
    "487": {
        "file_id": 14,
        "content": "This code defines a tracker class that handles saving of data to specified locations. It allows saving the latest, best, and meta information, with options for file type and paths. The `save_file` method is used to save files with optional flags for best and latest status. An assertion ensures that the save type is either 'checkpoint' or 'model'. A final assertion requires at least one saving option to be specified.",
        "type": "comment"
    },
    "488": {
        "file_id": 14,
        "content": "        Save a general file under save_meta_to\n        \"\"\"\n        raise NotImplementedError\nclass LocalSaver(BaseSaver):\n    def __init__(self,\n        data_path: str,\n        **kwargs\n    ):\n        super().__init__(data_path, **kwargs)\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        # Makes sure the directory exists to be saved to\n        print(f\"Saving {self.save_type} locally\")\n        if not self.data_path.exists():\n            self.data_path.mkdir(parents=True)\n    def save_file(self, local_path: str, save_path: str, **kwargs) -> None:\n        # Copy the file to save_path\n        save_path_file_name = Path(save_path).name\n        # Make sure parent directory exists\n        save_path_parent = Path(save_path).parent\n        if not save_path_parent.exists():\n            save_path_parent.mkdir(parents=True)\n        print(f\"Saving {save_path_file_name} {self.save_type} to local path {save_path}\")\n        shutil.copy(local_path, save_path)\nclass WandbSaver(BaseSaver):\n    def __init__(self, data_path: str, wandb_run_path: Optional[str] = None, **kwargs):",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:300-328"
    },
    "489": {
        "file_id": 14,
        "content": "This code defines two classes, LocalSaver and WandbSaver, which inherit from BaseSaver. Both classes are responsible for saving files in different locations. The LocalSaver saves files locally to a specified data_path, ensuring the directory exists beforehand. The WandbSaver is optional and requires a wandb_run_path parameter.",
        "type": "comment"
    },
    "490": {
        "file_id": 14,
        "content": "        super().__init__(data_path, **kwargs)\n        self.run_path = wandb_run_path\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        self.wandb = import_or_print_error('wandb', '`pip install wandb` to use the wandb logger')\n        os.environ[\"WANDB_SILENT\"] = \"true\"\n        # Makes sure that the user can upload tot his run\n        if self.run_path is not None:\n            entity, project, run_id = self.run_path.split(\"/\")\n            self.run = self.wandb.init(entity=entity, project=project, id=run_id)\n        else:\n            assert self.wandb.run is not None, 'You must be using the wandb logger if you are saving to wandb and have not set `wandb_run_path`'\n            self.run = self.wandb.run\n        # TODO: Now actually check if upload is possible\n        print(f\"Saving to wandb run {self.run.path}-{self.run.name}\")\n    def save_file(self, local_path: Path, save_path: str, **kwargs) -> None:\n        # In order to log something in the correct place in wandb, we need to have the same file structure here",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:329-346"
    },
    "491": {
        "file_id": 14,
        "content": "This code initializes a W&B run based on the `wandb_run_path` provided. It imports the W&B library, sets up the environment for uploading to W&B runs, and checks if the user has access to save files in the specified W&B run path.",
        "type": "comment"
    },
    "492": {
        "file_id": 14,
        "content": "        save_path_file_name = Path(save_path).name\n        print(f\"Saving {save_path_file_name} {self.save_type} to wandb run {self.run.path}-{self.run.name}\")\n        save_path = Path(self.data_path) / save_path\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copy(local_path, save_path)\n        self.run.save(str(save_path), base_path = str(self.data_path), policy='now')\nclass HuggingfaceSaver(BaseSaver):\n    def __init__(self, data_path: str, huggingface_repo: str, token_path: Optional[str] = None, **kwargs):\n        super().__init__(data_path, **kwargs)\n        self.huggingface_repo = huggingface_repo\n        self.token_path = token_path\n    def init(self, logger: BaseLogger, **kwargs):\n        # Makes sure this user can upload to the repo\n        self.hub = import_or_print_error('huggingface_hub', '`pip install huggingface_hub` to use the huggingface saver')\n        try:\n            identity = self.hub.whoami()  # Errors if not logged in\n            # Then we are logged in",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:347-365"
    },
    "493": {
        "file_id": 14,
        "content": "This code defines a `HuggingfaceSaver` class that saves files to a Hugging Face repository. It initializes the instance with a data path, Hugging Face repo, and optional token path. The `init` method checks if the user is logged in to the Hugging Face hub and then saves the file specified by `save_path` using `self.hub.upload`.",
        "type": "comment"
    },
    "494": {
        "file_id": 14,
        "content": "        except:\n            # We are not logged in. Use the token_path to set the token.\n            if not os.path.exists(self.token_path):\n                raise Exception(\"Not logged in to huggingface and no token_path specified. Please login with `huggingface-cli login` or if that does not work set the token_path.\")\n            with open(self.token_path, \"r\") as f:\n                token = f.read().strip()\n            self.hub.HfApi.set_access_token(token)\n            identity = self.hub.whoami()\n        print(f\"Saving to huggingface repo {self.huggingface_repo}\")\n    def save_file(self, local_path: Path, save_path: str, **kwargs) -> None:\n        # Saving to huggingface is easy, we just need to upload the file with the correct name\n        save_path_file_name = Path(save_path).name\n        print(f\"Saving {save_path_file_name} {self.save_type} to huggingface repo {self.huggingface_repo}\")\n        self.hub.upload_file(\n            path_or_fileobj=str(local_path),\n            path_in_repo=str(save_path),",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:366-382"
    },
    "495": {
        "file_id": 14,
        "content": "This code handles saving a file to the HuggingFace repo. If not logged in, it checks for a token path and uses it if available, or throws an exception. It then prints the saving path, logs in with the token (if provided), and finally uploads the file to the specified HuggingFace repo.",
        "type": "comment"
    },
    "496": {
        "file_id": 14,
        "content": "            repo_id=self.huggingface_repo\n        )\nsaver_type_map = {\n    'local': LocalSaver,\n    'wandb': WandbSaver,\n    'huggingface': HuggingfaceSaver\n}\ndef create_saver(saver_type: str, data_path: str, **kwargs) -> BaseSaver:\n    if saver_type == 'custom':\n        raise NotImplementedError('Custom savers are not supported yet. Please use a different saver type.')\n    try:\n        saver_class = saver_type_map[saver_type]\n    except KeyError:\n        raise ValueError(f'Unknown saver type: {saver_type}. Must be one of {list(saver_type_map.keys())}')\n    return saver_class(data_path, **kwargs)\nclass Tracker:\n    def __init__(self, data_path: Optional[str] = DEFAULT_DATA_PATH, overwrite_data_path: bool = False, dummy_mode: bool = False):\n        self.data_path = Path(data_path)\n        if not dummy_mode:\n            if not overwrite_data_path:\n                assert not self.data_path.exists(), f'Data path {self.data_path} already exists. Set overwrite_data_path to True to overwrite.'\n                if not self.data_path.exists():",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:383-407"
    },
    "497": {
        "file_id": 14,
        "content": "Function create_saver takes a saver type and data path, returns a BaseSaver object. It supports 'local', 'wandb', and 'huggingface' saver types. If the saver type is 'custom', it raises an error since custom savers aren't supported yet. Tracker initializes with optional data_path, overwrite_data_path (to overwrite existing path), and dummy_mode (if running in simulation mode). If not in dummy mode, asserts that the data path doesn't exist unless overwrite_data_path is True.",
        "type": "comment"
    },
    "498": {
        "file_id": 14,
        "content": "                    self.data_path.mkdir(parents=True)\n        self.logger: BaseLogger = None\n        self.loader: Optional[BaseLoader] = None\n        self.savers: List[BaseSaver]= []\n        self.dummy_mode = dummy_mode\n    def _load_auto_resume(self) -> bool:\n        # If the file does not exist, we return False. If autoresume is enabled we print a warning so that the user can know that this is the first run.\n        if not self.auto_resume_path.exists():\n            if self.logger.auto_resume:\n                print(\"Auto_resume is enabled but no auto_resume.json file exists. Assuming this is the first run.\")\n            return False\n        # Now we know that the autoresume file exists, but if we are not auto resuming we should remove it so that we don't accidentally load it next time\n        if not self.logger.auto_resume:\n            print(f'Removing auto_resume.json because auto_resume is not enabled in the config')\n            self.auto_resume_path.unlink()\n            return False\n        # Otherwise we read the json into a dictionary will will override parts of logger.__dict__",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:408-427"
    },
    "499": {
        "file_id": 14,
        "content": "This code initializes a tracker object, handling the data path creation, base logger and loader setup, saving list initialization, and dummy mode. It also includes a method to load auto-resume configuration if it exists, printing warnings for first run or removing the file if auto-resume is not enabled.",
        "type": "comment"
    }
}