{
    "500": {
        "file_id": 14,
        "content": "        with open(self.auto_resume_path, 'r') as f:\n            auto_resume_dict = json.load(f)\n        # Check if the logger is of the same type as the autoresume save\n        if auto_resume_dict[\"logger_type\"] != self.logger.__class__.__name__:\n            raise Exception(f'The logger type in the auto_resume file is {auto_resume_dict[\"logger_type\"]} but the current logger is {self.logger.__class__.__name__}. Either use the original logger type, set `auto_resume` to `False`, or delete your existing tracker-data folder.')\n        # Then we are ready to override the logger with the autoresume save\n        self.logger.__dict__[\"resume\"] = True\n        print(f\"Updating {self.logger.__dict__} with {auto_resume_dict}\")\n        self.logger.__dict__.update(auto_resume_dict)\n        return True\n    def _save_auto_resume(self):\n        # Gets the autoresume dict from the logger and adds \"logger_type\" to it then saves it to the auto_resume file\n        auto_resume_dict = self.logger.get_resume_data()\n        auto_resume_dict['logger_type'] = self.logger.__class__.__name__",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:428-442"
    },
    "501": {
        "file_id": 14,
        "content": "This code reads a previously saved state from the \"auto_resume_path\" and checks if the logger type matches the current logger. If they don't match, it raises an exception with instructions on how to proceed. Otherwise, it updates the logger with the auto-resume data and returns True.",
        "type": "comment"
    },
    "502": {
        "file_id": 14,
        "content": "        with open(self.auto_resume_path, 'w') as f:\n            json.dump(auto_resume_dict, f)\n    def init(self, full_config: BaseModel, extra_config: dict):\n        self.auto_resume_path = self.data_path / 'auto_resume.json'\n        # Check for resuming the run\n        self.did_auto_resume = self._load_auto_resume()\n        if self.did_auto_resume:\n            print(f'\\n\\nWARNING: RUN HAS BEEN AUTO-RESUMED WITH THE LOGGER TYPE {self.logger.__class__.__name__}.\\nIf this was not your intention, stop this run and set `auto_resume` to `False` in the config.\\n\\n')\n            print(f\"New logger config: {self.logger.__dict__}\")\n        self.save_metadata = dict(\n            version = version.parse(__version__)\n        )  # Data that will be saved alongside the checkpoint or model\n        self.blacklisted_checkpoint_metadata_keys = ['scaler', 'optimizer', 'model', 'version', 'step', 'steps']  # These keys would cause us to error if we try to save them as metadata\n        assert self.logger is not None, '`logger` must be set before `init` is called'",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:443-459"
    },
    "503": {
        "file_id": 14,
        "content": "This code is initializing a tracker object. It sets the auto_resume path, checks for resuming the run and prints a warning if it was automatically resumed. The save_metadata dictionary is created with version information and some keys are blacklisted from being saved as metadata to avoid errors during saving. The logger must be set before calling init method.",
        "type": "comment"
    },
    "504": {
        "file_id": 14,
        "content": "        if self.dummy_mode:\n            # The only thing we need is a loader\n            if self.loader is not None:\n                self.loader.init(self.logger)\n            return\n        assert len(self.savers) > 0, '`savers` must be set before `init` is called'\n        self.logger.init(full_config, extra_config)\n        if self.loader is not None:\n            self.loader.init(self.logger)\n        for saver in self.savers:\n            saver.init(self.logger)\n        if self.logger.auto_resume:\n            # Then we need to save the autoresume file. It is assumed after logger.init is called that the logger is ready to be saved.\n            self._save_auto_resume()\n    def add_logger(self, logger: BaseLogger):\n        self.logger = logger\n    def add_loader(self, loader: BaseLoader):\n        self.loader = loader\n    def add_saver(self, saver: BaseSaver):\n        self.savers.append(saver)\n    def log(self, *args, **kwargs):\n        if self.dummy_mode:\n            return\n        self.logger.log(*args, **kwargs)",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:460-489"
    },
    "505": {
        "file_id": 14,
        "content": "This code initializes trackers by first checking if in dummy mode, then initializing loaders and savers. The logger is initialized only if the `savers` list has items, and if `auto_resume` is enabled, it saves an autoresume file. The `add_logger`, `add_loader`, `add_saver`, and `log` methods are provided to interact with trackers' components.",
        "type": "comment"
    },
    "506": {
        "file_id": 14,
        "content": "    def log_images(self, *args, **kwargs):\n        if self.dummy_mode:\n            return\n        self.logger.log_images(*args, **kwargs)\n    def log_file(self, *args, **kwargs):\n        if self.dummy_mode:\n            return\n        self.logger.log_file(*args, **kwargs)\n    def save_config(self, current_config_path: str, config_name = 'config.json'):\n        if self.dummy_mode:\n            return\n        # Save the config under config_name in the root folder of data_path\n        shutil.copy(current_config_path, self.data_path / config_name)\n        for saver in self.savers:\n            if saver.saving_meta:\n                remote_path = Path(saver.save_meta_to) / config_name\n                saver.save_file(current_config_path, str(remote_path))\n    def add_save_metadata(self, state_dict_key: str, metadata: Any):\n        \"\"\"\n        Adds a new piece of metadata that will be saved along with the model or decoder.\n        \"\"\"\n        self.save_metadata[state_dict_key] = metadata\n    def _save_state_dict(self,",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:491-517"
    },
    "507": {
        "file_id": 14,
        "content": "This code is from the DALLE2-pytorch library and it contains several methods for logging images, files, saving configurations, and adding save metadata. The dummy_mode check prevents unnecessary actions when in a test mode. The save_config method copies the current config file to the root folder of the data_path and saves it remotely if specified by the saver. The add_save_metadata method adds new metadata that will be saved along with the model or decoder.",
        "type": "comment"
    },
    "508": {
        "file_id": 14,
        "content": " trainer: Union[DiffusionPriorTrainer, DecoderTrainer], save_type: str, file_path: str, **kwargs) -> Path:\n        \"\"\"\n        Gets the state dict to be saved and writes it to file_path.\n        If save_type is 'checkpoint', we save the entire trainer state dict.\n        If save_type is 'model', we save only the model state dict.\n        \"\"\"\n        assert save_type in ['checkpoint', 'model']\n        if save_type == 'checkpoint':\n            # Create a metadata dict without the blacklisted keys so we do not error when we create the state dict\n            metadata = {k: v for k, v in self.save_metadata.items() if k not in self.blacklisted_checkpoint_metadata_keys}\n            trainer.save(file_path, overwrite=True, **kwargs, **metadata)\n        elif save_type == 'model':\n            if isinstance(trainer, DiffusionPriorTrainer):\n                prior = trainer.ema_diffusion_prior.ema_model if trainer.use_ema else trainer.diffusion_prior\n                prior: DiffusionPrior = trainer.accelerator.unwrap_model(prior)",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:517-531"
    },
    "509": {
        "file_id": 14,
        "content": "This function saves the trainer's state dict, depending on the 'save_type' parameter. If 'checkpoint', it saves the entire trainer state without blacklisted metadata keys. If 'model', it saves only the model state if the trainer is a DiffusionPriorTrainer.",
        "type": "comment"
    },
    "510": {
        "file_id": 14,
        "content": "                # Remove CLIP if it is part of the model\n                original_clip = prior.clip\n                prior.clip = None\n                model_state_dict = prior.state_dict()\n                prior.clip = original_clip\n            elif isinstance(trainer, DecoderTrainer):\n                decoder: Decoder = trainer.accelerator.unwrap_model(trainer.decoder)\n                # Remove CLIP if it is part of the model\n                original_clip = decoder.clip\n                decoder.clip = None\n                if trainer.use_ema:\n                    trainable_unets = decoder.unets\n                    decoder.unets = trainer.unets  # Swap EMA unets in\n                    model_state_dict = decoder.state_dict()\n                    decoder.unets = trainable_unets  # Swap back\n                else:\n                    model_state_dict = decoder.state_dict()\n                decoder.clip = original_clip\n            else:\n                raise NotImplementedError('Saving this type of model with EMA mode enabled is not yet implemented. Actually, how did you get here?')",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:532-551"
    },
    "511": {
        "file_id": 14,
        "content": "This code checks the type of trainer and removes CLIP from the model if it is part of it. It then saves the state dictionary for the model, and optionally swaps EMA unets in or out depending on the use_ema flag. Finally, it restores the original CLIP state.",
        "type": "comment"
    },
    "512": {
        "file_id": 14,
        "content": "            state_dict = {\n                **self.save_metadata,\n                'model': model_state_dict\n            }\n            torch.save(state_dict, file_path)\n        return Path(file_path)\n    def save(self, trainer, is_best: bool, is_latest: bool, **kwargs):\n        if self.dummy_mode:\n            return\n        if not is_best and not is_latest:\n            # Nothing to do\n            return\n        # Save the checkpoint and model to data_path\n        checkpoint_path = self.data_path / 'checkpoint.pth'\n        self._save_state_dict(trainer, 'checkpoint', checkpoint_path, **kwargs)\n        model_path = self.data_path / 'model.pth'\n        self._save_state_dict(trainer, 'model', model_path, **kwargs)\n        print(\"Saved cached models\")\n        # Call the save methods on the savers\n        for saver in self.savers:\n            local_path = checkpoint_path if saver.save_type == 'checkpoint' else model_path\n            if saver.saving_latest and is_latest:\n                latest_checkpoint_path = saver.save_latest_to.format(**kwargs)",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:552-575"
    },
    "513": {
        "file_id": 14,
        "content": "This code saves the model and checkpoint to specified file paths. If not in dummy mode, it checks if the 'is_best' or 'is_latest' flag is set before proceeding with saving the state dictionary for 'checkpoint' and 'model'. It then prints a message confirming the saved cached models. Lastly, it calls save methods on savers, considering the 'saving_latest' flag and appropriate file paths.",
        "type": "comment"
    },
    "514": {
        "file_id": 14,
        "content": "                try:\n                    saver.save_file(local_path, latest_checkpoint_path, is_latest=True, **kwargs)\n                except Exception as e:\n                    self.logger.log_error(f'Error saving checkpoint: {e}', **kwargs)\n                    print(f'Error saving checkpoint: {e}')\n            if saver.saving_best and is_best:\n                best_checkpoint_path = saver.save_best_to.format(**kwargs)\n                try:\n                    saver.save_file(local_path, best_checkpoint_path, is_best=True, **kwargs)\n                except Exception as e:\n                    self.logger.log_error(f'Error saving checkpoint: {e}', **kwargs)\n                    print(f'Error saving checkpoint: {e}')\n    @property\n    def can_recall(self):\n        # Defines whether a recall can be performed.\n        return self.loader is not None and (not self.loader.only_auto_resume or self.did_auto_resume)\n    def recall(self):\n        if self.can_recall:\n            return self.loader.recall()\n        else:\n",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:576-598"
    },
    "515": {
        "file_id": 14,
        "content": "This code appears to be part of a class that manages loading and saving checkpoints for a model. It has a property called \"can_recall\" which determines if a recall (loading a previously saved checkpoint) can be performed based on whether the loader is not None and certain conditions about the loader's properties. If a recall is possible, the \"recall()\" function is called to perform the actual recall. Any errors that occur during saving are logged and printed.",
        "type": "comment"
    },
    "516": {
        "file_id": 14,
        "content": "            raise ValueError('Tried to recall, but no loader was set or auto-resume was not performed.')",
        "type": "code",
        "location": "/dalle2_pytorch/trackers.py:598-598"
    },
    "517": {
        "file_id": 14,
        "content": "Raises an error when no loader is set and auto-resume was not performed.",
        "type": "comment"
    },
    "518": {
        "file_id": 15,
        "content": "/dalle2_pytorch/train_configs.py",
        "type": "filepath"
    },
    "519": {
        "file_id": 15,
        "content": "The code sets up DALL-E 2 PyTorch training configurations, provides utility functions and tracker configuration, defines a class for model training/evaluation, and suggests potential efficiency improvements.",
        "type": "summary"
    },
    "520": {
        "file_id": 15,
        "content": "import json\nfrom torchvision import transforms as T\nfrom pydantic import BaseModel, validator, model_validator\nfrom typing import List, Optional, Union, Tuple, Dict, Any, TypeVar\nfrom x_clip import CLIP as XCLIP\nfrom open_clip import list_pretrained\nfrom coca_pytorch import CoCa\nfrom dalle2_pytorch.dalle2_pytorch import (\n    CoCaAdapter,\n    OpenAIClipAdapter,\n    OpenClipAdapter,\n    Unet,\n    Decoder,\n    DiffusionPrior,\n    DiffusionPriorNetwork,\n    XClipAdapter\n)\nfrom dalle2_pytorch.trackers import Tracker, create_loader, create_logger, create_saver\n# helper functions\ndef exists(val):\n    return val is not None\ndef default(val, d):\n    return val if exists(val) else d\nInnerType = TypeVar('InnerType')\nListOrTuple = Union[List[InnerType], Tuple[InnerType]]\nSingularOrIterable = Union[InnerType, ListOrTuple[InnerType]]\n# general pydantic classes\nclass TrainSplitConfig(BaseModel):\n    train: float = 0.75\n    val: float = 0.15\n    test: float = 0.1\n    @model_validator(mode = 'after')\n    def validate_all(self, m):\n        actual_sum = sum([*dict(self).values()])",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:1-43"
    },
    "521": {
        "file_id": 15,
        "content": "This code is defining various classes and functions for training configurations in a machine learning application, specifically related to the DALL-E 2 PyTorch model. It includes importing necessary modules, setting up pydantic models for train splits, and creating utility functions like `default` and `exists`.",
        "type": "comment"
    },
    "522": {
        "file_id": 15,
        "content": "        if actual_sum != 1.:\n            raise ValueError(f'{dict(self).keys()} must sum to 1.0. Found: {actual_sum}')\n        return self\nclass TrackerLogConfig(BaseModel):\n    log_type: str = 'console'\n    resume: bool = False  # For logs that are saved to unique locations, resume a previous run\n    auto_resume: bool = False  # If the process crashes and restarts, resume from the run that crashed\n    verbose: bool = False\n    class Config:\n        # Each individual log type has it's own arguments that will be passed through the config\n        extra = \"allow\"\n    def create(self, data_path: str):\n        kwargs = self.dict()\n        return create_logger(self.log_type, data_path, **kwargs)\nclass TrackerLoadConfig(BaseModel):\n    load_from: Optional[str] = None\n    only_auto_resume: bool = False  # Only attempt to load if the logger is auto-resuming\n    class Config:\n        extra = \"allow\"\n    def create(self, data_path: str):\n        kwargs = self.dict()\n        if self.load_from is None:\n            return None",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:44-73"
    },
    "523": {
        "file_id": 15,
        "content": "The code defines two classes, `TrackerLogConfig` and `TrackerLoadConfig`, which inherit from `BaseModel`. These classes have various attributes such as `log_type`, `resume`, `auto_resume`, and `verbose`. They also have a method called `create` that takes in a `data_path` parameter and returns a logger object. The classes ensure their attributes sum up to 1.0, and allow additional arguments for each individual log type. The `TrackerLoadConfig` class has an optional attribute `load_from`, which determines if the logger should load from a previous run. If `load_from` is set to `None`, it returns None instead of loading.",
        "type": "comment"
    },
    "524": {
        "file_id": 15,
        "content": "        return create_loader(self.load_from, data_path, **kwargs)\nclass TrackerSaveConfig(BaseModel):\n    save_to: str = 'local'\n    save_all: bool = False\n    save_latest: bool = True\n    save_best: bool = True\n    class Config:\n        extra = \"allow\"\n    def create(self, data_path: str):\n        kwargs = self.dict()\n        return create_saver(self.save_to, data_path, **kwargs)\nclass TrackerConfig(BaseModel):\n    data_path: str = '.tracker_data'\n    overwrite_data_path: bool = False\n    log: TrackerLogConfig\n    load: Optional[TrackerLoadConfig] = None\n    save: Union[List[TrackerSaveConfig], TrackerSaveConfig]\n    def create(self, full_config: BaseModel, extra_config: dict, dummy_mode: bool = False) -> Tracker:\n        tracker = Tracker(self.data_path, dummy_mode=dummy_mode, overwrite_data_path=self.overwrite_data_path)\n        # Add the logger\n        tracker.add_logger(self.log.create(self.data_path))\n        # Add the loader\n        if self.load is not None:\n            tracker.add_loader(self.load.create(self.data_path))",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:74-102"
    },
    "525": {
        "file_id": 15,
        "content": "This code defines classes for tracker configuration and load/save operations. The TrackerConfig class contains information about the data path, overwrite option, logger settings, and optional load configurations. The create method of TrackerConfig initializes a new Tracker object and adds a logger if present in the configuration. If there is a defined load configuration, it also adds a loader to the tracker.",
        "type": "comment"
    },
    "526": {
        "file_id": 15,
        "content": "        # Add the saver or savers\n        if isinstance(self.save, list):\n            for save_config in self.save:\n                tracker.add_saver(save_config.create(self.data_path))\n        else:\n            tracker.add_saver(self.save.create(self.data_path))\n        # Initialize all the components and verify that all data is valid\n        tracker.init(full_config, extra_config)\n        return tracker\n# diffusion prior pydantic classes\nclass AdapterConfig(BaseModel):\n    make: str = \"openai\"\n    model: str = \"ViT-L/14\"\n    base_model_kwargs: Optional[Dict[str, Any]] = None\n    def create(self):\n        if self.make == \"openai\":\n            return OpenAIClipAdapter(self.model)\n        elif self.make == \"open_clip\":\n            pretrained = dict(list_pretrained())\n            checkpoint = pretrained[self.model]\n            return OpenClipAdapter(name=self.model, pretrained=checkpoint)\n        elif self.make == \"x-clip\":\n            return XClipAdapter(XCLIP(**self.base_model_kwargs))\n        elif self.make == \"coca\":",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:103-129"
    },
    "527": {
        "file_id": 15,
        "content": "This code defines a function that initializes and returns a tracker object, which is responsible for managing savers and components of the model. It also includes classes for different types of adapters used in the model. The tracker object verifies data validity after initialization.",
        "type": "comment"
    },
    "528": {
        "file_id": 15,
        "content": "            return CoCaAdapter(CoCa(**self.base_model_kwargs))\n        else:\n            raise AttributeError(\"No adapter with that name is available.\")\nclass DiffusionPriorNetworkConfig(BaseModel):\n    dim: int\n    depth: int\n    max_text_len: Optional[int] = None\n    num_timesteps: Optional[int] = None\n    num_time_embeds: int = 1\n    num_image_embeds: int = 1\n    num_text_embeds: int = 1\n    dim_head: int = 64\n    heads: int = 8\n    ff_mult: int = 4\n    norm_in: bool = False\n    norm_out: bool = True\n    attn_dropout: float = 0.\n    ff_dropout: float = 0.\n    final_proj: bool = True\n    normformer: bool = False\n    rotary_emb: bool = True\n    class Config:\n        extra = \"allow\"\n    def create(self):\n        kwargs = self.dict()\n        return DiffusionPriorNetwork(**kwargs)\nclass DiffusionPriorConfig(BaseModel):\n    clip: Optional[AdapterConfig] = None\n    net: DiffusionPriorNetworkConfig\n    image_embed_dim: int\n    image_size: int\n    image_channels: int = 3\n    timesteps: int = 1000\n    sample_timesteps: Optional[int] = None",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:130-167"
    },
    "529": {
        "file_id": 15,
        "content": "This code defines configurations for a neural network model. It includes classes for adapters, diffusion prior networks, and diffusion prior models. The adapter class takes in base_model_kwargs and returns an instance of either CoCaAdapter or raises AttributeError if no matching adapter found. DiffusionPriorNetworkConfig defines the architecture specifications like dimensions, depth, and dropout rates. DiffusionPriorConfig handles configurations for clip adapters, diffusion prior networks, image embedding dimensions, image size, and number of timesteps. The create() function returns an instance of the model based on its configuration.",
        "type": "comment"
    },
    "530": {
        "file_id": 15,
        "content": "    cond_drop_prob: float = 0.\n    loss_type: str = 'l2'\n    predict_x_start: bool = True\n    beta_schedule: str = 'cosine'\n    condition_on_text_encodings: bool = True\n    class Config:\n        extra = \"allow\"\n    def create(self):\n        kwargs = self.dict()\n        has_clip = exists(kwargs.pop('clip'))\n        kwargs.pop('net')\n        clip = None\n        if has_clip:\n            clip = self.clip.create()\n        diffusion_prior_network = self.net.create()\n        return DiffusionPrior(net = diffusion_prior_network, clip = clip, **kwargs)\nclass DiffusionPriorTrainConfig(BaseModel):\n    epochs: int = 1\n    lr: float = 1.1e-4\n    wd: float = 6.02e-2\n    max_grad_norm: float = 0.5\n    use_ema: bool = True\n    ema_beta: float = 0.99\n    amp: bool = False\n    warmup_steps: Optional[int] = None   # number of warmup steps\n    save_every_seconds: int = 3600       # how often to save\n    eval_timesteps: List[int] = [64]     # which sampling timesteps to evaluate with\n    best_validation_loss: float = 1e9    # the current best valudation loss observed",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:168-201"
    },
    "531": {
        "file_id": 15,
        "content": "The code defines a class for training configurations, including epochs, learning rate, weight decay, and other parameters. It also contains functions to create instances of diffusion prior networks and conditioning models. The class is part of the DALLE2-pytorch framework and is used for training the model.",
        "type": "comment"
    },
    "532": {
        "file_id": 15,
        "content": "    current_epoch: int = 0               # the current epoch\n    num_samples_seen: int = 0            # the current number of samples seen\n    random_seed: int = 0                 # manual seed for torch\nclass DiffusionPriorDataConfig(BaseModel):\n    image_url: str                   # path to embeddings folder\n    meta_url: str                    # path to metadata (captions) for images\n    splits: TrainSplitConfig         # define train, validation, test splits for your dataset\n    batch_size: int                  # per-gpu batch size used to train the model\n    num_data_points: int = 25e7      # total number of datapoints to train on\n    eval_every_seconds: int = 3600   # validation statistics will be performed this often\nclass TrainDiffusionPriorConfig(BaseModel):\n    prior: DiffusionPriorConfig\n    data: DiffusionPriorDataConfig\n    train: DiffusionPriorTrainConfig\n    tracker: TrackerConfig\n    @classmethod\n    def from_json_path(cls, json_path):\n        with open(json_path) as f:\n            config = json.load(f)",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:202-223"
    },
    "533": {
        "file_id": 15,
        "content": "The code defines a configuration class for training the DiffusionPrior model, which contains details such as the data source, batch size, total number of datapoints to train on, and validation frequency. It also has methods to load configurations from JSON files.",
        "type": "comment"
    },
    "534": {
        "file_id": 15,
        "content": "        return cls(**config)\n# decoder pydantic classes\nclass UnetConfig(BaseModel):\n    dim: int\n    dim_mults: ListOrTuple[int]\n    image_embed_dim: Optional[int] = None\n    text_embed_dim: Optional[int] = None\n    cond_on_text_encodings: Optional[bool] = None\n    cond_dim: Optional[int] = None\n    channels: int = 3\n    self_attn: SingularOrIterable[bool] = False\n    attn_dim_head: int = 32\n    attn_heads: int = 16\n    init_cross_embed: bool = True\n    class Config:\n        extra = \"allow\"\nclass DecoderConfig(BaseModel):\n    unets: ListOrTuple[UnetConfig]\n    image_size: Optional[int] = None\n    image_sizes: ListOrTuple[int] = None\n    clip: Optional[AdapterConfig] = None   # The clip model to use if embeddings are not provided\n    channels: int = 3\n    timesteps: int = 1000\n    sample_timesteps: Optional[SingularOrIterable[Optional[int]]] = None\n    loss_type: str = 'l2'\n    beta_schedule: Optional[ListOrTuple[str]] = None  # None means all cosine\n    learned_variance: SingularOrIterable[bool] = True\n    image_cond_drop_prob: float = 0.1",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:224-255"
    },
    "535": {
        "file_id": 15,
        "content": "The code defines two Pydantic classes, UnetConfig and DecoderConfig, which represent the configurations for the DALL-E 2 model. The UnetConfig class handles the configuration of the UNet transformer in the decoder while the DecoderConfig class includes various settings like the number of UNet blocks, image size, clip model, timesteps, loss type, and more.",
        "type": "comment"
    },
    "536": {
        "file_id": 15,
        "content": "    text_cond_drop_prob: float = 0.5\n    def create(self):\n        decoder_kwargs = self.dict()\n        unet_configs = decoder_kwargs.pop('unets')\n        unets = [Unet(**config) for config in unet_configs]\n        has_clip = exists(decoder_kwargs.pop('clip'))\n        clip = None\n        if has_clip:\n            clip = self.clip.create()\n        return Decoder(unets, clip=clip, **decoder_kwargs)\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        if exists(values.get('image_size')) ^ exists(image_sizes):\n            return image_sizes\n        raise ValueError('either image_size or image_sizes is required, but not both')\n    class Config:\n        extra = \"allow\"\nclass DecoderDataConfig(BaseModel):\n    webdataset_base_url: str                     # path to a webdataset with jpg images\n    img_embeddings_url: Optional[str] = None     # path to .npy files with embeddings\n    text_embeddings_url: Optional[str] = None    # path to .npy files with embeddings\n    num_workers: int = 4",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:256-284"
    },
    "537": {
        "file_id": 15,
        "content": "This code defines a class \"TrainConfigs\" that creates a decoder for DALL-E 2 training. It uses the Unet architecture, optionally includes CLIP for visual guidance, and allows specifying image sizes through 'image_size' or list of 'image_sizes'. The class also provides configurations for loading data from webdataset with jpg images, embedding files, and setting the number of workers for data loading.",
        "type": "comment"
    },
    "538": {
        "file_id": 15,
        "content": "    batch_size: int = 64\n    start_shard: int = 0\n    end_shard: int = 9999999\n    shard_width: int = 6\n    index_width: int = 4\n    splits: TrainSplitConfig\n    shuffle_train: bool = True\n    resample_train: bool = False\n    preprocessing: Dict[str, Any] = {'ToTensor': True}\n    @property\n    def img_preproc(self):\n        def _get_transformation(transformation_name, **kwargs):\n            if transformation_name == \"RandomResizedCrop\":\n                return T.RandomResizedCrop(**kwargs)\n            elif transformation_name == \"RandomHorizontalFlip\":\n                return T.RandomHorizontalFlip()\n            elif transformation_name == \"ToTensor\":\n                return T.ToTensor()\n        transforms = []\n        for transform_name, transform_kwargs_or_bool in self.preprocessing.items():\n            transform_kwargs = {} if not isinstance(transform_kwargs_or_bool, dict) else transform_kwargs_or_bool\n            transforms.append(_get_transformation(transform_name, **transform_kwargs))\n        return T.Compose(transforms)",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:285-309"
    },
    "539": {
        "file_id": 15,
        "content": "This code defines a training configuration with batch size, sharding settings, transformation preprocessing, and boolean flags for shuffling and resampling. It also includes a property method to generate the image preprocessing transforms based on provided names and optional arguments.",
        "type": "comment"
    },
    "540": {
        "file_id": 15,
        "content": "class DecoderTrainConfig(BaseModel):\n    epochs: int = 20\n    lr: SingularOrIterable[float] = 1e-4\n    wd: SingularOrIterable[float] = 0.01\n    warmup_steps: Optional[SingularOrIterable[int]] = None\n    find_unused_parameters: bool = True\n    static_graph: bool = True\n    max_grad_norm: SingularOrIterable[float] = 0.5\n    save_every_n_samples: int = 100000\n    n_sample_images: int = 6                       # The number of example images to produce when sampling the train and test dataset\n    cond_scale: Union[float, List[float]] = 1.0\n    device: str = 'cuda:0'\n    epoch_samples: Optional[int] = None                      # Limits the number of samples per epoch. None means no limit. Required if resample_train is true as otherwise the number of samples per epoch is infinite.\n    validation_samples: Optional[int] = None                 # Same as above but for validation.\n    save_immediately: bool = False\n    use_ema: bool = True\n    ema_beta: float = 0.999\n    amp: bool = False\n    unet_training_mask: Optional[ListOrTuple[bool]] = None   # If None, use all unets",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:311-329"
    },
    "541": {
        "file_id": 15,
        "content": "This code defines a DecoderTrainConfig class with various configuration options for training the decoder model in DALLE2. The class includes settings for epochs, learning rate, weight decay, warmup steps, finding unused parameters, static graph usage, gradient clipping, saving samples, generating example images, scaling conditions, device selection, sample limits per epoch and validation, saving immediately, using exponential moving average (EMA), EMA beta value, using mixed precision training (AMP), and unet training masks.",
        "type": "comment"
    },
    "542": {
        "file_id": 15,
        "content": "class DecoderEvaluateConfig(BaseModel):\n    n_evaluation_samples: int = 1000\n    FID: Optional[Dict[str, Any]] = None\n    IS: Optional[Dict[str, Any]] = None\n    KID: Optional[Dict[str, Any]] = None\n    LPIPS: Optional[Dict[str, Any]] = None\nclass TrainDecoderConfig(BaseModel):\n    decoder: DecoderConfig\n    data: DecoderDataConfig\n    train: DecoderTrainConfig\n    evaluate: DecoderEvaluateConfig\n    tracker: TrackerConfig\n    seed: int = 0\n    @classmethod\n    def from_json_path(cls, json_path):\n        with open(json_path) as f:\n            config = json.load(f)\n            print(config)\n        return cls(**config)\n    @model_validator(mode = 'after')\n    def check_has_embeddings(self, m):\n        # Makes sure that enough information is provided to get the embeddings specified for training\n        values = dict(self)\n        data_config, decoder_config = values.get('data'), values.get('decoder')\n        if not exists(data_config) or not exists(decoder_config):\n            # Then something else errored and we should just pass through",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:331-361"
    },
    "543": {
        "file_id": 15,
        "content": "This code defines two classes, \"DecoderEvaluateConfig\" and \"TrainDecoderConfig\", which inherit from the \"BaseModel\" class. The \"DecoderEvaluateConfig\" class specifies evaluation metrics like FID, IS, KID, and LPIPS, while the \"TrainDecoderConfig\" class combines various configuration elements including a decoder, data, training settings, evaluation settings, tracker, and seed. The \"from_json_path\" method loads configuration from a JSON file, and the \"check_has_embeddings\" validator ensures that enough information is provided to get the embeddings for training.",
        "type": "comment"
    },
    "544": {
        "file_id": 15,
        "content": "            return values\n        using_text_embeddings = any([unet.cond_on_text_encodings for unet in decoder_config.unets])\n        using_clip = exists(decoder_config.clip)\n        img_emb_url = data_config.img_embeddings_url\n        text_emb_url = data_config.text_embeddings_url\n        if using_text_embeddings:\n            # Then we need some way to get the embeddings\n            assert using_clip or exists(text_emb_url), 'If text conditioning, either clip or text_embeddings_url must be provided'\n        if using_clip:\n            if using_text_embeddings:\n                assert not exists(text_emb_url) or not exists(img_emb_url), 'Loaded clip, but also provided text_embeddings_url and img_embeddings_url. This is redundant. Remove the clip model or the text embeddings'\n            else:\n                assert not exists(img_emb_url), 'Loaded clip, but also provided img_embeddings_url. This is redundant. Remove the clip model or the embeddings'\n        if text_emb_url:\n            assert using_te",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:362-380"
    },
    "545": {
        "file_id": 15,
        "content": "This code checks if the text embeddings and/or CLIP model are being used, ensuring that only one of these is provided to avoid redundancy. It asserts that either the CLIP or text embeddings URL must be present if text conditioning is enabled, and if only the CLIP model is loaded, it asserts that neither the text embeddings nor image embeddings URL should be provided.",
        "type": "comment"
    },
    "546": {
        "file_id": 15,
        "content": "xt_embeddings, \"Text embeddings are being loaded, but text embeddings are not being conditioned on. This will slow down the dataloader for no reason.\"\n        return m",
        "type": "code",
        "location": "/dalle2_pytorch/train_configs.py:380-382"
    },
    "547": {
        "file_id": 15,
        "content": "This code snippet indicates that text embeddings are being loaded but are not necessary for the task, causing unnecessary slowdown in the dataloader. It is recommended to remove this step for efficiency.",
        "type": "comment"
    },
    "548": {
        "file_id": 16,
        "content": "/dalle2_pytorch/trainer.py",
        "type": "filepath"
    },
    "549": {
        "file_id": 16,
        "content": "The code initializes DeepSpeed's trainer, sets model parameters, distributes the model, and handles precision. It also initializes optimizers and schedulers, prepares dataloaders, validates compatibility, performs computations, and returns total loss.",
        "type": "summary"
    },
    "550": {
        "file_id": 16,
        "content": "import time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom functools import partial, wraps\nfrom contextlib import nullcontext\nfrom collections.abc import Iterable\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR\nfrom torch.cuda.amp import autocast, GradScaler\nfrom dalle2_pytorch.dalle2_pytorch import Decoder, DiffusionPrior\nfrom dalle2_pytorch.optimizer import get_optimizer\nfrom dalle2_pytorch.version import __version__\nfrom packaging import version\nimport pytorch_warmup as warmup\nfrom ema_pytorch import EMA\nfrom accelerate import Accelerator, DistributedType\nimport numpy as np\n# helper functions\ndef exists(val):\n    return val is not None\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\ndef cast_tuple(val, length = 1):\n    return val if isinstance(val, tuple) else ((val,) * length)\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:1-43"
    },
    "551": {
        "file_id": 16,
        "content": "The code imports various libraries and defines several utility functions for working with tensors, learning rates, optimizers, and distributed training. It also includes helper functions to handle default values and handle dictionaries. These utilities are likely used throughout the codebase to train and evaluate models efficiently.",
        "type": "comment"
    },
    "552": {
        "file_id": 16,
        "content": "def group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n# decorators\ndef cast_torch_tensor(fn):\n    @wraps(fn)\n    def inner(model, *args, **kwargs):\n        device = kwargs.pop('_device', next(model.parameters()).device)\n        cast_device = kwargs.pop('_cast_device', True)",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:45-78"
    },
    "553": {
        "file_id": 16,
        "content": "group_dict_by_key: Creates two dictionaries, one for keys that match the condition and another for those that do not, grouping by key.\nstring_begins_with: Returns a boolean value indicating whether a given string starts with a specified prefix.\ngroup_by_key_prefix: Groups dictionary items based on whether their keys start with a certain prefix.\ngroupby_prefix_and_trim: Similar to group_by_key_prefix, but also trims the common prefix from the keys and returns two dictionaries.\nnum_to_groups: Divides a given number into groups based on a specified divisor, appending any remainder to the last group.\ncast_torch_tensor: A decorator that wraps a function to cast its input and output tensors to specific devices.",
        "type": "comment"
    },
    "554": {
        "file_id": 16,
        "content": "        cast_deepspeed_precision = kwargs.pop('_cast_deepspeed_precision', True)\n        kwargs_keys = kwargs.keys()\n        all_args = (*args, *kwargs.values())\n        split_kwargs_index = len(all_args) - len(kwargs_keys)\n        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))\n        if cast_device:\n            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))\n        if cast_deepspeed_precision:\n            try:\n                accelerator = model.accelerator\n                if accelerator is not None and accelerator.distributed_type == DistributedType.DEEPSPEED:\n                    cast_type_map = {\n                        \"fp16\": torch.half,\n                        \"bf16\": torch.bfloat16,\n                        \"no\": torch.float\n                    }\n                    precision_type = cast_type_map[accelerator.mixed_precision]\n                    all_args = tuple(map(lambda t: t.to(precision_type) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:79-99"
    },
    "555": {
        "file_id": 16,
        "content": "This code handles argument casting and device assignment for a DeepSpeed-accelerated PyTorch model. It first checks if arguments are DeepSpeed precision types, then casts the tensors to the appropriate type if necessary. This ensures that the model's arguments are correctly prepared for training or evaluation within a DeepSpeed framework.",
        "type": "comment"
    },
    "556": {
        "file_id": 16,
        "content": "            except AttributeError:\n                # Then this model doesn't have an accelerator\n                pass\n        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))\n        out = fn(model, *args, **kwargs)\n        return out\n    return inner\n# gradient accumulation functions\ndef split_iterable(it, split_size):\n    accum = []\n    for ind in range(ceil(len(it) / split_size)):\n        start_index = ind * split_size\n        accum.append(it[start_index: (start_index + split_size)])\n    return accum\ndef split(t, split_size = None):\n    if not exists(split_size):\n        return t\n    if isinstance(t, torch.Tensor):\n        return t.split(split_size, dim = 0)\n    if isinstance(t, Iterable):\n        return split_iterable(t, split_size)\n    return TypeError\ndef find_first(cond, arr):\n    for el in arr:\n        if cond(el):\n            return el\n    return None\ndef split_args_and_kwargs(*args, split_size = None, **kwargs):",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:100-138"
    },
    "557": {
        "file_id": 16,
        "content": "This code defines functions for splitting arguments and keywords, as well as handling gradient accumulation. It includes a function to split an iterable into chunks of specified size (`split_iterable`), a `split` function for tensors and iterables, and a `find_first` function to find the first item in an array that meets a given condition. The last function defined is `split_args_and_kwargs`, which splits arguments and keywords based on a specified size.",
        "type": "comment"
    },
    "558": {
        "file_id": 16,
        "content": "    all_args = (*args, *kwargs.values())\n    len_all_args = len(all_args)\n    first_tensor = find_first(lambda t: isinstance(t, torch.Tensor), all_args)\n    assert exists(first_tensor)\n    batch_size = len(first_tensor)\n    split_size = default(split_size, batch_size)\n    num_chunks = ceil(batch_size / split_size)\n    dict_len = len(kwargs)\n    dict_keys = kwargs.keys()\n    split_kwargs_index = len_all_args - dict_len\n    split_all_args = [split(arg, split_size = split_size) if exists(arg) and isinstance(arg, (torch.Tensor, Iterable)) else ((arg,) * num_chunks) for arg in all_args]\n    chunk_sizes = tuple(map(len, split_all_args[0]))\n    for (chunk_size, *chunked_all_args) in tuple(zip(chunk_sizes, *split_all_args)):\n        chunked_args, chunked_kwargs_values = chunked_all_args[:split_kwargs_index], chunked_all_args[split_kwargs_index:]\n        chunked_kwargs = dict(tuple(zip(dict_keys, chunked_kwargs_values)))\n        chunk_size_frac = chunk_size / batch_size\n        yield chunk_size_frac, (chunked_args, chunked_kwargs)",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:139-159"
    },
    "559": {
        "file_id": 16,
        "content": "This code splits the input arguments and keyword arguments into chunks based on batch size, split size, and dictionary keys. It then yields the chunk size fraction and the split chunked arguments and keyword arguments for further processing.",
        "type": "comment"
    },
    "560": {
        "file_id": 16,
        "content": "# diffusion prior trainer\ndef prior_sample_in_chunks(fn):\n    @wraps(fn)\n    def inner(self, *args, max_batch_size = None, **kwargs):\n        if not exists(max_batch_size):\n            return fn(self, *args, **kwargs)\n        outputs = [fn(self, *chunked_args, **chunked_kwargs) for _, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs)]\n        return torch.cat(outputs, dim = 0)\n    return inner\nclass DiffusionPriorTrainer(nn.Module):\n    def __init__(\n        self,\n        diffusion_prior,\n        accelerator = None,\n        use_ema = True,\n        lr = 3e-4,\n        wd = 1e-2,\n        eps = 1e-6,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        **kwargs\n    ):\n        super().__init__()\n        assert isinstance(diffusion_prior, DiffusionPrior)\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n        accelerator_kwargs, kwargs = groupby_prefix_and_trim('accelerator_', kwargs)",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:161-192"
    },
    "561": {
        "file_id": 16,
        "content": "This code defines a `DiffusionPriorTrainer` class that takes in a `diffusion_prior`, and allows for training with different batch sizes by splitting arguments and keywords into chunks. It also supports optional accelerator, learning rate, weight decay, epsilon, max gradient norm, grouped weight decay parameters, warmup steps, and cosine decay maximum steps.",
        "type": "comment"
    },
    "562": {
        "file_id": 16,
        "content": "        if not exists(accelerator):\n            accelerator = Accelerator(**accelerator_kwargs)\n        # assign some helpful member vars\n        self.accelerator = accelerator\n        self.text_conditioned = diffusion_prior.condition_on_text_encodings\n        # setting the device\n        self.device = accelerator.device\n        diffusion_prior.to(self.device)\n        # save model\n        self.diffusion_prior = diffusion_prior\n        # mixed precision checks\n        if (\n            exists(self.accelerator) \n            and self.accelerator.distributed_type == DistributedType.DEEPSPEED \n            and self.diffusion_prior.clip is not None\n            ):\n            # Then we need to make sure clip is using the correct precision or else deepspeed will error\n            cast_type_map = {\n                \"fp16\": torch.half,\n                \"bf16\": torch.bfloat16,\n                \"no\": torch.float\n            }\n            precision_type = cast_type_map[accelerator.mixed_precision]\n            assert precision",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:194-225"
    },
    "563": {
        "file_id": 16,
        "content": "Checking if an accelerator is specified, assigning member variables for helpful operations, setting device and transferring model to that device, saving the diffusion prior model, and checking mixed precision settings if applicable.",
        "type": "comment"
    },
    "564": {
        "file_id": 16,
        "content": "_type == torch.float, \"DeepSpeed currently only supports float32 precision when using on the fly embedding generation from clip\"\n            self.diffusion_prior.clip.to(precision_type)\n        # optimizer stuff\n        self.optim_kwargs = dict(lr=lr, wd=wd, eps=eps, group_wd_params=group_wd_params)\n        self.optimizer = get_optimizer(\n            self.diffusion_prior.parameters(),\n            **self.optim_kwargs,\n            **kwargs\n        )\n        if exists(cosine_decay_max_steps):\n            self.scheduler = CosineAnnealingLR(self.optimizer, T_max = cosine_decay_max_steps)\n        else:\n            self.scheduler = LambdaLR(self.optimizer, lr_lambda = lambda _: 1.0)\n        self.warmup_scheduler = warmup.LinearWarmup(self.optimizer, warmup_period = warmup_steps) if exists(warmup_steps) else None\n        # distribute the model if using HFA\n        self.diffusion_prior, self.optimizer, self.scheduler = self.accelerator.prepare(self.diffusion_prior, self.optimizer, self.scheduler)\n        # exponential moving average stuff",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:225-249"
    },
    "565": {
        "file_id": 16,
        "content": "This code initializes the trainer for DeepSpeed, setting precision, optimizer, and scheduler. It checks if on-the-fly embedding generation from CLIP is supported and changes precision accordingly. It also distributes the model using HFA and applies exponential moving average techniques.",
        "type": "comment"
    },
    "566": {
        "file_id": 16,
        "content": "        self.use_ema = use_ema\n        if self.use_ema:\n            self.ema_diffusion_prior = EMA(self.accelerator.unwrap_model(self.diffusion_prior), **ema_kwargs)\n        # gradient clipping if needed\n        self.max_grad_norm = max_grad_norm\n        # track steps internally\n        self.register_buffer('step', torch.tensor([0], device = self.device))\n    # utility\n    def save(self, path, overwrite = True, **kwargs):\n        # only save on the main process\n        if self.accelerator.is_main_process:\n            print(f\"Saving checkpoint at step: {self.step.item()}\")\n            path = Path(path)\n            assert not (path.exists() and not overwrite)\n            path.parent.mkdir(parents = True, exist_ok = True)\n            # FIXME: LambdaLR can't be saved due to pickling issues\n            save_obj = dict(\n                optimizer = self.optimizer.state_dict(),\n                scheduler = self.scheduler.state_dict(),\n                warmup_scheduler = self.warmup_scheduler,\n                model = self.accelerator.unwrap_model(self.diffusion_prior).state_dict(),",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:251-280"
    },
    "567": {
        "file_id": 16,
        "content": "The code snippet initializes a trainer object with an option for exponential moving average (EMA), gradient clipping, and tracks steps internally. It also defines a save method to save the optimizer, scheduler, model state dictionaries, and warmup scheduler on the main process. Note that LambdaLR cannot be saved due to pickling issues.",
        "type": "comment"
    },
    "568": {
        "file_id": 16,
        "content": "                version = version.parse(__version__),\n                step = self.step,\n                **kwargs\n            )\n            if self.use_ema:\n                save_obj = {\n                    **save_obj,\n                    'ema': self.ema_diffusion_prior.state_dict(),\n                    'ema_model': self.ema_diffusion_prior.ema_model.state_dict() # save the ema model specifically for easy ema-only reload\n                }\n            torch.save(save_obj, str(path))\n    def load(self, path_or_state, overwrite_lr = True, strict = True):\n        \"\"\"\n        Load a checkpoint of a diffusion prior trainer.\n        Will load the entire trainer, including the optimizer and EMA.\n        Params:\n            - path_or_state (str | torch): a path to the DiffusionPriorTrainer checkpoint file\n            - overwrite_lr (bool): wether or not to overwrite the stored LR with the LR specified in the new trainer\n            - strict (bool): kwarg for `torch.nn.Module.load_state_dict`, will force an exact checkpoint match",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:281-304"
    },
    "569": {
        "file_id": 16,
        "content": "This code saves and loads a checkpoint for a diffusion prior trainer. It also handles saving the EMA (Exponential Moving Average) model separately for easy ema-only reload, and allows overwriting the learning rate if needed. The `load` method loads an entire trainer, including its optimizer and EMA.",
        "type": "comment"
    },
    "570": {
        "file_id": 16,
        "content": "        Returns:\n            loaded_obj (dict): The loaded checkpoint dictionary\n        \"\"\"\n        # all processes need to load checkpoint. no restriction here\n        if isinstance(path_or_state, str):\n            path = Path(path_or_state)\n            assert path.exists()\n            loaded_obj = torch.load(str(path), map_location=self.device)\n        elif isinstance(path_or_state, dict):\n            loaded_obj = path_or_state\n        if version.parse(__version__) != loaded_obj['version']:\n            print(f'loading saved diffusion prior at version {loaded_obj[\"version\"]} but current package version is at {__version__}')\n        # unwrap the model when loading from checkpoint\n        self.accelerator.unwrap_model(self.diffusion_prior).load_state_dict(loaded_obj['model'], strict = strict)\n        self.step.copy_(torch.ones_like(self.step, device=self.device) * loaded_obj['step'].to(self.device))\n        self.optimizer.load_state_dict(loaded_obj['optimizer'])\n        self.scheduler.load_state_dict(loaded_obj['scheduler'])",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:306-327"
    },
    "571": {
        "file_id": 16,
        "content": "This function loads a checkpoint from a specified path or dictionary, handling both string paths and existing dictionaries. It checks if the loaded version matches the current package version, then unwraps and loads the model's state dict, sets step values, and loads optimizer and scheduler states as well.",
        "type": "comment"
    },
    "572": {
        "file_id": 16,
        "content": "        # set warmupstep\n        if exists(self.warmup_scheduler):\n            self.warmup_scheduler.last_step = self.step.item()\n        # ensure new lr is used if different from old one\n        if overwrite_lr:\n            new_lr = self.optim_kwargs[\"lr\"]\n            for group in self.optimizer.param_groups:\n                group[\"lr\"] = new_lr if group[\"lr\"] > 0.0 else 0.0\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            self.ema_diffusion_prior.load_state_dict(loaded_obj['ema'], strict = strict)\n            # below might not be necessary, but I had a suspicion that this wasn't being loaded correctly\n            self.ema_diffusion_prior.ema_model.load_state_dict(loaded_obj[\"ema_model\"])\n        return loaded_obj\n    # model functionality\n    def update(self):\n        if exists(self.max_grad_norm):\n            self.accelerator.clip_grad_norm_(self.diffusion_prior.parameters(), self.max_grad_norm)\n        self.optimizer.step()\n        self.optimizer.zero_grad()\n        # accelerator will ocassionally skip optimizer steps in a \"dynamic loss scaling strategy\"",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:329-358"
    },
    "573": {
        "file_id": 16,
        "content": "This function handles the warmup step, updating the learning rate if needed, loading EMA diffusion prior state from a checkpoint, and performing model update with optimization.",
        "type": "comment"
    },
    "574": {
        "file_id": 16,
        "content": "        if not self.accelerator.optimizer_step_was_skipped:\n            sched_context = self.warmup_scheduler.dampening if exists(self.warmup_scheduler) else nullcontext\n            with sched_context():\n                self.scheduler.step()\n        if self.use_ema:\n            self.ema_diffusion_prior.update()\n        self.step += 1\n    @torch.no_grad()\n    @cast_torch_tensor\n    @prior_sample_in_chunks\n    def p_sample_loop(self, *args, **kwargs):\n        model = self.ema_diffusion_prior.ema_model if self.use_ema else self.diffusion_prior\n        return model.p_sample_loop(*args, **kwargs)\n    @torch.no_grad()\n    @cast_torch_tensor\n    @prior_sample_in_chunks\n    def sample(self, *args, **kwargs):\n        model = self.ema_diffusion_prior.ema_model if self.use_ema else self.diffusion_prior\n        return model.sample(*args, **kwargs)\n    @torch.no_grad()\n    def sample_batch_size(self, *args, **kwargs):\n        model = self.ema_diffusion_prior.ema_model if self.use_ema else self.diffusion_prior\n        return model.sample_batch_size(*args, **kwargs)",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:359-386"
    },
    "575": {
        "file_id": 16,
        "content": "The code defines several methods for using the diffusion prior model to generate samples. It uses exponential moving average (EMA) for model averaging, if `use_ema` is enabled. The `p_sample_loop`, `sample`, and `sample_batch_size` methods use `torch.no_grad()` for performance optimization, and `cast_torch_tensor` and `prior_sample_in_chunks` decorators are used to process data in chunks.",
        "type": "comment"
    },
    "576": {
        "file_id": 16,
        "content": "    @torch.no_grad()\n    @cast_torch_tensor\n    @prior_sample_in_chunks\n    def embed_text(self, *args, **kwargs):\n        return self.accelerator.unwrap_model(self.diffusion_prior).clip.embed_text(*args, **kwargs)\n    @cast_torch_tensor\n    def forward(\n        self,\n        *args,\n        max_batch_size = None,\n        **kwargs\n    ):\n        total_loss = 0.\n        for chunk_size_frac, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs):\n            with self.accelerator.autocast():\n                loss = self.diffusion_prior(*chunked_args, **chunked_kwargs)\n                loss = loss * chunk_size_frac\n            total_loss += loss.item()\n            if self.training:\n                self.accelerator.backward(loss)\n        return total_loss\n# decoder trainer\ndef decoder_sample_in_chunks(fn):\n    @wraps(fn)\n    def inner(self, *args, max_batch_size = None, **kwargs):\n        if not exists(max_batch_size):\n            return fn(self, *args, **kwargs)\n        if self.decoder.unconditional:",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:388-423"
    },
    "577": {
        "file_id": 16,
        "content": "This code defines a trainer with a function `embed_text` that uses the unwrapped model for embedding text, and a `forward` method that performs forward pass in chunks to handle large batch sizes. The `decoder_sample_in_chunks` decorator enables chunking when sample decoding.",
        "type": "comment"
    },
    "578": {
        "file_id": 16,
        "content": "            batch_size = kwargs.get('batch_size')\n            batch_sizes = num_to_groups(batch_size, max_batch_size)\n            outputs = [fn(self, *args, **{**kwargs, 'batch_size': sub_batch_size}) for sub_batch_size in batch_sizes]\n        else:\n            outputs = [fn(self, *chunked_args, **chunked_kwargs) for _, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs)]\n        return torch.cat(outputs, dim = 0)\n    return inner\nclass DecoderTrainer(nn.Module):\n    def __init__(\n        self,\n        decoder,\n        accelerator = None,\n        dataloaders = None,\n        use_ema = True,\n        lr = 1e-4,\n        wd = 1e-2,\n        eps = 1e-8,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        max_grad_norm = 0.5,\n        amp = False,\n        group_wd_params = True,\n        **kwargs\n    ):\n        super().__init__()\n        assert isinstance(decoder, Decoder)\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n        self.accelerator = default(accelerator, Accelerator)",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:424-454"
    },
    "579": {
        "file_id": 16,
        "content": "The function is a trainer that takes a decoder, accelerator, and other parameters. It can handle batching the inputs or splitting arguments and keywords to train the decoder in chunks, depending on the size of input data. The returned inner function is used for training the model using the provided configuration.",
        "type": "comment"
    },
    "580": {
        "file_id": 16,
        "content": "        self.num_unets = len(decoder.unets)\n        self.use_ema = use_ema\n        self.ema_unets = nn.ModuleList([])\n        self.amp = amp\n        # be able to finely customize learning rate, weight decay\n        # per unet\n        lr, wd, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, wd, eps, warmup_steps, cosine_decay_max_steps))\n        assert all([unet_lr <= 1e-2 for unet_lr in lr]), 'your learning rate is too high, recommend sticking with 1e-4, at most 5e-4'\n        optimizers = []\n        schedulers = []\n        warmup_schedulers = []\n        for unet, unet_lr, unet_wd, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps in zip(decoder.unets, lr, wd, eps, warmup_steps, cosine_decay_max_steps):\n            if isinstance(unet, nn.Identity):\n                optimizers.append(None)\n                schedulers.append(None)\n                warmup_schedulers.append(None)\n            else:\n                optimizer = get_optimizer(\n                    unet.parameters(),",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:456-481"
    },
    "581": {
        "file_id": 16,
        "content": "The code initializes the trainer with specific configurations for each UNET in the decoder. It checks learning rate, weight decay, warmup steps, and cosine decay max steps for each UNET. If a UNET is an identity, it assigns no optimizer or scheduler. Otherwise, it gets an appropriate optimizer for the UNET's parameters.",
        "type": "comment"
    },
    "582": {
        "file_id": 16,
        "content": "                    lr = unet_lr,\n                    wd = unet_wd,\n                    eps = unet_eps,\n                    group_wd_params = group_wd_params,\n                    **kwargs\n                )\n                optimizers.append(optimizer)\n                if exists(unet_cosine_decay_max_steps):\n                    scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n                else:\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps) if exists(unet_warmup_steps) else None\n                warmup_schedulers.append(warmup_scheduler)\n                schedulers.append(scheduler)\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n        # gradient clipping if needed\n        self.max_grad_norm = max_grad_norm\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n        if self.accelerator.distributed_type == DistributedType.DEEPSPEED and decoder.clip is not None:",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:482-510"
    },
    "583": {
        "file_id": 16,
        "content": "The code initializes optimizers, optionally schedulers for learning rate adjustments, and an exponential moving average (EMA) for the UNETs. It also registers a buffer for tracking steps and handles gradient clipping if needed based on distributed type.",
        "type": "comment"
    },
    "584": {
        "file_id": 16,
        "content": "            # Then we need to make sure clip is using the correct precision or else deepspeed will error\n            cast_type_map = {\n                \"fp16\": torch.half,\n                \"bf16\": torch.bfloat16,\n                \"no\": torch.float\n            }\n            precision_type = cast_type_map[accelerator.mixed_precision]\n            assert precision_type == torch.float, \"DeepSpeed currently only supports float32 precision when using on the fly embedding generation from clip\"\n            clip = decoder.clip\n            clip.to(precision_type)\n        decoder, *optimizers = list(self.accelerator.prepare(decoder, *optimizers))\n        self.decoder = decoder\n        # prepare dataloaders\n        train_loader = val_loader = None\n        if exists(dataloaders):\n            train_loader, val_loader = self.accelerator.prepare(dataloaders[\"train\"], dataloaders[\"val\"])\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        # store optimizers\n        for opt_ind, optimizer in zip(range(len(optimizers)), optimizers):",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:511-537"
    },
    "585": {
        "file_id": 16,
        "content": "This code ensures that the correct precision is used by DeepSpeed and prepares the decoder, optimizers, and dataloaders for training. It converts the clip to the specified precision type, then prepares them using DeepSpeed's accelerator. The train_loader and val_loader are stored for later use.",
        "type": "comment"
    },
    "586": {
        "file_id": 16,
        "content": "            setattr(self, f'optim{opt_ind}', optimizer)\n        # store schedulers\n        for sched_ind, scheduler in zip(range(len(schedulers)), schedulers):\n            setattr(self, f'sched{sched_ind}', scheduler)\n        # store warmup schedulers\n        self.warmup_schedulers = warmup_schedulers\n    def validate_and_return_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n        assert exists(unet_number) and 1 <= unet_number <= self.num_unets\n        return unet_number\n    def num_steps_taken(self, unet_number = None):\n        unet_number = self.validate_and_return_unet_number(unet_number)\n        return self.steps[unet_number - 1].item()\n    def save(self, path, overwrite = True, **kwargs):\n        path = Path(path)\n        assert not (path.exists() and not overwrite)\n        path.parent.mkdir(parents = True, exist_ok = True)\n        save_obj = dict(\n            model = self.accelerator.unwrap_model(self.decoder).state_dict(),",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:538-566"
    },
    "587": {
        "file_id": 16,
        "content": "This code defines a class with optimizers, schedulers, and warmup schedulers. It also validates the unet number and returns the number of steps taken by a specific unet. The save function saves the model's state dict to a specified path.",
        "type": "comment"
    },
    "588": {
        "file_id": 16,
        "content": "            version = __version__,\n            steps = self.steps.cpu(),\n            **kwargs\n        )\n        for ind in range(0, self.num_unets):\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'sched{ind}'\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            optimizer_state_dict = optimizer.state_dict() if exists(optimizer) else None\n            scheduler_state_dict = scheduler.state_dict() if exists(scheduler) else None\n            save_obj = {**save_obj, optimizer_key: optimizer_state_dict, scheduler_key: scheduler_state_dict}\n        if self.use_ema:\n            save_obj = {**save_obj, 'ema': self.ema_unets.state_dict()}\n        self.accelerator.save(save_obj, str(path))\n    def load_state_dict(self, loaded_obj, only_model = False, strict = True):\n        if version.parse(__version__) != version.parse(loaded_obj['version']):\n            self.accelerator.print(f'loading saved decoder at version {loaded_obj[\"version\"]}, but current package version is {__version__}')",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:567-591"
    },
    "589": {
        "file_id": 16,
        "content": "This code snippet saves the model state, optimizer state, and scheduler state if they exist, and an optional Exponential Moving Average (EMA) state. It checks the version compatibility before loading the saved state dictionary.",
        "type": "comment"
    },
    "590": {
        "file_id": 16,
        "content": "        self.accelerator.unwrap_model(self.decoder).load_state_dict(loaded_obj['model'], strict = strict)\n        self.steps.copy_(loaded_obj['steps'])\n        if only_model:\n            return loaded_obj\n        for ind, last_step in zip(range(0, self.num_unets), self.steps.tolist()):\n            optimizer_key = f'optim{ind}'\n            optimizer = getattr(self, optimizer_key)\n            scheduler_key = f'sched{ind}'\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = self.warmup_schedulers[ind]\n            if exists(optimizer):\n                optimizer.load_state_dict(loaded_obj[optimizer_key])\n            if exists(scheduler):\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n            if exists(warmup_scheduler):\n                warmup_scheduler.last_step = last_step\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n    def load(self, path, only_model = False, strict = True):",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:593-622"
    },
    "591": {
        "file_id": 16,
        "content": "This code loads a model and its associated optimizers, schedulers, and warmup schedulers from the given path. It also checks if early-stopping (ema) was used and loads that as well. The function returns the loaded state of each component if only_model is True, otherwise it continues with training.",
        "type": "comment"
    },
    "592": {
        "file_id": 16,
        "content": "        path = Path(path)\n        assert path.exists()\n        loaded_obj = torch.load(str(path), map_location = 'cpu')\n        self.load_state_dict(loaded_obj, only_model = only_model, strict = strict)\n        return loaded_obj\n    @property\n    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n    def increment_step(self, unet_number):\n        assert 1 <= unet_number <= self.num_unets\n        unet_index_tensor = torch.tensor(unet_number - 1, device = self.steps.device)\n        self.steps += F.one_hot(unet_index_tensor, num_classes = len(self.steps))\n    def update(self, unet_number = None):\n        unet_number = self.validate_and_return_unet_number(unet_number)\n        index = unet_number - 1\n        optimizer = getattr(self, f'optim{index}')\n        scheduler = getattr(self, f'sched{index}')\n        if exists(self.max_grad_norm):\n            self.accelerator.clip_grad_norm_(self.decoder.parameters(), self.max_grad_norm)  # Automatically unscales gradients\n        optimizer.step()",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:623-652"
    },
    "593": {
        "file_id": 16,
        "content": "This function loads a saved state and returns it. It also provides access to the unets (U-Nets) in the model and allows incrementing the step of a specific unet. The update method updates the optimizer and scheduler for a specified unet.",
        "type": "comment"
    },
    "594": {
        "file_id": 16,
        "content": "        optimizer.zero_grad()\n        warmup_scheduler = self.warmup_schedulers[index]\n        scheduler_context = warmup_scheduler.dampening if exists(warmup_scheduler) else nullcontext\n        with scheduler_context():\n            scheduler.step()\n        if self.use_ema:\n            ema_unet = self.ema_unets[index]\n            ema_unet.update()\n        self.increment_step(unet_number)\n    @torch.no_grad()\n    @cast_torch_tensor\n    @decoder_sample_in_chunks\n    def sample(self, *args, **kwargs):\n        distributed = self.accelerator.num_processes > 1\n        base_decoder = self.accelerator.unwrap_model(self.decoder)\n        was_training = base_decoder.training\n        base_decoder.eval()\n        if kwargs.pop('use_non_ema', False) or not self.use_ema:\n            out = base_decoder.sample(*args, **kwargs, distributed = distributed)\n            base_decoder.train(was_training)\n            return out\n        trainable_unets = self.accelerator.unwrap_model(self.decoder).unets\n        base_decoder.unets = self.unets                  # swap in exponential moving averaged unets for sampling",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:653-683"
    },
    "595": {
        "file_id": 16,
        "content": "This code is responsible for the sampling process in a specific model. It uses gradient descent to optimize the model and updates the exponential moving average (EMA) unets if ema is enabled. The sample function enables evaluation mode, handles non-ema usage or disabled use_ema, and returns the output based on the input arguments. The distributed argument is used for multi-process sampling.",
        "type": "comment"
    },
    "596": {
        "file_id": 16,
        "content": "        output = base_decoder.sample(*args, **kwargs, distributed = distributed)\n        base_decoder.unets = trainable_unets             # restore original training unets\n        # cast the ema_model unets back to original device\n        for ema in self.ema_unets:\n            ema.restore_ema_model_device()\n        base_decoder.train(was_training)\n        return output\n    @torch.no_grad()\n    @cast_torch_tensor\n    @prior_sample_in_chunks\n    def embed_text(self, *args, **kwargs):\n        return self.accelerator.unwrap_model(self.decoder).clip.embed_text(*args, **kwargs)\n    @torch.no_grad()\n    @cast_torch_tensor\n    @prior_sample_in_chunks\n    def embed_image(self, *args, **kwargs):\n        return self.accelerator.unwrap_model(self.decoder).clip.embed_image(*args, **kwargs)\n    @cast_torch_tensor\n    def forward(\n        self,\n        *args,\n        unet_number = None,\n        max_batch_size = None,\n        return_lowres_cond_image=False,\n        **kwargs\n    ):\n        unet_number = self.validate_and_return_unet_number(unet_number)",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:685-717"
    },
    "597": {
        "file_id": 16,
        "content": "This code defines a function for embedding text and image using the decoder's CLIP module. It also restores the original training unets, casts torch tensors, validates and returns the correct unet number, and allows for conditional lowres image return.",
        "type": "comment"
    },
    "598": {
        "file_id": 16,
        "content": "        total_loss = 0.\n        cond_images = []\n        for chunk_size_frac, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs):\n            with self.accelerator.autocast():\n                loss_obj = self.decoder(*chunked_args, unet_number = unet_number, return_lowres_cond_image=return_lowres_cond_image, **chunked_kwargs)\n                # loss_obj may be a tuple with loss and cond_image\n                if return_lowres_cond_image:\n                    loss, cond_image = loss_obj\n                else:\n                    loss = loss_obj\n                    cond_image = None\n                loss = loss * chunk_size_frac\n                if cond_image is not None:\n                    cond_images.append(cond_image)\n            total_loss += loss.item()\n            if self.training:\n                self.accelerator.backward(loss)\n        if return_lowres_cond_image:\n            return total_loss, torch.stack(cond_images)\n        else:\n            return total_loss",
        "type": "code",
        "location": "/dalle2_pytorch/trainer.py:719-742"
    },
    "599": {
        "file_id": 16,
        "content": "This code chunk splits the input arguments and keywords into multiple smaller chunks, then iterates over them to perform computations with auto-cast enabled. The resulting losses are accumulated, and if conditional images are returned, they are stacked together. Finally, the total loss is returned.",
        "type": "comment"
    }
}