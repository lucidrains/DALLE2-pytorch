{
    "600": {
        "file_id": 17,
        "content": "/dalle2_pytorch/utils.py",
        "type": "filepath"
    },
    "601": {
        "file_id": 17,
        "content": "This code snippet includes helper functions for time, print, and import operations. It defines a Timer class for measuring elapsed time, a print_ribbon function to format print statements with a banner, and an import_or_print_error function to handle module imports, displaying an error message if necessary and exiting the program.",
        "type": "summary"
    },
    "602": {
        "file_id": 17,
        "content": "import time\nimport importlib\n# helper functions\ndef exists(val):\n    return val is not None\n# time helpers\nclass Timer:\n    def __init__(self):\n        self.reset()\n    def reset(self):\n        self.last_time = time.time()\n    def elapsed(self):\n        return time.time() - self.last_time\n# print helpers\ndef print_ribbon(s, symbol = '=', repeat = 40):\n    flank = symbol * repeat\n    return f'{flank} {s} {flank}'\n# import helpers\ndef import_or_print_error(pkg_name, err_str = None):\n    try:\n        return importlib.import_module(pkg_name)\n    except ModuleNotFoundError as e:\n        if exists(err_str):\n            print(err_str)\n        exit()",
        "type": "code",
        "location": "/dalle2_pytorch/utils.py:1-35"
    },
    "603": {
        "file_id": 17,
        "content": "This code snippet includes helper functions for time, print, and import operations. It defines a Timer class for measuring elapsed time, a print_ribbon function to format print statements with a banner, and an import_or_print_error function to handle module imports, displaying an error message if necessary and exiting the program.",
        "type": "comment"
    },
    "604": {
        "file_id": 18,
        "content": "/dalle2_pytorch/version.py",
        "type": "filepath"
    },
    "605": {
        "file_id": 18,
        "content": "This code defines the version number of the DALLE2-pytorch library, currently set as '1.15.6'.",
        "type": "summary"
    },
    "606": {
        "file_id": 18,
        "content": "__version__ = '1.15.6'",
        "type": "code",
        "location": "/dalle2_pytorch/version.py:1-1"
    },
    "607": {
        "file_id": 18,
        "content": "This code defines the version number of the DALLE2-pytorch library, currently set as '1.15.6'.",
        "type": "comment"
    },
    "608": {
        "file_id": 19,
        "content": "/dalle2_pytorch/vqgan_vae.py",
        "type": "filepath"
    },
    "609": {
        "file_id": 19,
        "content": "Code describes VQGAN-VAE and Vision Transformer architectures for image generation models, including convolutional layers, self-attention mechanisms, layer normalization, initializes model, calculates losses, determines adaptive weight, applies clamp function, calculates combined loss, returns reconstructed feature maps if required.",
        "type": "summary"
    },
    "610": {
        "file_id": 19,
        "content": "import copy\nimport math\nfrom math import sqrt\nfrom functools import partial, wraps\nfrom vector_quantize_pytorch import VectorQuantize as VQ\nimport torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\nfrom torch.autograd import grad as torch_grad\nimport torchvision\nfrom einops import rearrange, reduce, repeat, pack, unpack\nfrom einops.layers.torch import Rearrange\n# constants\nMList = nn.ModuleList\n# helper functions\ndef exists(val):\n    return val is not None\ndef default(val, d):\n    return val if exists(val) else d\n# decorators\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\ndef remove_vgg(fn):\n    @wraps(fn)\n    def inner(self, *args, **kwargs):\n        has_vgg = hasattr(self, 'vgg')\n        if has_vgg:\n            vgg = self.vgg\n            delattr(self, 'vgg')\n        out = fn(self, *args, **kwargs)\n        if has_vgg:\n            self.vgg = vgg",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:1-51"
    },
    "611": {
        "file_id": 19,
        "content": "This code imports various libraries and defines several constants, helper functions, and decorators for use in a deep learning model. It also sets up a class for a Vector Quantize module using PyTorch, with functionality to evaluate the model and remove the VGG feature if present.",
        "type": "comment"
    },
    "612": {
        "file_id": 19,
        "content": "        return out\n    return inner\n# keyword argument helpers\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\ndef string_begins_with(prefix, string_input):\n    return string_input.startswith(prefix)\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n# tensor helper functions\ndef log(t, eps = 1e-10):\n    return torch.log(t + eps)\ndef gradient_penalty(images, output, weight = 10):\n    batch_size = images.shape[0]",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:53-87"
    },
    "613": {
        "file_id": 19,
        "content": "This code contains various utility functions. \"pick_and_pop\" removes and returns keys from a dictionary, \"group_dict_by_key\" groups dictionary items by key condition, \"string_begins_with\" checks if a string begins with a given prefix, \"group_by_key_prefix\" groups dictionary items based on a key prefix, and \"groupby_prefix_and_trim\" trims key prefixes before grouping. Lastly, the \"log\" function calculates the natural logarithm of an input tensor, and the \"gradient_penalty\" function is used to calculate a gradient penalty for image generation tasks.",
        "type": "comment"
    },
    "614": {
        "file_id": 19,
        "content": "    gradients = torch_grad(outputs = output, inputs = images,\n                           grad_outputs = torch.ones(output.size(), device = images.device),\n                           create_graph = True, retain_graph = True, only_inputs = True)[0]\n    gradients = rearrange(gradients, 'b ... -> b (...)')\n    return weight * ((gradients.norm(2, dim = 1) - 1) ** 2).mean()\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\ndef leaky_relu(p = 0.1):\n    return nn.LeakyReLU(0.1)\ndef stable_softmax(t, dim = -1, alpha = 32 ** 2):\n    t = t / alpha\n    t = t - torch.amax(t, dim = dim, keepdim = True).detach()\n    return (t * alpha).softmax(dim = dim)\ndef safe_div(numer, denom, eps = 1e-8):\n    return numer / (denom + eps)\n# gan losses\ndef hinge_discr_loss(fake, real):\n    return (F.relu(1 + fake) + F.relu(1 - real)).mean()\ndef hinge_gen_loss(fake):\n    return -fake.mean()\ndef bce_discr_loss(fake, real):\n    return (-log(1 - torch.sigmoid(fake)) - log(torch.sigmoid(real))).mean()\ndef bce_gen_loss(fake):\n    return -log(torch.sigmoid(fake)).mean()",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:88-121"
    },
    "615": {
        "file_id": 19,
        "content": "This code contains several utility functions and loss functions used in the VQ-VAE-GAN model. It includes functions for gradient calculations, normalization, activation functions, and various GAN losses. The functions are defined to be reusable throughout the codebase.",
        "type": "comment"
    },
    "616": {
        "file_id": 19,
        "content": "def grad_layer_wrt_loss(loss, layer):\n    return torch_grad(\n        outputs = loss,\n        inputs = layer,\n        grad_outputs = torch.ones_like(loss),\n        retain_graph = True\n    )[0].detach()\n# vqgan vae\nclass LayerNormChan(nn.Module):\n    def __init__(\n        self,\n        dim,\n        eps = 1e-5\n    ):\n        super().__init__()\n        self.eps = eps\n        self.gamma = nn.Parameter(torch.ones(1, dim, 1, 1))\n    def forward(self, x):\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) / (var + self.eps).sqrt() * self.gamma\n# discriminator\nclass Discriminator(nn.Module):\n    def __init__(\n        self,\n        dims,\n        channels = 3,\n        groups = 16,\n        init_kernel_size = 5\n    ):\n        super().__init__()\n        dim_pairs = zip(dims[:-1], dims[1:])\n        self.layers = MList([nn.Sequential(nn.Conv2d(channels, dims[0], init_kernel_size, padding = init_kernel_size // 2), leaky_relu())])\n        for dim_in, dim_out in dim_pairs:",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:123-163"
    },
    "617": {
        "file_id": 19,
        "content": "The code defines a function to compute gradients of a layer wrt the loss, and introduces two custom modules: LayerNormChan for layer normalization and Discriminator for a convolutional network. The discriminator consists of multiple layers with decreasing kernel sizes, each followed by a leaky ReLU activation function. These components are part of the VQGAN-VAE architecture in DALLE2-pytorch.",
        "type": "comment"
    },
    "618": {
        "file_id": 19,
        "content": "            self.layers.append(nn.Sequential(\n                nn.Conv2d(dim_in, dim_out, 4, stride = 2, padding = 1),\n                nn.GroupNorm(groups, dim_out),\n                leaky_relu()\n            ))\n        dim = dims[-1]\n        self.to_logits = nn.Sequential( # return 5 x 5, for PatchGAN-esque training\n            nn.Conv2d(dim, dim, 1),\n            leaky_relu(),\n            nn.Conv2d(dim, 1, 4)\n        )\n    def forward(self, x):\n        for net in self.layers:\n            x = net(x)\n        return self.to_logits(x)\n# positional encoding\nclass ContinuousPositionBias(nn.Module):\n    \"\"\" from https://arxiv.org/abs/2111.09883 \"\"\"\n    def __init__(self, *, dim, heads, layers = 2):\n        super().__init__()\n        self.net = MList([])\n        self.net.append(nn.Sequential(nn.Linear(2, dim), leaky_relu()))\n        for _ in range(layers - 1):\n            self.net.append(nn.Sequential(nn.Linear(dim, dim), leaky_relu()))\n        self.net.append(nn.Linear(dim, heads))\n        self.register_buffer('rel_pos', None, persistent = False)",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:164-197"
    },
    "619": {
        "file_id": 19,
        "content": "The code defines a VQGAN-VAE model. It uses convolutional layers and group normalization for downsampling the input image, followed by linear layers and leaky ReLU activation functions in a sequential manner to generate logits. The `ContinuousPositionBias` class is used for positional encoding in the model.",
        "type": "comment"
    },
    "620": {
        "file_id": 19,
        "content": "    def forward(self, x):\n        n, device = x.shape[-1], x.device\n        fmap_size = int(sqrt(n))\n        if not exists(self.rel_pos):\n            pos = torch.arange(fmap_size, device = device)\n            grid = torch.stack(torch.meshgrid(pos, pos, indexing = 'ij'))\n            grid = rearrange(grid, 'c i j -> (i j) c')\n            rel_pos = rearrange(grid, 'i c -> i 1 c') - rearrange(grid, 'j c -> 1 j c')\n            rel_pos = torch.sign(rel_pos) * torch.log(rel_pos.abs() + 1)\n            self.register_buffer('rel_pos', rel_pos, persistent = False)\n        rel_pos = self.rel_pos.float()\n        for layer in self.net:\n            rel_pos = layer(rel_pos)\n        bias = rearrange(rel_pos, 'i j h -> h i j')\n        return x + bias\n# resnet encoder / decoder\nclass ResnetEncDec(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        channels = 3,\n        layers = 4,\n        layer_mults = None,\n        num_resnet_blocks = 1,\n        resnet_groups = 16,\n        first_conv_kernel_size = 5,\n        use_attn = True,",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:199-232"
    },
    "621": {
        "file_id": 19,
        "content": "The code defines a VQ-VAE implementation with a resnet encoder/decoder for image generation. The function calculates relative positional embeddings and applies them to the input, then passes the result through a resnet encoder/decoder network before returning the transformed input. The ResnetEncDec class creates an instance of the resnet encoder/decoder with optional parameters such as dimensions, channels, layers, layer_mults, num_resnet_blocks, resnet_groups, first_conv_kernel_size, and use_attn.",
        "type": "comment"
    },
    "622": {
        "file_id": 19,
        "content": "        attn_dim_head = 64,\n        attn_heads = 8,\n        attn_dropout = 0.,\n    ):\n        super().__init__()\n        assert dim % resnet_groups == 0, f'dimension {dim} must be divisible by {resnet_groups} (groups for the groupnorm)'\n        self.layers = layers\n        self.encoders = MList([])\n        self.decoders = MList([])\n        layer_mults = default(layer_mults, list(map(lambda t: 2 ** t, range(layers))))\n        assert len(layer_mults) == layers, 'layer multipliers must be equal to designated number of layers'\n        layer_dims = [dim * mult for mult in layer_mults]\n        dims = (dim, *layer_dims)\n        self.encoded_dim = dims[-1]\n        dim_pairs = zip(dims[:-1], dims[1:])\n        append = lambda arr, t: arr.append(t)\n        prepend = lambda arr, t: arr.insert(0, t)\n        if not isinstance(num_resnet_blocks, tuple):\n            num_resnet_blocks = (*((0,) * (layers - 1)), num_resnet_blocks)\n        if not isinstance(use_attn, tuple):\n            use_attn = (*((False,) * (layers - 1)), use_attn)",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:233-262"
    },
    "623": {
        "file_id": 19,
        "content": "This code defines a class with specified parameters for layers, encoders, and decoders. It ensures the dimension is divisible by resnet_groups. The layer multipliers are stored in a list and used to determine the dimensions of each layer. num_resnet_blocks and use_attn are checked to make sure they match the designated number of layers.",
        "type": "comment"
    },
    "624": {
        "file_id": 19,
        "content": "        assert len(num_resnet_blocks) == layers, 'number of resnet blocks config must be equal to number of layers'\n        assert len(use_attn) == layers\n        for layer_index, (dim_in, dim_out), layer_num_resnet_blocks, layer_use_attn in zip(range(layers), dim_pairs, num_resnet_blocks, use_attn):\n            append(self.encoders, nn.Sequential(nn.Conv2d(dim_in, dim_out, 4, stride = 2, padding = 1), leaky_relu()))\n            prepend(self.decoders, nn.Sequential(nn.ConvTranspose2d(dim_out, dim_in, 4, 2, 1), leaky_relu()))\n            if layer_use_attn:\n                prepend(self.decoders, VQGanAttention(dim = dim_out, heads = attn_heads, dim_head = attn_dim_head, dropout = attn_dropout))\n            for _ in range(layer_num_resnet_blocks):\n                append(self.encoders, ResBlock(dim_out, groups = resnet_groups))\n                prepend(self.decoders, GLUResBlock(dim_out, groups = resnet_groups))\n            if layer_use_attn:\n                append(self.encoders, VQGanAttention(dim = dim_out, heads = attn_heads, dim_head = attn_dim_head, dropout = attn_dropout))",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:264-279"
    },
    "625": {
        "file_id": 19,
        "content": "This code creates encoder and decoder blocks for a VQ-VAE model. It asserts that the number of resnet blocks and use_attn match the layers, then iterates over each layer creating convolutional layers, LeakyReLU activation functions, optionally adding attention modules, and repeating a specific number of residual blocks in both encoders and decoders.",
        "type": "comment"
    },
    "626": {
        "file_id": 19,
        "content": "        prepend(self.encoders, nn.Conv2d(channels, dim, first_conv_kernel_size, padding = first_conv_kernel_size // 2))\n        append(self.decoders, nn.Conv2d(dim, channels, 1))\n    def get_encoded_fmap_size(self, image_size):\n        return image_size // (2 ** self.layers)\n    @property\n    def last_dec_layer(self):\n        return self.decoders[-1].weight\n    def encode(self, x):\n        for enc in self.encoders:\n            x = enc(x)\n        return x\n    def decode(self, x):\n        for dec in self.decoders:\n            x = dec(x)\n        return x\nclass GLUResBlock(nn.Module):\n    def __init__(self, chan, groups = 16):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(chan, chan * 2, 3, padding = 1),\n            nn.GLU(dim = 1),\n            nn.GroupNorm(groups, chan),\n            nn.Conv2d(chan, chan * 2, 3, padding = 1),\n            nn.GLU(dim = 1),\n            nn.GroupNorm(groups, chan),\n            nn.Conv2d(chan, chan, 1)\n        )\n    def forward(self, x):\n        return self.net(x) + x",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:281-315"
    },
    "627": {
        "file_id": 19,
        "content": "The code defines a class for a VQGAN-VAE model. It consists of encoder and decoder blocks, along with a GLUResBlock for the residual connections in the decoder. The encoder and decoder are composed of convolutional layers that reduce and increase image size respectively. The encoded image size is defined as the original image size divided by 2 to the power of the number of layers. The model can encode and decode images using the encoder and decoder blocks, and the last decoder layer's weights can be accessed separately.",
        "type": "comment"
    },
    "628": {
        "file_id": 19,
        "content": "class ResBlock(nn.Module):\n    def __init__(self, chan, groups = 16):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(chan, chan, 3, padding = 1),\n            nn.GroupNorm(groups, chan),\n            leaky_relu(),\n            nn.Conv2d(chan, chan, 3, padding = 1),\n            nn.GroupNorm(groups, chan),\n            leaky_relu(),\n            nn.Conv2d(chan, chan, 1)\n        )\n    def forward(self, x):\n        return self.net(x) + x\n# vqgan attention layer\nclass VQGanAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        dropout = 0.\n    ):\n        super().__init__()\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        inner_dim = heads * dim_head\n        self.dropout = nn.Dropout(dropout)\n        self.pre_norm = LayerNormChan(dim)\n        self.cpb = ContinuousPositionBias(dim = dim // 4, heads = heads)\n        self.to_qkv = nn.Conv2d(dim, inner_dim * 3, 1, bias = False)\n        self.to_out = nn.Conv2d(inner_dim, dim, 1, bias = False)",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:317-354"
    },
    "629": {
        "file_id": 19,
        "content": "This code defines a residual block and a VQGAN attention layer for image processing. The ResBlock consists of multiple 2D convolutions and GroupNorm layers, followed by leaky ReLU activation functions. The VQGANAttention class is responsible for self-attention in the VQGAN model, using continuous position bias and multi-head attention with dropout regularization.",
        "type": "comment"
    },
    "630": {
        "file_id": 19,
        "content": "    def forward(self, x):\n        h = self.heads\n        height, width, residual = *x.shape[-2:], x.clone()\n        x = self.pre_norm(x)\n        q, k, v = self.to_qkv(x).chunk(3, dim = 1)\n        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = h), (q, k, v))\n        sim = einsum('b h c i, b h c j -> b h i j', q, k) * self.scale\n        sim = self.cpb(sim)\n        attn = stable_softmax(sim, dim = -1)\n        attn = self.dropout(attn)\n        out = einsum('b h i j, b h c j -> b h c i', attn, v)\n        out = rearrange(out, 'b h c (x y) -> b (h c) x y', x = height, y = width)\n        out = self.to_out(out)\n        return out + residual\n# ViT encoder / decoder\nclass RearrangeImage(nn.Module):\n    def forward(self, x):\n        n = x.shape[1]\n        w = h = int(sqrt(n))\n        return rearrange(x, 'b (h w) ... -> b h w ...', h = h, w = w)\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        heads = 8,\n        dim_head = 32\n    ):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:356-396"
    },
    "631": {
        "file_id": 19,
        "content": "This code defines a class for the Attention module in a ViT (Vision Transformer) model. It performs multi-head attention using key, query, and value tensors, followed by a softmax function to compute attention weights. The output is then passed through a linear layer and layer normalization before being added back to the input with residual connection.",
        "type": "comment"
    },
    "632": {
        "file_id": 19,
        "content": "        self.heads = heads\n        self.scale = dim_head ** -0.5\n        inner_dim = dim_head * heads\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim)\n    def forward(self, x):\n        h = self.heads\n        x = self.norm(x)\n        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n        q = q * self.scale\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n        attn = sim.softmax(dim = -1)\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\ndef FeedForward(dim, mult = 4):\n    return nn.Sequential(\n        nn.LayerNorm(dim),\n        nn.Linear(dim, dim * mult, bias = False),\n        nn.GELU(),\n        nn.Linear(dim * mult, dim, bias = False)\n    )\nclass Transformer(nn.Module):\n    def __init__(\n        self,",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:397-433"
    },
    "633": {
        "file_id": 19,
        "content": "This code defines a MultiHeadAttention module for a transformer model. It initializes the attention head count and scale, calculates inner dimension based on head count and input dimension. The forward function performs multi-head attention by splitting input into query, key, value tensors, scaling query tensor, computing similarity between query and key, subtracting maximum similarity to avoid zero gradients, performing softmax on attention scores, and finally producing output tensor through weighted sum of value tensors. The FeedForward function defines a feedforward network for the transformer model, consisting of layer normalization, linear layers with GELU activation function.",
        "type": "comment"
    },
    "634": {
        "file_id": 19,
        "content": "        dim,\n        *,\n        layers,\n        dim_head = 32,\n        heads = 8,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(layers):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n        self.norm = nn.LayerNorm(dim)\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return self.norm(x)\nclass ViTEncDec(nn.Module):\n    def __init__(\n        self,\n        dim,\n        channels = 3,\n        layers = 4,\n        patch_size = 8,\n        dim_head = 32,\n        heads = 8,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.encoded_dim = dim\n        self.patch_size = patch_size\n        input_dim = channels * (patch_size ** 2)\n        self.encoder = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:434-476"
    },
    "635": {
        "file_id": 19,
        "content": "The code defines a class for an encoder-decoder architecture, which is part of the Vision Transformer (ViT) model. It utilizes attention and feedforward layers, and includes layer normalization in its forward pass. The encoder section takes input images, reshapes them into patches, and passes them through multiple attention and feedforward layers.",
        "type": "comment"
    },
    "636": {
        "file_id": 19,
        "content": "            nn.Linear(input_dim, dim),\n            Transformer(\n                dim = dim,\n                dim_head = dim_head,\n                heads = heads,\n                ff_mult = ff_mult,\n                layers = layers\n            ),\n            RearrangeImage(),\n            Rearrange('b h w c -> b c h w')\n        )\n        self.decoder = nn.Sequential(\n            Rearrange('b c h w -> b (h w) c'),\n            Transformer(\n                dim = dim,\n                dim_head = dim_head,\n                heads = heads,\n                ff_mult = ff_mult,\n                layers = layers\n            ),\n            nn.Sequential(\n                nn.Linear(dim, dim * 4, bias = False),\n                nn.Tanh(),\n                nn.Linear(dim * 4, input_dim, bias = False),\n            ),\n            RearrangeImage(),\n            Rearrange('b h w (p1 p2 c) -> b c (h p1) (w p2)', p1 = patch_size, p2 = patch_size)\n        )\n    def get_encoded_fmap_size(self, image_size):\n        return image_size // self.patch_size\n    @property",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:477-510"
    },
    "637": {
        "file_id": 19,
        "content": "The code defines a VQ-VAE model for image generation, consisting of an encoder and decoder. The encoder processes the input image and outputs a compressed codebook index followed by a positional embedding. The decoder then reconstructs the original image from these inputs using a series of transformers and linear layers. The get_encoded_fmap_size function calculates the encoded feature map size based on the input image size.",
        "type": "comment"
    },
    "638": {
        "file_id": 19,
        "content": "    def last_dec_layer(self):\n        return self.decoder[-3][-1].weight\n    def encode(self, x):\n        return self.encoder(x)\n    def decode(self, x):\n        return self.decoder(x)\n# main vqgan-vae classes\nclass NullVQGanVAE(nn.Module):\n    def __init__(\n        self,\n        *,\n        channels\n    ):\n        super().__init__()\n        self.encoded_dim = channels\n        self.layers = 0\n    def get_encoded_fmap_size(self, size):\n        return size\n    def copy_for_eval(self):\n        return self\n    def encode(self, x):\n        return x\n    def decode(self, x):\n        return x\nclass VQGanVAE(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        image_size,\n        channels = 3,\n        layers = 4,\n        l2_recon_loss = False,\n        use_hinge_loss = True,\n        vgg = None,\n        vq_codebook_dim = 256,\n        vq_codebook_size = 512,\n        vq_decay = 0.8,\n        vq_commitment_weight = 1.,\n        vq_kmeans_init = True,\n        vq_use_cosine_sim = True,\n        use_vgg_and_gan = True,\n        vae_type = 'resnet',",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:511-562"
    },
    "639": {
        "file_id": 19,
        "content": "This code defines two classes: NullVQGanVAE and VQGanVAE. The NullVQGanVAE is a placeholder class without any specific layers or functionality, while the VQGanVAE class represents a variant of the VAE model with optional features like VGG loss, GAN integration, and customizable parameters for codebook dimensions and layers.",
        "type": "comment"
    },
    "640": {
        "file_id": 19,
        "content": "        discr_layers = 4,\n        **kwargs\n    ):\n        super().__init__()\n        vq_kwargs, kwargs = groupby_prefix_and_trim('vq_', kwargs)\n        encdec_kwargs, kwargs = groupby_prefix_and_trim('encdec_', kwargs)\n        self.image_size = image_size\n        self.channels = channels\n        self.codebook_size = vq_codebook_size\n        if vae_type == 'resnet':\n            enc_dec_klass = ResnetEncDec\n        elif vae_type == 'vit':\n            enc_dec_klass = ViTEncDec\n        else:\n            raise ValueError(f'{vae_type} not valid')\n        self.enc_dec = enc_dec_klass(\n            dim = dim,\n            channels = channels,\n            layers = layers,\n            **encdec_kwargs\n        )\n        self.vq = VQ(\n            dim = self.enc_dec.encoded_dim,\n            codebook_dim = vq_codebook_dim,\n            codebook_size = vq_codebook_size,\n            decay = vq_decay,\n            commitment_weight = vq_commitment_weight,\n            accept_image_fmap = True,\n            kmeans_init = vq_kmeans_init,\n            use_cosine_sim = vq_use_cosine_sim,",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:563-596"
    },
    "641": {
        "file_id": 19,
        "content": "This code initializes a VQ-VAE model with given parameters. It uses a specified encoder-decoder network (ResNet or ViT), codebook size, and other VQ-specific options. The VQ module is initialized based on the dimensionality of the encoder-decoder's encoded output, and the codebook size and related options. If an invalid VAE type is given, a ValueError is raised.",
        "type": "comment"
    },
    "642": {
        "file_id": 19,
        "content": "            **vq_kwargs\n        )\n        # reconstruction loss\n        self.recon_loss_fn = F.mse_loss if l2_recon_loss else F.l1_loss\n        # turn off GAN and perceptual loss if grayscale\n        self.vgg = None\n        self.discr = None\n        self.use_vgg_and_gan = use_vgg_and_gan\n        if not use_vgg_and_gan:\n            return\n        # preceptual loss\n        if exists(vgg):\n            self.vgg = vgg\n        else:\n            self.vgg = torchvision.models.vgg16(pretrained = True)\n            self.vgg.classifier = nn.Sequential(*self.vgg.classifier[:-2])\n        # gan related losses\n        layer_mults = list(map(lambda t: 2 ** t, range(discr_layers)))\n        layer_dims = [dim * mult for mult in layer_mults]\n        dims = (dim, *layer_dims)\n        self.discr = Discriminator(dims = dims, channels = channels)\n        self.discr_loss = hinge_discr_loss if use_hinge_loss else bce_discr_loss\n        self.gen_loss = hinge_gen_loss if use_hinge_loss else bce_gen_loss\n    @property\n    def encoded_dim(self):",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:597-633"
    },
    "643": {
        "file_id": 19,
        "content": "This code defines a VQGAN-VAE model with optional GAN and perceptual loss components. It initializes the VGG model, Discriminator, and sets the reconstruction and generator losses based on provided arguments. The encoded_dim property returns the dimension of the encoded images.",
        "type": "comment"
    },
    "644": {
        "file_id": 19,
        "content": "        return self.enc_dec.encoded_dim\n    def get_encoded_fmap_size(self, image_size):\n        return self.enc_dec.get_encoded_fmap_size(image_size)\n    def copy_for_eval(self):\n        device = next(self.parameters()).device\n        vae_copy = copy.deepcopy(self.cpu())\n        if vae_copy.use_vgg_and_gan:\n            del vae_copy.discr\n            del vae_copy.vgg\n        vae_copy.eval()\n        return vae_copy.to(device)\n    @remove_vgg\n    def state_dict(self, *args, **kwargs):\n        return super().state_dict(*args, **kwargs)\n    @remove_vgg\n    def load_state_dict(self, *args, **kwargs):\n        return super().load_state_dict(*args, **kwargs)\n    @property\n    def codebook(self):\n        return self.vq.codebook\n    def encode(self, fmap):\n        fmap = self.enc_dec.encode(fmap)\n        return fmap\n    def decode(self, fmap, return_indices_and_loss = False):\n        fmap, indices, commit_loss = self.vq(fmap)\n        fmap = self.enc_dec.decode(fmap)\n        if not return_indices_and_loss:\n            return fmap",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:634-672"
    },
    "645": {
        "file_id": 19,
        "content": "This code defines a class with methods to get encoded dimensions, calculate encoded frame map size, copy the model for evaluation, save and load state dictionary while removing VGG, encode input frames, and decode encoded frames.",
        "type": "comment"
    },
    "646": {
        "file_id": 19,
        "content": "        return fmap, indices, commit_loss\n    def forward(\n        self,\n        img,\n        return_loss = False,\n        return_discr_loss = False,\n        return_recons = False,\n        add_gradient_penalty = True\n    ):\n        batch, channels, height, width, device = *img.shape, img.device\n        assert height == self.image_size and width == self.image_size, 'height and width of input image must be equal to {self.image_size}'\n        assert channels == self.channels, 'number of channels on image or sketch is not equal to the channels set on this VQGanVAE'\n        fmap = self.encode(img)\n        fmap, indices, commit_loss = self.decode(fmap, return_indices_and_loss = True)\n        if not return_loss and not return_discr_loss:\n            return fmap\n        assert return_loss ^ return_discr_loss, 'you should either return autoencoder loss or discriminator loss, but not both'\n        # whether to return discriminator loss\n        if return_discr_loss:\n            assert exists(self.discr), 'discriminator must exist to train it'",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:674-700"
    },
    "647": {
        "file_id": 19,
        "content": "This function encodes an input image, decodes it, and can optionally return autoencoder or discriminator losses. It expects the image to have the specified dimensions and number of channels. The code asserts that the image's height, width, and number of channels match the expected values, and that only one type of loss is returned at a time.",
        "type": "comment"
    },
    "648": {
        "file_id": 19,
        "content": "            fmap.detach_()\n            img.requires_grad_()\n            fmap_discr_logits, img_discr_logits = map(self.discr, (fmap, img))\n            discr_loss = self.discr_loss(fmap_discr_logits, img_discr_logits)\n            if add_gradient_penalty:\n                gp = gradient_penalty(img, img_discr_logits)\n                loss = discr_loss + gp\n            if return_recons:\n                return loss, fmap\n            return loss\n        # reconstruction loss\n        recon_loss = self.recon_loss_fn(fmap, img)\n        # early return if training on grayscale\n        if not self.use_vgg_and_gan:\n            if return_recons:\n                return recon_loss, fmap\n            return recon_loss\n        # perceptual loss\n        img_vgg_input = img\n        fmap_vgg_input = fmap\n        if img.shape[1] == 1:\n            # handle grayscale for vgg\n            img_vgg_input, fmap_vgg_input = map(lambda t: repeat(t, 'b 1 ... -> b c ...', c = 3), (img_vgg_input, fmap_vgg_input))\n        img_vgg_feats = self.vgg(img_vgg_input)",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:702-739"
    },
    "649": {
        "file_id": 19,
        "content": "The code is calculating the reconstruction and perceptual loss for an image generation model. It also includes gradient penalty for the discriminator loss, and optionally returns the reconstructed feature map.",
        "type": "comment"
    },
    "650": {
        "file_id": 19,
        "content": "        recon_vgg_feats = self.vgg(fmap_vgg_input)\n        perceptual_loss = F.mse_loss(img_vgg_feats, recon_vgg_feats)\n        # generator loss\n        gen_loss = self.gen_loss(self.discr(fmap))\n        # calculate adaptive weight\n        last_dec_layer = self.enc_dec.last_dec_layer\n        norm_grad_wrt_gen_loss = grad_layer_wrt_loss(gen_loss, last_dec_layer).norm(p = 2)\n        norm_grad_wrt_perceptual_loss = grad_layer_wrt_loss(perceptual_loss, last_dec_layer).norm(p = 2)\n        adaptive_weight = safe_div(norm_grad_wrt_perceptual_loss, norm_grad_wrt_gen_loss)\n        adaptive_weight.clamp_(max = 1e4)\n        # combine losses\n        loss = recon_loss + perceptual_loss + commit_loss + adaptive_weight * gen_loss\n        if return_recons:\n            return loss, fmap\n        return loss",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae.py:740-764"
    },
    "651": {
        "file_id": 19,
        "content": "This code calculates a combination of losses, including reconstruction, perceptual, and commitment. The adaptive weight is determined based on the gradients of these losses. A clamp function limits the adaptive weight to prevent extreme values. Finally, the combined loss is calculated and returned. If return_recons is True, fmap is also returned.",
        "type": "comment"
    },
    "652": {
        "file_id": 20,
        "content": "/dalle2_pytorch/vqgan_vae_trainer.py",
        "type": "filepath"
    },
    "653": {
        "file_id": 20,
        "content": "This code defines ImageDataset and VQGanVAETrainer classes for loading image data and training a VAE model, setting parameters, optimizers, and creating loaders. It trains the model, logs losses, saves models, and tracks progress in a results folder.",
        "type": "summary"
    },
    "654": {
        "file_id": 20,
        "content": "from math import sqrt\nimport copy\nfrom random import choice\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom PIL import Image\nimport torch\nfrom torch import nn\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as T\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid, save_image\nfrom einops import rearrange\nfrom dalle2_pytorch.vqgan_vae import VQGanVAE\nfrom dalle2_pytorch.optimizer import get_optimizer\nfrom ema_pytorch import EMA\n# helpers\ndef exists(val):\n    return val is not None\ndef noop(*args, **kwargs):\n    pass\ndef cycle(dl):\n    while True:\n        for data in dl:\n            yield data\ndef cast_tuple(t):\n    return t if isinstance(t, (tuple, list)) else (t,)\ndef yes_or_no(question):\n    answer = input(f'{question} (y/n) ')\n    return answer.lower() in ('yes', 'y')\ndef accum_log(log, new_logs):\n    for key, new_value in new_logs.items():\n        old_value = log.get(key, 0.)\n        log[key] = old_value + new_value",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae_trainer.py:1-47"
    },
    "655": {
        "file_id": 20,
        "content": "This code contains several utility functions and helper methods. It includes import statements for various libraries, classes for data handling and model training, as well as custom functions for logging, looping, and user input.",
        "type": "comment"
    },
    "656": {
        "file_id": 20,
        "content": "    return log\n# classes\nclass ImageDataset(Dataset):\n    def __init__(\n        self,\n        folder,\n        image_size,\n        exts = ['jpg', 'jpeg', 'png']\n    ):\n        super().__init__()\n        self.folder = folder\n        self.image_size = image_size\n        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n        print(f'{len(self.paths)} training samples found at {folder}')\n        self.transform = T.Compose([\n            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n            T.Resize(image_size),\n            T.RandomHorizontalFlip(),\n            T.CenterCrop(image_size),\n            T.ToTensor()\n        ])\n    def __len__(self):\n        return len(self.paths)\n    def __getitem__(self, index):\n        path = self.paths[index]\n        img = Image.open(path)\n        return self.transform(img)\n# main trainer class\nclass VQGanVAETrainer(nn.Module):\n    def __init__(\n        self,\n        vae,\n        *,\n        num_train_steps,\n        lr,\n        batch_size,",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae_trainer.py:48-91"
    },
    "657": {
        "file_id": 20,
        "content": "The code defines a class \"ImageDataset\" for loading and transforming image data, and a main trainer class \"VQGanVAETrainer\" for training a VAE model. The \"ImageDataset\" class initializes with a folder path, image size, and extension types to filter the images, then applies image transformations like converting to RGB mode, resizing, horizontal flipping, cropping, and tensor conversion. The \"VQGanVAETrainer\" class initializes with parameters like the VAE model, number of training steps, learning rate, and batch size for the training process.",
        "type": "comment"
    },
    "658": {
        "file_id": 20,
        "content": "        folder,\n        grad_accum_every,\n        wd = 0.,\n        save_results_every = 100,\n        save_model_every = 1000,\n        results_folder = './results',\n        valid_frac = 0.05,\n        random_split_seed = 42,\n        ema_beta = 0.995,\n        ema_update_after_step = 500,\n        ema_update_every = 10,\n        apply_grad_penalty_every = 4,\n        amp = False\n    ):\n        super().__init__()\n        assert isinstance(vae, VQGanVAE), 'vae must be instance of VQGanVAE'\n        image_size = vae.image_size\n        self.vae = vae\n        self.ema_vae = EMA(vae, update_after_step = ema_update_after_step, update_every = ema_update_every)\n        self.register_buffer('steps', torch.Tensor([0]))\n        self.num_train_steps = num_train_steps\n        self.batch_size = batch_size\n        self.grad_accum_every = grad_accum_every\n        all_parameters = set(vae.parameters())\n        discr_parameters = set(vae.discr.parameters())\n        vae_parameters = all_parameters - discr_parameters\n        self.optim = get_optimizer(vae_parameters, lr = lr, wd = wd)",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae_trainer.py:92-123"
    },
    "659": {
        "file_id": 20,
        "content": "The code initializes an instance of a VQGanVAE and sets up various parameters for training. It checks if the provided vae is of type VQGanVAE, then assigns image size, creates an EMA model with specified update steps and intervals, registers a buffer for tracking steps, sets number of train steps, batch size, grad accumulation every, and initializes optimizer with specified learning rate and weight decay.",
        "type": "comment"
    },
    "660": {
        "file_id": 20,
        "content": "        self.discr_optim = get_optimizer(discr_parameters, lr = lr, wd = wd)\n        self.amp = amp\n        self.scaler = GradScaler(enabled = amp)\n        self.discr_scaler = GradScaler(enabled = amp)\n        # create dataset\n        self.ds = ImageDataset(folder, image_size = image_size)\n        # split for validation\n        if valid_frac > 0:\n            train_size = int((1 - valid_frac) * len(self.ds))\n            valid_size = len(self.ds) - train_size\n            self.ds, self.valid_ds = random_split(self.ds, [train_size, valid_size], generator = torch.Generator().manual_seed(random_split_seed))\n            print(f'training with dataset of {len(self.ds)} samples and validating with randomly splitted {len(self.valid_ds)} samples')\n        else:\n            self.valid_ds = self.ds\n            print(f'training with shared training and valid dataset of {len(self.ds)} samples')\n        # dataloader\n        self.dl = cycle(DataLoader(\n            self.ds,\n            batch_size = batch_size,\n            shuffle = True",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae_trainer.py:124-150"
    },
    "661": {
        "file_id": 20,
        "content": "This code initializes a Discriminator optimizer, Amplitude Signed-Precision (AMP) for mixed precision training, GradScaler for handling gradients, creates an ImageDataset from the given folder and image size, splits the dataset into training and validation if valid_frac is greater than 0, creates DataLoader for the dataset with specified batch_size and shuffle set to True.",
        "type": "comment"
    },
    "662": {
        "file_id": 20,
        "content": "        ))\n        self.valid_dl = cycle(DataLoader(\n            self.valid_ds,\n            batch_size = batch_size,\n            shuffle = True\n        ))\n        self.save_model_every = save_model_every\n        self.save_results_every = save_results_every\n        self.apply_grad_penalty_every = apply_grad_penalty_every\n        self.results_folder = Path(results_folder)\n        if len([*self.results_folder.glob('**/*')]) > 0 and yes_or_no('do you want to clear previous experiment checkpoints and results?'):\n            rmtree(str(self.results_folder))\n        self.results_folder.mkdir(parents = True, exist_ok = True)\n    def train_step(self):\n        device = next(self.vae.parameters()).device\n        steps = int(self.steps.item())\n        apply_grad_penalty = not (steps % self.apply_grad_penalty_every)\n        self.vae.train()\n        # logs\n        logs = {}\n        # update vae (generator)\n        for _ in range(self.grad_accum_every):\n            img = next(self.dl)\n            img = img.to(device)\n            with autocast(enabled = self.amp):",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae_trainer.py:151-188"
    },
    "663": {
        "file_id": 20,
        "content": "The code initializes the valid data loader and sets parameters for saving models, results, and applying gradient penalty. It checks if previous experiment checkpoints and results should be cleared, creates the results folder if needed, and defines the train_step function for training the VAE (generator).",
        "type": "comment"
    },
    "664": {
        "file_id": 20,
        "content": "                loss = self.vae(\n                    img,\n                    return_loss = True,\n                    apply_grad_penalty = apply_grad_penalty\n                )\n                self.scaler.scale(loss / self.grad_accum_every).backward()\n            accum_log(logs, {'loss': loss.item() / self.grad_accum_every})\n        self.scaler.step(self.optim)\n        self.scaler.update()\n        self.optim.zero_grad()\n        # update discriminator\n        if exists(self.vae.discr):\n            discr_loss = 0\n            for _ in range(self.grad_accum_every):\n                img = next(self.dl)\n                img = img.to(device)\n                with autocast(enabled = self.amp):\n                    loss = self.vae(img, return_discr_loss = True)\n                    self.discr_scaler.scale(loss / self.grad_accum_every).backward()\n                accum_log(logs, {'discr_loss': loss.item() / self.grad_accum_every})\n            self.discr_scaler.step(self.discr_optim)\n            self.discr_scaler.update()\n            self.discr_optim.zero_grad()",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae_trainer.py:189-221"
    },
    "665": {
        "file_id": 20,
        "content": "This code trains a VAE model and updates the discriminator. It uses scaling, accumulation, and gradients for efficient backpropagation. The loss is calculated and logged for both VAE and discriminator, then optimizers are updated.",
        "type": "comment"
    },
    "666": {
        "file_id": 20,
        "content": "            # log\n            print(f\"{steps}: vae loss: {logs['loss']} - discr loss: {logs['discr_loss']}\")\n        # update exponential moving averaged generator\n        self.ema_vae.update()\n        # sample results every so often\n        if not (steps % self.save_results_every):\n            for model, filename in ((self.ema_vae.ema_model, f'{steps}.ema'), (self.vae, str(steps))):\n                model.eval()\n                imgs = next(self.dl)\n                imgs = imgs.to(device)\n                recons = model(imgs)\n                nrows = int(sqrt(self.batch_size))\n                imgs_and_recons = torch.stack((imgs, recons), dim = 0)\n                imgs_and_recons = rearrange(imgs_and_recons, 'r b ... -> (b r) ...')\n                imgs_and_recons = imgs_and_recons.detach().cpu().float().clamp(0., 1.)\n                grid = make_grid(imgs_and_recons, nrow = 2, normalize = True, value_range = (0, 1))\n                logs['reconstructions'] = grid\n                save_image(grid, str(self.results_folder / f'{filename}.png'))",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae_trainer.py:223-251"
    },
    "667": {
        "file_id": 20,
        "content": "This code snippet logs the VAE and discriminator losses, updates the exponential moving average (EMA) generator model, saves models every save_results_every steps, and generates and saves reconstruction images for training.",
        "type": "comment"
    },
    "668": {
        "file_id": 20,
        "content": "            print(f'{steps}: saving to {str(self.results_folder)}')\n        # save model every so often\n        if not (steps % self.save_model_every):\n            state_dict = self.vae.state_dict()\n            model_path = str(self.results_folder / f'vae.{steps}.pt')\n            torch.save(state_dict, model_path)\n            ema_state_dict = self.ema_vae.state_dict()\n            model_path = str(self.results_folder / f'vae.{steps}.ema.pt')\n            torch.save(ema_state_dict, model_path)\n            print(f'{steps}: saving model to {str(self.results_folder)}')\n        self.steps += 1\n        return logs\n    def train(self, log_fn = noop):\n        device = next(self.vae.parameters()).device\n        while self.steps < self.num_train_steps:\n            logs = self.train_step()\n            log_fn(logs)\n        print('training complete')",
        "type": "code",
        "location": "/dalle2_pytorch/vqgan_vae_trainer.py:253-278"
    },
    "669": {
        "file_id": 20,
        "content": "Saves the VAE model and EMA-VAE model periodically during training, tracking progress in specified results folder.",
        "type": "comment"
    },
    "670": {
        "file_id": 21,
        "content": "/prior.md",
        "type": "filepath"
    },
    "671": {
        "file_id": 21,
        "content": "This code uses diffusion prior and CLIP to generate images from text prompts, implements pre-trained decoders, compares EMA models, checks image embeddings in DALLE2-pytorch, and discusses overfitting and running diffusion model training scripts.",
        "type": "summary"
    },
    "672": {
        "file_id": 21,
        "content": "# Diffusion Prior\nThis readme serves as an introduction to the diffusion prior.\n## Intro\nA properly trained prior will allow you to translate between two embedding spaces. If you know *a priori* that two embeddings are connected some waythen ability the translate between them could extremely helpful.\n### Motivation\nBefore we dive into the model, lets look at a quick example of where the model may be helpful.\nFor demonstration purposes we will imagine that we wish to generate images from text using CLIP and a Decoder.\n> [CLIP](https://openai.com/blog/clip/) is a contrastive model that learns to maximize the cosine similarity between a given image and caption, however, there is no guarantee that these embeddings are in the same space. While the embeddings generated are ***close*** the image and text embeddings occupy two disjoint sets.\n```python\n# Load Models\nclip_model = clip.load(\"ViT-L/14\")\ndecoder = Decoder(checkpoint=\"best.pth\") # A decoder trained on CLIP Image embeddings\n# Retrieve prompt from user and encode with CLIP",
        "type": "code",
        "location": "/prior.md:1-21"
    },
    "673": {
        "file_id": 21,
        "content": "This code introduces the concept of a diffusion prior, which is a trained model that allows translation between two embedding spaces. It motivates the use case of generating images from text using CLIP and a Decoder when embeddings are not guaranteed to be in the same space. The code loads CLIP and a pre-trained decoder, then retrieves a prompt from the user and encodes it with CLIP for further processing.",
        "type": "comment"
    },
    "674": {
        "file_id": 21,
        "content": "prompt = \"A corgi wearing sunglasses\"\ntokenized_text = tokenize(prompt)\ntext_embedding = clip_model.encode_text(tokenized_text)\n# Now, pass the text embedding to the decoder\npredicted_image = decoder.sample(text_embedding)\n```\n> **Question**: *Can you spot the issue here?*\n>\n> **Answer**: *Were trying to generate an image from a text embedding!*\nUnfortunately, we run into the issue previously mentioned--the image embeddings and the text embeddings are not interchangeable! Now let's look at a better solution\n```python\n# Load Models\nprior= Prior(checkpoint=\"prior.pth\") # A decoder trained to go from: text-> clip text emb -> clip img emb\ndecoder = Decoder(checkpoint=\"decoder.pth\") # A decoder trained on CLIP Image embeddings\n# Retrieve prompt from user and encode with a prior\nprompt = \"A corgi wearing sunglasses\"\ntokenized_text = tokenize(prompt)\ntext_embedding = prior.sample(tokenized_text) # <-- now we get an embedding in the same space as images!\n# Now, pass the predicted image embedding to the decoder\npredicted_image = decoder.sample(text_embedding)",
        "type": "code",
        "location": "/prior.md:22-47"
    },
    "675": {
        "file_id": 21,
        "content": "This code snippet demonstrates the process of generating an image from a text prompt using deep learning models. The decoder model is trained to convert text into embeddings that are in the same space as CLIP image embeddings. First, we load two models: Prior and Decoder. Then, we retrieve a user-inputted prompt, tokenize it, and use the Prior model to sample a text embedding in the same space as images. Finally, we pass this text embedding into the Decoder model to generate an image.",
        "type": "comment"
    },
    "676": {
        "file_id": 21,
        "content": "```\nWith the prior we are able to successfully generate embeddings *within* CLIP's image space! For this reason, the decoder will perform much better as it receives input that is much closer to its training data.\n> **You may be asking yourself the following question:**\n>\n> *\"Why don't you just train the decoder on clip text embeddings instead of image embeddings?\"*\n>\n> OpenAI covers this topic in their [DALLE-2 paper](https://arxiv.org/abs/2204.06125). The TL;DR is *\"it doesn't work as well as decoders trained on image embeddings\"*...also...its just an example :smile:\n## Usage\nTo utilize a pre-trained prior, its quite simple.\n### Loading Checkpoints\n```python\nimport torch\nfrom dalle2_pytorch import DiffusionPrior, DiffusionPriorNetwork, OpenAIClipAdapter\nfrom dalle2_pytorch.trainer import DiffusionPriorTrainer\ndef load_diffusion_model(dprior_path):\n    prior_network = DiffusionPriorNetwork(\n        dim=768,\n        depth=24,\n        dim_head=64,\n        heads=32,\n        normformer=True,\n        attn_dropout=5e-2,",
        "type": "code",
        "location": "/prior.md:48-76"
    },
    "677": {
        "file_id": 21,
        "content": "The code demonstrates how to load a pre-trained prior model for use in generating embeddings within CLIP's image space, enhancing the performance of the decoder. The usage section outlines the necessary steps to load a checkpoint from a specific path using `load_diffusion_model()`.",
        "type": "comment"
    },
    "678": {
        "file_id": 21,
        "content": "        ff_dropout=5e-2,\n        num_time_embeds=1,\n        num_image_embeds=1,\n        num_text_embeds=1,\n        num_timesteps=1000,\n        ff_mult=4\n    )\n    diffusion_prior = DiffusionPrior(\n        net=prior_network,\n        clip=OpenAIClipAdapter(\"ViT-L/14\"),\n        image_embed_dim=768,\n        timesteps=1000,\n        cond_drop_prob=0.1,\n        loss_type=\"l2\",\n        condition_on_text_encodings=True,\n    )\n    trainer = DiffusionPriorTrainer(\n        diffusion_prior=diffusion_prior,\n        lr=1.1e-4,\n        wd=6.02e-2,\n        max_grad_norm=0.5,\n        amp=False,\n        group_wd_params=True,\n        use_ema=True,\n        device=device,\n        accelerator=None,\n    )\n    trainer.load(dprior_path)\n    return trainer\n```\n Here we instantiate a model matches the configuration it was trained with, and then load the weights (*just like any other PyTorch model!*)\n### Sampling\nOnce we have a pre-trained model, generating embeddings is quite simple!\n```python\n# tokenize the text\ntokenized_text = clip.tokenize(\"<your amazing prompt>\")",
        "type": "code",
        "location": "/prior.md:77-119"
    },
    "679": {
        "file_id": 21,
        "content": "Here, a pre-trained model is instantiated and its weights are loaded. This can be done just like any other PyTorch model. To generate embeddings from text, first tokenize the input text using `clip.tokenize()`.",
        "type": "comment"
    },
    "680": {
        "file_id": 21,
        "content": "# predict an embedding\npredicted_embedding = prior.sample(tokenized_text, n_samples_per_batch=2, cond_scale=1.0)\n```\nThe resulting tensor returned from `.sample()` is of the same shape as your training data along the non-batch dimension(s). For example, a prior trained on `ViT-L/14` embeddings will predict an embedding of shape (1, 768).\n> For CLIP priors, this is quite handy as it means that you can use prior.sample(tokenizer_text) as a drop in replacement for clip.encode_text().\n**Some things to note:**\n* It is possible to specify the number of embeddings to sample from (the default suggested by OpenAI is `n=2`). Put simply, the idea here is that you avoid getting unlucky with a bad embedding generation by creating two; and selecting the one with the higher cosine similarity with the prompt.\n* You may specify a higher conditioning scale than the default (`1.0`). It is unclear whether OpenAI uses a higher value for the prior specifically, or only on the decoder. Local testing has shown poor results with anything higher than `1.0` but *ymmv*.",
        "type": "code",
        "location": "/prior.md:120-130"
    },
    "681": {
        "file_id": 21,
        "content": "The code snippet is predicting an embedding using the prior's sample function, which returns a tensor of the same shape as the training data. The number of embeddings to sample can be specified and conditioning scale can be adjusted for better results. It serves as a replacement for clip.encode_text() in CLIP priors.",
        "type": "comment"
    },
    "682": {
        "file_id": 21,
        "content": "---\n## Training\n### Overview\nTraining the prior is a relatively straightforward process thanks to the Trainer base class. The major step that is required of you is preparing a dataset in the format that EmbeddingReader expects. Having pre-computed embeddings massively increases training efficiency and is generally recommended as you will likely benefit from having them on hand for other tasks as well. Once you have a dataset, you are ready to move onto configuration\n## Dataset\nTo train the prior, it is highly recommended to use precomputed embeddings for the images. To obtain these for a custom dataset, you can leverage [img2datset](https://github.com/rom1504/img2dataset) to pull images from a list of URLs and [clip_retrieval](https://github.com/rom1504/clip-retrieval#clip-inference) for generating the actual embeddings that can be used in the prior's dataloader.\n## Configuration\nThe configuration file allows for you to easily track and reproduce experiments. It is a simple JSON file that wil",
        "type": "code",
        "location": "/prior.md:132-146"
    },
    "683": {
        "file_id": 21,
        "content": "Training the prior involves preparing a dataset in the format expected by EmbeddingReader. Precomputed embeddings for images significantly increase training efficiency and are beneficial for other tasks as well. To obtain precomputed embeddings, you can use img2dataset and clip_retrieval. The configuration file enables tracking and reproducing experiments.",
        "type": "comment"
    },
    "684": {
        "file_id": 21,
        "content": "l specify the architecture, dataset, and training parameters. For more information and specifics please see the configuration README.\n## Distributed Training\nIf you would like to train in a distributed manner we have opted to leverage huggingface new Accelerate library. HFA makes it extremely simple to distribute work across multiple GPUs and nodes. All that is required of you is to follow the simple CLI configuration tool [more information here](https://huggingface.co/docs/accelerate/accelerator).\n## Evaluation\nThere are a variety of metrics available to you when training the prior. You can read a brief description of each in the table below:\n| Metric                              | Description                                                                                                                                                                                                                                                  | Comments                                                ",
        "type": "code",
        "location": "/prior.md:146-155"
    },
    "685": {
        "file_id": 21,
        "content": "This code describes the architecture, dataset, and training parameters for a specific task. It also mentions distributed training using HuggingFace's Accelerate library and various evaluation metrics available during training.",
        "type": "comment"
    },
    "686": {
        "file_id": 21,
        "content": "                                                                                                                                                                                                                                                                                                |\n| ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Online Model Validation             | The validation loss associated ",
        "type": "code",
        "location": "/prior.md:155-157"
    },
    "687": {
        "file_id": 21,
        "content": "This code is for calculating the validation loss associated with the online model validation process. The calculated validation loss will be used to evaluate the performance of the trained model during inference.",
        "type": "comment"
    },
    "688": {
        "file_id": 21,
        "content": "with your online model.                                                                                                                                                                                                       | Ideally validation loss will be as low as possible. Using L2 loss, values as low as `0.1` and lower are possible after around 1 Billion samples seen.                                                                                                                                                                                                |\n| EMA Validation                      | This metric measures the validation loss associated with your EMA model.                                                                                                                                                                                     | This will likely lag behind your \"online\" model's validation loss, but should outperform in the long-term.                                 ",
        "type": "code",
        "location": "/prior.md:157-158"
    },
    "689": {
        "file_id": 21,
        "content": "This code is discussing the usage of an Exponential Moving Average (EMA) model in a machine learning context. The EMA model's performance is compared to the online model, specifically focusing on validation loss as a metric. The lower the validation loss, the better the model's performance, with values around 0.1 achievable after billions of samples. However, the EMA validation loss might lag behind but should outperform in the long term.",
        "type": "comment"
    },
    "690": {
        "file_id": 21,
        "content": "                                                                                                                                                                                                             |\n| Baseline Similarity                 | Baseline similarity refers to the similarity between your dataset's prompts and associated image embeddings. This will serve as a guide for your prior's performance in cosine similarity.                                                                    | Generally `0.3` is considered a good cosine similarity for caption similarity.                                                                                                                                                                                                                                                                         |\n| Similarity With Original Image      | This metric will measure the cosine similarity between your prior's predicted image embedding and the actual image",
        "type": "code",
        "location": "/prior.md:158-160"
    },
    "691": {
        "file_id": 21,
        "content": "This code snippet is explaining the concept of baseline similarity in the context of DALLE2-pytorch, where it refers to the similarity between dataset prompts and image embeddings. It also mentions that generally, a cosine similarity value of 0.3 is considered good for caption similarity. Additionally, there's information about another metric - similarity with original image, which measures cosine similarity between the prior's predicted image embedding and the actual image.",
        "type": "comment"
    },
    "692": {
        "file_id": 21,
        "content": " that the caption was associated with. This is useful for determining wether your prior is generating images with the right contents.      | Values around `0.75`+ are obtainable. This metric should improve rapidly in the early stages of training and plateau with diminishing increases over time. If it takes hundreds of millions of samples to reach above `0.5`/`0.6` similarity--then you likely are suffering from some kind of training error or inefficiency (i.e. not using EMA) |\n| Difference From Baseline Similarity | Sometimes its useful to visualize a metric in another light. This metric will show you how your prior's predicted image embeddings match up with the baseline similarity measured in your dataset.                                                           | This value should float around `0.0` with some room for variation. After a billion samples seen, values are within `0.01`+/- of `0.0`. If this climbs to high, (~>`0.02`) then this may be a sign that your model is overfitting ",
        "type": "code",
        "location": "/prior.md:160-161"
    },
    "693": {
        "file_id": 21,
        "content": "The code provides information about the similarity metric between generated images and captions, as well as the difference from baseline similarity. The values should improve rapidly in early stages of training and plateau over time, while staying around 0 for the difference metric. Values above 0.5/0.6 or climbing to high values may indicate issues with training efficiency or overfitting, respectively.",
        "type": "comment"
    },
    "694": {
        "file_id": 21,
        "content": "somehow.                                                                                                       |\n| Similarity With Text                | This metric is your bread and butter cosine similarity between the predicted image embedding and the original caption given to the prior. Monitoring this metric will be on of your main focuses and is probably the second most important behind your loss. | As mentioned, this value should be close to baseline similarity. We have observed early rapid increase with diminishing returns as the prior learns to generate valid image embeddings. If this value increases too far beyond the baseline similarity--it could be an indication that your model is overfitting.                                       |\n| Similarity With Unrelated Caption   | This metric will attempt to exposed an overfit prior by feeding it arbitrary prompts (from your dataset) and then measure the similarity of this predicted embedding with some other image.                     ",
        "type": "code",
        "location": "/prior.md:161-163"
    },
    "695": {
        "file_id": 21,
        "content": "The code measures the cosine similarity between predicted image embeddings and original captions, as well as with unrelated captions to detect overfitting. Monitoring these metrics is crucial for model performance, as they indicate how well the model is learning from captions and generating valid image embeddings.",
        "type": "comment"
    },
    "696": {
        "file_id": 21,
        "content": "                                              | Early on we found that a poorly trained/modeled prior could effectively fool CLIP into believing that the cosine similarity between two images were high (when in fact the caption and image were completely unrelated). With this in mind--a low value is ideal, anything below `0.1` is probably safe.                                              |\n## Launching the script\nNow that youve done all the prep its time for the easy part! \nTo actually launch the script, you will either use `accelerate launch train_diffusion_prior.py --config_path <path to your config>` to launch with distributed training & huggingface accelerate or `python train_diffusion_prior.py` if you would like to train on your gpu/cpu without huggingface accelerate.\n## Checkpointing\nCheckpoints will be saved to the directory specified in your configuration file.\nAdditionally, a final checkpoint is saved before running the test split. This file will be saved to the same directory and",
        "type": "code",
        "location": "/prior.md:163-175"
    },
    "697": {
        "file_id": 21,
        "content": "The code provides instructions on how to launch the training script for a diffusion model using either distributed training with HuggingFace Accelerate or without it. It also mentions that checkpoints will be saved in the directory specified in the configuration file, and an additional final checkpoint will be saved before running the test split. The prior value should ideally be kept low to avoid fooling CLIP into believing unrelated captions and images have high cosine similarity.",
        "type": "comment"
    },
    "698": {
        "file_id": 21,
        "content": " titled latest.pth. This is to avoid problems where your `save_every` configuration does not overlap with the number of steps required to do a complete pass through the data.\n## Things To Keep In Mind\nThe prior has not been trained for tasks other than the traditional CLIP embedding translationat least yet.\nAs we finalize the replication of unCLIP, there will almost assuredly be experiments attempting to apply the prior network to other tasks.\nWith that in mind, you are more or less a pioneer in embedding-translation if you are reading this and attempting something you dont see documentation for!",
        "type": "code",
        "location": "/prior.md:175-183"
    },
    "699": {
        "file_id": 21,
        "content": "This code snippet is providing information about the \"latest.pth\" file and its purpose to avoid potential problems with `save_every` configuration not overlapping with data requirements. It also mentions that the prior network has not been trained for tasks other than traditional CLIP embedding translation, hinting at future experiments applying the prior network to other tasks.",
        "type": "comment"
    }
}