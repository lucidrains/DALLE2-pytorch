{
    "700": {
        "file_id": 22,
        "content": "/setup.py",
        "type": "filepath"
    },
    "701": {
        "file_id": 22,
        "content": "This code is a setup script for the dalle2-pytorch package using setuptools, defining project details and dependencies like PyTorch, Torchvision, and more. It's a Python project with beta development status, targeting developers in AI field, licensed under MIT, requires Python 3.6.",
        "type": "summary"
    },
    "702": {
        "file_id": 22,
        "content": "from setuptools import setup, find_packages\nexec(open('dalle2_pytorch/version.py').read())\nsetup(\n  name = 'dalle2-pytorch',\n  packages = find_packages(exclude=[]),\n  include_package_data = True,\n  entry_points={\n    'console_scripts': [\n      'dalle2_pytorch = dalle2_pytorch.cli:main',\n      'dream = dalle2_pytorch.cli:dream'\n    ],\n  },\n  version = __version__,\n  license='MIT',\n  description = 'DALL-E 2',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  long_description_content_type = 'text/markdown',\n  url = 'https://github.com/lucidrains/dalle2-pytorch',\n  keywords = [\n    'artificial intelligence',\n    'deep learning',\n    'text to image'\n  ],\n  install_requires=[\n    'accelerate',\n    'click',\n    'open-clip-torch>=2.0.0,<3.0.0',\n    'clip-anytorch>=2.5.2',\n    'coca-pytorch>=0.0.5',\n    'ema-pytorch>=0.0.7',\n    'einops>=0.7.0',\n    'embedding-reader',\n    'kornia>=0.5.4',\n    'numpy',\n    'packaging',\n    'pillow',\n    'pydantic>=2',\n    'pytorch-warmup',\n    'resize-right>=0.0.2',\n    'rotary-embedding-torch',",
        "type": "code",
        "location": "/setup.py:1-42"
    },
    "703": {
        "file_id": 22,
        "content": "This code is a setup script for the dalle2-pytorch package using setuptools. It defines the name, packages, entry points, version, license, description, author, URL, keywords, and install_requires. The script imports necessary modules and sets up dependencies for installation.",
        "type": "comment"
    },
    "704": {
        "file_id": 22,
        "content": "    'torch>=1.10',\n    'torchvision',\n    'tqdm',\n    'vector-quantize-pytorch',\n    'x-clip>=0.4.4',\n    'webdataset>=0.2.5',\n    'fsspec>=2022.1.0',\n    'torchmetrics[image]>=0.8.0'\n  ],\n  classifiers=[\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'License :: OSI Approved :: MIT License',\n    'Programming Language :: Python :: 3.6',\n  ],\n)",
        "type": "code",
        "location": "/setup.py:43-59"
    },
    "705": {
        "file_id": 22,
        "content": "This is a Python project setup file, using setuptools. It depends on PyTorch >= 1.10, Torchvision, Tqdm, VectorQuantizePytorch, X-Clip >= 0.4.4, Webdataset >= 0.2.5, FSSpec >= 2022.1.0, and TorchMetrics[image] >= 0.8.0. The project has a beta development status, is intended for developers, relates to artificial intelligence, is licensed under MIT, and requires Python 3.6.",
        "type": "comment"
    },
    "706": {
        "file_id": 23,
        "content": "/train_decoder.py",
        "type": "filepath"
    },
    "707": {
        "file_id": 23,
        "content": "This code divides shards, initializes training, and trains UNet models for DALL-E 2 using PyTorch. It also supports distributed training and executes as a standalone program.",
        "type": "summary"
    },
    "708": {
        "file_id": 23,
        "content": "from pathlib import Path\nfrom typing import List\nfrom datetime import timedelta\nfrom dalle2_pytorch.trainer import DecoderTrainer\nfrom dalle2_pytorch.dataloaders import create_image_embedding_dataloader\nfrom dalle2_pytorch.trackers import Tracker\nfrom dalle2_pytorch.train_configs import DecoderConfig, TrainDecoderConfig\nfrom dalle2_pytorch.utils import Timer, print_ribbon\nfrom dalle2_pytorch.dalle2_pytorch import Decoder, resize_image_to\nfrom clip import tokenize\nimport torchvision\nimport torch\nfrom torch import nn\nfrom torchmetrics.image.fid import FrechetInceptionDistance\nfrom torchmetrics.image.inception import InceptionScore\nfrom torchmetrics.image.kid import KernelInceptionDistance\nfrom torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\nfrom accelerate import Accelerator, DistributedDataParallelKwargs, InitProcessGroupKwargs\nfrom accelerate.utils import dataclasses as accelerate_dataclasses\nimport webdataset as wds\nimport click\n# constants\nTRAIN_CALC_LOSS_EVERY_ITERS = 10\nVALID_CALC_LOSS_EVERY_ITERS = 10",
        "type": "code",
        "location": "/train_decoder.py:1-28"
    },
    "709": {
        "file_id": 23,
        "content": "This code imports various modules and defines constants for training a decoder model in the DALLE2-pytorch framework. It uses DecoderTrainer, dataloaders, trackers, train configs, utilities, and models from the dalle2_pytorch package. It also includes metrics such as FrechetInceptionDistance, InceptionScore, KernelInceptionDistance, and LearnedPerceptualImagePatchSimilarity for evaluation. Accelerate is used for accelerated training, and webdataset is used for data loading.",
        "type": "comment"
    },
    "710": {
        "file_id": 23,
        "content": "# helpers functions\ndef exists(val):\n    return val is not None\n# main functions\ndef create_dataloaders(\n    available_shards,\n    webdataset_base_url,\n    img_embeddings_url=None,\n    text_embeddings_url=None,\n    shard_width=6,\n    num_workers=4,\n    batch_size=32,\n    n_sample_images=6,\n    shuffle_train=True,\n    resample_train=False,\n    img_preproc = None,\n    index_width=4,\n    train_prop = 0.75,\n    val_prop = 0.15,\n    test_prop = 0.10,\n    seed = 0,\n    **kwargs\n):\n    \"\"\"\n    Randomly splits the available shards into train, val, and test sets and returns a dataloader for each\n    \"\"\"\n    assert train_prop + test_prop + val_prop == 1\n    num_train = round(train_prop*len(available_shards))\n    num_test = round(test_prop*len(available_shards))\n    num_val = len(available_shards) - num_train - num_test\n    assert num_train + num_test + num_val == len(available_shards), f\"{num_train} + {num_test} + {num_val} = {num_train + num_test + num_val} != {len(available_shards)}\"\n    train_split, test_split, val_split =",
        "type": "code",
        "location": "/train_decoder.py:30-64"
    },
    "711": {
        "file_id": 23,
        "content": "This function takes available shards, URLs for embeddings, and other parameters to randomly split them into train, validation, and test sets, then returns dataloaders for each. It asserts that the proportions of splits sum up to 1, calculates the actual number of samples in each split based on the proportion, and checks if the sum of splits matches the total number of available shards.",
        "type": "comment"
    },
    "712": {
        "file_id": 23,
        "content": " torch.utils.data.random_split(available_shards, [num_train, num_test, num_val], generator=torch.Generator().manual_seed(seed))\n    # The shard number in the webdataset file names has a fixed width. We zero pad the shard numbers so they correspond to a filename.\n    train_urls = [webdataset_base_url.format(str(shard).zfill(shard_width)) for shard in train_split]\n    test_urls = [webdataset_base_url.format(str(shard).zfill(shard_width)) for shard in test_split]\n    val_urls = [webdataset_base_url.format(str(shard).zfill(shard_width)) for shard in val_split]\n    create_dataloader = lambda tar_urls, shuffle=False, resample=False, for_sampling=False: create_image_embedding_dataloader(\n        tar_url=tar_urls,\n        num_workers=num_workers,\n        batch_size=batch_size if not for_sampling else n_sample_images,\n        img_embeddings_url=img_embeddings_url,\n        text_embeddings_url=text_embeddings_url,\n        index_width=index_width,\n        shuffle_num = None,\n        extra_keys= [\"txt\"],\n        shuffle_shards = shuffle,",
        "type": "code",
        "location": "/train_decoder.py:64-80"
    },
    "713": {
        "file_id": 23,
        "content": "This code randomly splits available shards into training, testing, and validation sets. It then generates corresponding URLs for each set by zero-padding the shard numbers to match the filename format. A lambda function is created to handle creating a dataloader for image embeddings using these URLs, considering various parameters like batch size and number of workers.",
        "type": "comment"
    },
    "714": {
        "file_id": 23,
        "content": "        resample_shards = resample, \n        img_preproc=img_preproc,\n        handler=wds.handlers.warn_and_continue\n    )\n    train_dataloader = create_dataloader(train_urls, shuffle=shuffle_train, resample=resample_train)\n    train_sampling_dataloader = create_dataloader(train_urls, shuffle=False, for_sampling=True)\n    val_dataloader = create_dataloader(val_urls, shuffle=False)\n    test_dataloader = create_dataloader(test_urls, shuffle=False)\n    test_sampling_dataloader = create_dataloader(test_urls, shuffle=False, for_sampling=True)\n    return {\n        \"train\": train_dataloader,\n        \"train_sampling\": train_sampling_dataloader,\n        \"val\": val_dataloader,\n        \"test\": test_dataloader,\n        \"test_sampling\": test_sampling_dataloader\n    }\ndef get_dataset_keys(dataloader):\n    \"\"\"\n    It is sometimes neccesary to get the keys the dataloader is returning. Since the dataset is burried in the dataloader, we need to do a process to recover it.\n    \"\"\"\n    # If the dataloader is actually a WebLoader, we need to extract the real dataloader",
        "type": "code",
        "location": "/train_decoder.py:81-103"
    },
    "715": {
        "file_id": 23,
        "content": "The code creates multiple data loaders for training, validation, and testing datasets. It returns a dictionary with each dataset's corresponding dataloader. The `get_dataset_keys` function extracts the real dataloader if the input is a WebLoader.",
        "type": "comment"
    },
    "716": {
        "file_id": 23,
        "content": "    if isinstance(dataloader, wds.WebLoader):\n        dataloader = dataloader.pipeline[0]\n    return dataloader.dataset.key_map\ndef get_example_data(dataloader, device, n=5):\n    \"\"\"\n    Samples the dataloader and returns a zipped list of examples\n    \"\"\"\n    images = []\n    img_embeddings = []\n    text_embeddings = []\n    captions = []\n    for img, emb, txt in dataloader:\n        img_emb, text_emb = emb.get('img'), emb.get('text')\n        if img_emb is not None:\n            img_emb = img_emb.to(device=device, dtype=torch.float)\n            img_embeddings.extend(list(img_emb))\n        else:\n            # Then we add None img.shape[0] times\n            img_embeddings.extend([None]*img.shape[0])\n        if text_emb is not None:\n            text_emb = text_emb.to(device=device, dtype=torch.float)\n            text_embeddings.extend(list(text_emb))\n        else:\n            # Then we add None img.shape[0] times\n            text_embeddings.extend([None]*img.shape[0])\n        img = img.to(device=device, dtype=torch.float)",
        "type": "code",
        "location": "/train_decoder.py:104-130"
    },
    "717": {
        "file_id": 23,
        "content": "The code samples the dataloader and returns a zipped list of examples. It iterates through each image, extracts its embedding, converts it to the device's format, extends the respective lists for images and text embeddings, and finally returns them.",
        "type": "comment"
    },
    "718": {
        "file_id": 23,
        "content": "        images.extend(list(img))\n        captions.extend(list(txt))\n        if len(images) >= n:\n            break\n    return list(zip(images[:n], img_embeddings[:n], text_embeddings[:n], captions[:n]))\ndef generate_samples(trainer, example_data, clip=None, start_unet=1, end_unet=None, condition_on_text_encodings=False, cond_scale=1.0, device=None, text_prepend=\"\", match_image_size=True):\n    \"\"\"\n    Takes example data and generates images from the embeddings\n    Returns three lists: real images, generated images, and captions\n    \"\"\"\n    real_images, img_embeddings, text_embeddings, txts = zip(*example_data)\n    sample_params = {}\n    if img_embeddings[0] is None:\n        # Generate image embeddings from clip\n        imgs_tensor = torch.stack(real_images)\n        assert clip is not None, \"clip is None, but img_embeddings is None\"\n        imgs_tensor.to(device=device)\n        img_embeddings, img_encoding = clip.embed_image(imgs_tensor)\n        sample_params[\"image_embed\"] = img_embeddings\n    else:\n        # Then we are using precomputed image embeddings",
        "type": "code",
        "location": "/train_decoder.py:131-152"
    },
    "719": {
        "file_id": 23,
        "content": "This function generates samples by taking example data and creating real images, generated images, and captions. If image embeddings are None, it generates them using the clip model. It returns three lists: real images, generated images, and captions.",
        "type": "comment"
    },
    "720": {
        "file_id": 23,
        "content": "        img_embeddings = torch.stack(img_embeddings)\n        sample_params[\"image_embed\"] = img_embeddings\n    if condition_on_text_encodings:\n        if text_embeddings[0] is None:\n            # Generate text embeddings from text\n            assert clip is not None, \"clip is None, but text_embeddings is None\"\n            tokenized_texts = tokenize(txts, truncate=True).to(device=device)\n            text_embed, text_encodings = clip.embed_text(tokenized_texts)\n            sample_params[\"text_encodings\"] = text_encodings\n        else:\n            # Then we are using precomputed text embeddings\n            text_embeddings = torch.stack(text_embeddings)\n            sample_params[\"text_encodings\"] = text_embeddings\n    sample_params[\"start_at_unet_number\"] = start_unet\n    sample_params[\"stop_at_unet_number\"] = end_unet\n    if start_unet > 1:\n        # If we are only training upsamplers\n        sample_params[\"image\"] = torch.stack(real_images)\n    if device is not None:\n        sample_params[\"_device\"] = device",
        "type": "code",
        "location": "/train_decoder.py:153-172"
    },
    "721": {
        "file_id": 23,
        "content": "This code is responsible for preparing training samples by stacking image and text embeddings, setting parameters for start and stop U-net layers, and handling the case where real images are provided. If real images exist, it stacks them as part of the sample. The code also considers whether to generate text embeddings or use precomputed ones and ensures everything is on the specified device.",
        "type": "comment"
    },
    "722": {
        "file_id": 23,
        "content": "    samples = trainer.sample(**sample_params, _cast_deepspeed_precision=False)  # At sampling time we don't want to cast to FP16\n    generated_images = list(samples)\n    captions = [text_prepend + txt for txt in txts]\n    if match_image_size:\n        generated_image_size = generated_images[0].shape[-1]\n        real_images = [resize_image_to(image, generated_image_size, clamp_range=(0, 1)) for image in real_images]\n    return real_images, generated_images, captions\ndef generate_grid_samples(trainer, examples, clip=None, start_unet=1, end_unet=None, condition_on_text_encodings=False, cond_scale=1.0, device=None, text_prepend=\"\"):\n    \"\"\"\n    Generates samples and uses torchvision to put them in a side by side grid for easy viewing\n    \"\"\"\n    real_images, generated_images, captions = generate_samples(trainer, examples, clip, start_unet, end_unet, condition_on_text_encodings, cond_scale, device, text_prepend)\n    grid_images = [torchvision.utils.make_grid([original_image, generated_image]) for original_image, generated_image in zip(real_images, generated_images)]",
        "type": "code",
        "location": "/train_decoder.py:173-186"
    },
    "723": {
        "file_id": 23,
        "content": "This function generates samples, combines them with real images in a grid format for easy viewing. It first calls `generate_samples` to get the real and generated images along with their corresponding captions. Then it uses `torchvision.utils.make_grid` to create grids of original and generated images.",
        "type": "comment"
    },
    "724": {
        "file_id": 23,
        "content": "    return grid_images, captions\ndef evaluate_trainer(trainer, dataloader, device, start_unet, end_unet, clip=None, condition_on_text_encodings=False, cond_scale=1.0, inference_device=None, n_evaluation_samples=1000, FID=None, IS=None, KID=None, LPIPS=None):\n    \"\"\"\n    Computes evaluation metrics for the decoder\n    \"\"\"\n    metrics = {}\n    # Prepare the data\n    examples = get_example_data(dataloader, device, n_evaluation_samples)\n    if len(examples) == 0:\n        print(\"No data to evaluate. Check that your dataloader has shards.\")\n        return metrics\n    real_images, generated_images, captions = generate_samples(trainer, examples, clip, start_unet, end_unet, condition_on_text_encodings, cond_scale, inference_device)\n    real_images = torch.stack(real_images).to(device=device, dtype=torch.float)\n    generated_images = torch.stack(generated_images).to(device=device, dtype=torch.float)\n    # Convert from [0, 1] to [0, 255] and from torch.float to torch.uint8\n    int_real_images = real_images.mul(255).add(0.5).clamp(0, 255).type(torch.uint8)",
        "type": "code",
        "location": "/train_decoder.py:187-203"
    },
    "725": {
        "file_id": 23,
        "content": "This function computes evaluation metrics for a decoder. It prepares data, generates samples using the trainer and start/end unets, converts images from [0, 1] to [0, 255], and types them as uint8. The generated and real images are then stored in variables for further evaluation metrics calculations.",
        "type": "comment"
    },
    "726": {
        "file_id": 23,
        "content": "    int_generated_images = generated_images.mul(255).add(0.5).clamp(0, 255).type(torch.uint8)\n    def null_sync(t, *args, **kwargs):\n        return [t]\n    if exists(FID):\n        fid = FrechetInceptionDistance(**FID, dist_sync_fn=null_sync)\n        fid.to(device=device)\n        fid.update(int_real_images, real=True)\n        fid.update(int_generated_images, real=False)\n        metrics[\"FID\"] = fid.compute().item()\n    if exists(IS):\n        inception = InceptionScore(**IS, dist_sync_fn=null_sync)\n        inception.to(device=device)\n        inception.update(int_real_images)\n        is_mean, is_std = inception.compute()\n        metrics[\"IS_mean\"] = is_mean.item()\n        metrics[\"IS_std\"] = is_std.item()\n    if exists(KID):\n        kernel_inception = KernelInceptionDistance(**KID, dist_sync_fn=null_sync)\n        kernel_inception.to(device=device)\n        kernel_inception.update(int_real_images, real=True)\n        kernel_inception.update(int_generated_images, real=False)\n        kid_mean, kid_std = kernel_inception.compute()",
        "type": "code",
        "location": "/train_decoder.py:204-227"
    },
    "727": {
        "file_id": 23,
        "content": "This code calculates and stores metrics for the quality of generated images, including Frechet Inception Distance (FID), Inception Score (IS), and Kernel Inception Distance (KID). It first scales the generated images, then checks if specific configuration files exist for each metric. If they do, it creates an instance of the corresponding metric class, sets it up on the device, updates with real and generated images, and computes the metric values. The computed metrics are stored in the \"metrics\" dictionary.",
        "type": "comment"
    },
    "728": {
        "file_id": 23,
        "content": "        metrics[\"KID_mean\"] = kid_mean.item()\n        metrics[\"KID_std\"] = kid_std.item()\n    if exists(LPIPS):\n        # Convert from [0, 1] to [-1, 1]\n        renorm_real_images = real_images.mul(2).sub(1).clamp(-1,1)\n        renorm_generated_images = generated_images.mul(2).sub(1).clamp(-1,1)\n        lpips = LearnedPerceptualImagePatchSimilarity(**LPIPS, dist_sync_fn=null_sync)\n        lpips.to(device=device)\n        lpips.update(renorm_real_images, renorm_generated_images)\n        metrics[\"LPIPS\"] = lpips.compute().item()\n    if trainer.accelerator.num_processes > 1:\n        # Then we should sync the metrics\n        metrics_order = sorted(metrics.keys())\n        metrics_tensor = torch.zeros(1, len(metrics), device=device, dtype=torch.float)\n        for i, metric_name in enumerate(metrics_order):\n            metrics_tensor[0, i] = metrics[metric_name]\n        metrics_tensor = trainer.accelerator.gather(metrics_tensor)\n        metrics_tensor = metrics_tensor.mean(dim=0)\n        for i, metric_name in enumerate(metrics_order):",
        "type": "code",
        "location": "/train_decoder.py:228-247"
    },
    "729": {
        "file_id": 23,
        "content": "This code calculates metrics such as KID and LPIPS for a model's performance. It stores the values in a dictionary, normalizes the images if LPIPS is present, applies the LearnedPerceptualImagePatchSimilarity function, and syncs the calculated metrics across processes using accelerator functions.",
        "type": "comment"
    },
    "730": {
        "file_id": 23,
        "content": "            metrics[metric_name] = metrics_tensor[i].item()\n    return metrics\ndef save_trainer(tracker: Tracker, trainer: DecoderTrainer, epoch: int, sample: int, next_task: str, validation_losses: List[float], samples_seen: int, is_latest=True, is_best=False):\n    \"\"\"\n    Logs the model with an appropriate method depending on the tracker\n    \"\"\"\n    tracker.save(trainer, is_best=is_best, is_latest=is_latest, epoch=epoch, sample=sample, next_task=next_task, validation_losses=validation_losses, samples_seen=samples_seen)\ndef recall_trainer(tracker: Tracker, trainer: DecoderTrainer):\n    \"\"\"\n    Loads the model with an appropriate method depending on the tracker\n    \"\"\"\n    trainer.accelerator.print(print_ribbon(f\"Loading model from {type(tracker.loader).__name__}\"))\n    state_dict = tracker.recall()\n    trainer.load_state_dict(state_dict, only_model=False, strict=True)\n    return state_dict.get(\"epoch\", 0), state_dict.get(\"validation_losses\", []), state_dict.get(\"next_task\", \"train\"), state_dict.get(\"sample\", 0), state_dict.get(\"samples_seen\", 0)",
        "type": "code",
        "location": "/train_decoder.py:248-264"
    },
    "731": {
        "file_id": 23,
        "content": "This code contains three functions: 1) `train_decoder`, which updates metrics based on the current metric; 2) `save_trainer`, which logs the model using an appropriate method according to the tracker; and 3) `recall_trainer`, which loads the model using the tracker. The code is part of a larger system that likely involves training a machine learning model, tracking its progress, and recalling it for further use or evaluation.",
        "type": "comment"
    },
    "732": {
        "file_id": 23,
        "content": "def train(\n    dataloaders,\n    decoder: Decoder,\n    accelerator: Accelerator,\n    tracker: Tracker,\n    inference_device,\n    clip=None,\n    evaluate_config=None,\n    epoch_samples = None,  # If the training dataset is resampling, we have to manually stop an epoch\n    validation_samples = None,\n    save_immediately=False,\n    epochs = 20,\n    n_sample_images = 5,\n    save_every_n_samples = 100000,\n    unet_training_mask=None,\n    condition_on_text_encodings=False,\n    cond_scale=1.0,\n    **kwargs\n):\n    \"\"\"\n    Trains a decoder on a dataset.\n    \"\"\"\n    is_master = accelerator.process_index == 0\n    if not exists(unet_training_mask):\n        # Then the unet mask should be true for all unets in the decoder\n        unet_training_mask = [True] * len(decoder.unets)\n    assert len(unet_training_mask) == len(decoder.unets), f\"The unet training mask should be the same length as the number of unets in the decoder. Got {len(unet_training_mask)} and {trainer.num_unets}\"\n    trainable_unet_numbers = [i+1 for i, trainable in enumerate(unet_training_mask) if trainable]",
        "type": "code",
        "location": "/train_decoder.py:266-294"
    },
    "733": {
        "file_id": 23,
        "content": "The function trains a decoder on a dataset, using the specified dataloaders, Decoder instance, and Accelerator. It also has optional arguments for clip, evaluate_config, epoch_samples, validation_samples, save_immediately, epochs, n_sample_images, save_every_n_samples, unet_training_mask, condition_on_text_encodings, and cond_scale. The function checks if the unet_training_mask exists and asserts that its length matches the number of unets in the decoder. It also assigns trainable unet numbers to a list.",
        "type": "comment"
    },
    "734": {
        "file_id": 23,
        "content": "    first_trainable_unet = trainable_unet_numbers[0]\n    last_trainable_unet = trainable_unet_numbers[-1]\n    def move_unets(unet_training_mask):\n        for i in range(len(decoder.unets)):\n            if not unet_training_mask[i]:\n                # Replace the unet from the module list with a nn.Identity(). This training script never uses unets that aren't being trained so this is fine.\n                decoder.unets[i] = nn.Identity().to(inference_device)\n    # Remove non-trainable unets\n    move_unets(unet_training_mask)\n    trainer = DecoderTrainer(\n        decoder=decoder,\n        accelerator=accelerator,\n        dataloaders=dataloaders,\n        **kwargs\n    )\n    # Set up starting model and parameters based on a recalled state dict\n    start_epoch = 0\n    validation_losses = []\n    next_task = 'train'\n    sample = 0\n    samples_seen = 0\n    val_sample = 0\n    step = lambda: int(trainer.num_steps_taken(unet_number=first_trainable_unet))\n    if tracker.can_recall:\n        start_epoch, validation_losses, next_task, recalled_sample, samples_seen = recall_trainer(tracker, trainer)",
        "type": "code",
        "location": "/train_decoder.py:295-322"
    },
    "735": {
        "file_id": 23,
        "content": "The code is removing non-trainable UNet modules and setting up a trainer for the given task. It also checks if the state can be recalled from a previous training session and updates relevant variables accordingly.",
        "type": "comment"
    },
    "736": {
        "file_id": 23,
        "content": "        if next_task == 'train':\n            sample = recalled_sample\n        if next_task == 'val':\n            val_sample = recalled_sample\n        accelerator.print(f\"Loaded model from {type(tracker.loader).__name__} on epoch {start_epoch} having seen {samples_seen} samples with minimum validation loss {min(validation_losses) if len(validation_losses) > 0 else 'N/A'}\")\n        accelerator.print(f\"Starting training from task {next_task} at sample {sample} and validation sample {val_sample}\")\n    trainer.to(device=inference_device)\n    accelerator.print(print_ribbon(\"Generating Example Data\", repeat=40))\n    accelerator.print(\"This can take a while to load the shard lists...\")\n    if is_master:\n        train_example_data = get_example_data(dataloaders[\"train_sampling\"], inference_device, n_sample_images)\n        accelerator.print(\"Generated training examples\")\n        test_example_data = get_example_data(dataloaders[\"test_sampling\"], inference_device, n_sample_images)\n        accelerator.print(\"Generated testing examples\")",
        "type": "code",
        "location": "/train_decoder.py:323-337"
    },
    "737": {
        "file_id": 23,
        "content": "The code loads a model and starts training from the specified task, either 'train' or 'val'. It prints the details of the loaded model, including epoch, samples seen, and minimum validation loss. The trainer is moved to the inference device. Example data for both training and testing is generated using get_example_data function with the specified number of sample images.",
        "type": "comment"
    },
    "738": {
        "file_id": 23,
        "content": "    send_to_device = lambda arr: [x.to(device=inference_device, dtype=torch.float) for x in arr]\n    sample_length_tensor = torch.zeros(1, dtype=torch.int, device=inference_device)\n    unet_losses_tensor = torch.zeros(TRAIN_CALC_LOSS_EVERY_ITERS, trainer.num_unets, dtype=torch.float, device=inference_device)\n    for epoch in range(start_epoch, epochs):\n        accelerator.print(print_ribbon(f\"Starting epoch {epoch}\", repeat=40))\n        timer = Timer()\n        last_sample = sample\n        last_snapshot = sample\n        if next_task == 'train':\n            for i, (img, emb, txt) in enumerate(dataloaders[\"train\"]):\n                # We want to count the total number of samples across all processes\n                sample_length_tensor[0] = len(img)\n                all_samples = accelerator.gather(sample_length_tensor)  # TODO: accelerator.reduce is broken when this was written. If it is fixed replace this.\n                total_samples = all_samples.sum().item()\n                sample += total_samples\n                samples_seen += total_samples",
        "type": "code",
        "location": "/train_decoder.py:339-357"
    },
    "739": {
        "file_id": 23,
        "content": "Iterating over epochs in training mode, counting the total number of samples across all processes. Gathering sample length tensors using accelerator's gather function and summing them up to get the total samples seen. Updating sample and samples_seen variables accordingly.",
        "type": "comment"
    },
    "740": {
        "file_id": 23,
        "content": "                img_emb = emb.get('img')\n                has_img_embedding = img_emb is not None\n                if has_img_embedding:\n                    img_emb, = send_to_device((img_emb,))\n                text_emb = emb.get('text')\n                has_text_embedding = text_emb is not None\n                if has_text_embedding:\n                    text_emb, = send_to_device((text_emb,))\n                img, = send_to_device((img,))\n                trainer.train()\n                for unet in range(1, trainer.num_unets+1):\n                    # Check if this is a unet we are training\n                    if not unet_training_mask[unet-1]: # Unet index is the unet number - 1\n                        continue\n                    forward_params = {}\n                    if has_img_embedding:\n                        forward_params['image_embed'] = img_emb\n                    else:\n                        # Forward pass automatically generates embedding\n                        assert clip is not None\n                        img_embed, img_encoding = clip.embed_image(img)",
        "type": "code",
        "location": "/train_decoder.py:358-380"
    },
    "741": {
        "file_id": 23,
        "content": "This code checks if there are image or text embeddings available, sends them to the device, and then trains a model. It also performs a forward pass for image embedding generation if necessary.",
        "type": "comment"
    },
    "742": {
        "file_id": 23,
        "content": "                        forward_params['image_embed'] = img_embed\n                    if condition_on_text_encodings:\n                        if has_text_embedding:\n                            forward_params['text_encodings'] = text_emb\n                        else:\n                            # Then we need to pass the text instead\n                            assert clip is not None\n                            tokenized_texts = tokenize(txt, truncate=True).to(inference_device)\n                            assert tokenized_texts.shape[0] == len(img), f\"The number of texts ({tokenized_texts.shape[0]}) should be the same as the number of images ({len(img)})\"\n                            text_embed, text_encodings = clip.embed_text(tokenized_texts)\n                            forward_params['text_encodings'] = text_encodings\n                    loss = trainer.forward(img, **forward_params, unet_number=unet, _device=inference_device)\n                    trainer.update(unet_number=unet)\n                    unet_losses_tensor[i % TRAIN_CALC_LOSS_EVERY_ITERS, unet-1] = loss",
        "type": "code",
        "location": "/train_decoder.py:381-394"
    },
    "743": {
        "file_id": 23,
        "content": "This code chunk is for training the DALL-E 2 model's decoder. It first checks if image and text embeddings are provided, and if not, it tokenizes the text and generates text embeddings using the CLIP model. Then, it passes the required parameters to the trainer and updates the model, storing the loss for each unit in the unet_losses_tensor array.",
        "type": "comment"
    },
    "744": {
        "file_id": 23,
        "content": "                samples_per_sec = (sample - last_sample) / timer.elapsed()\n                timer.reset()\n                last_sample = sample\n                if i % TRAIN_CALC_LOSS_EVERY_ITERS == 0:\n                    # We want to average losses across all processes\n                    unet_all_losses = accelerator.gather(unet_losses_tensor)\n                    mask = unet_all_losses != 0\n                    unet_average_loss = (unet_all_losses * mask).sum(dim=0) / mask.sum(dim=0)\n                    loss_map = { f\"Unet {index} Training Loss\": loss.item() for index, loss in enumerate(unet_average_loss) if unet_training_mask[index] }\n                    # gather decay rate on each UNet\n                    ema_decay_list = {f\"Unet {index} EMA Decay\": ema_unet.get_current_decay() for index, ema_unet in enumerate(trainer.ema_unets) if unet_training_mask[index]}\n                    log_data = {\n                        \"Epoch\": epoch,\n                        \"Sample\": sample,\n                        \"Step\": i,",
        "type": "code",
        "location": "/train_decoder.py:396-413"
    },
    "745": {
        "file_id": 23,
        "content": "This code is calculating the samples per second and resetting timers, then averaging the losses across all processes for a UNet model. It gathers the decay rate on each UNet, logs epoch, sample, and step information.",
        "type": "comment"
    },
    "746": {
        "file_id": 23,
        "content": "                        \"Samples per second\": samples_per_sec,\n                        \"Samples Seen\": samples_seen,\n                        **ema_decay_list,\n                        **loss_map\n                    }\n                    if is_master:\n                        tracker.log(log_data, step=step())\n                if is_master and (last_snapshot + save_every_n_samples < sample or (save_immediately and i == 0)):  # This will miss by some amount every time, but it's not a big deal... I hope\n                    # It is difficult to gather this kind of info on the accelerator, so we have to do it on the master\n                    print(\"Saving snapshot\")\n                    last_snapshot = sample\n                    # We need to know where the model should be saved\n                    save_trainer(tracker, trainer, epoch, sample, next_task, validation_losses, samples_seen)\n                    if exists(n_sample_images) and n_sample_images > 0:\n                        trainer.eval()\n             ",
        "type": "code",
        "location": "/train_decoder.py:414-431"
    },
    "747": {
        "file_id": 23,
        "content": "This code snippet is logging data and saving a snapshot of the model at specific intervals. It logs samples per second, samples seen, EMA decay parameters, and loss metrics. The snapshot is saved if the current sample meets certain conditions or every time an immediate save command is issued. The code prints \"Saving snapshot\" when a snapshot is taken.",
        "type": "comment"
    },
    "748": {
        "file_id": 23,
        "content": "           train_images, train_captions = generate_grid_samples(trainer, train_example_data, clip, first_trainable_unet, last_trainable_unet, condition_on_text_encodings, cond_scale, inference_device, \"Train: \")\n                        tracker.log_images(train_images, captions=train_captions, image_section=\"Train Samples\", step=step())\n                if epoch_samples is not None and sample >= epoch_samples:\n                    break\n            next_task = 'val'\n            sample = 0\n        all_average_val_losses = None\n        if next_task == 'val':\n            trainer.eval()\n            accelerator.print(print_ribbon(f\"Starting Validation {epoch}\", repeat=40))\n            last_val_sample = val_sample\n            val_sample_length_tensor = torch.zeros(1, dtype=torch.int, device=inference_device)\n            average_val_loss_tensor = torch.zeros(1, trainer.num_unets, dtype=torch.float, device=inference_device)\n            timer = Timer()\n            accelerator.wait_for_everyone()\n            i = 0",
        "type": "code",
        "location": "/train_decoder.py:431-448"
    },
    "749": {
        "file_id": 23,
        "content": "This code is used for training a model and validating it. It generates samples from the training dataset, logs them, checks if it should stop based on sample count, switches to validation mode, and initializes variables for validation.",
        "type": "comment"
    },
    "750": {
        "file_id": 23,
        "content": "            for i, (img, emb, txt) in enumerate(dataloaders['val']):  # Use the accelerate prepared loader\n                val_sample_length_tensor[0] = len(img)\n                all_samples = accelerator.gather(val_sample_length_tensor)\n                total_samples = all_samples.sum().item()\n                val_sample += total_samples\n                img_emb = emb.get('img')\n                has_img_embedding = img_emb is not None\n                if has_img_embedding:\n                    img_emb, = send_to_device((img_emb,))\n                text_emb = emb.get('text')\n                has_text_embedding = text_emb is not None\n                if has_text_embedding:\n                    text_emb, = send_to_device((text_emb,))\n                img, = send_to_device((img,))\n                for unet in range(1, len(decoder.unets)+1):\n                    if not unet_training_mask[unet-1]: # Unet index is the unet number - 1\n                        # No need to evaluate an unchanging unet\n                        continue",
        "type": "code",
        "location": "/train_decoder.py:449-467"
    },
    "751": {
        "file_id": 23,
        "content": "This code is part of the DALLE2-pytorch training process. It iterates over the validation dataloader, gathers sample lengths, calculates total samples, and checks for image and text embeddings. If available, it sends these embeddings along with images to the device for further processing. This code ensures that all necessary data is properly prepared and sent to the device for evaluation.",
        "type": "comment"
    },
    "752": {
        "file_id": 23,
        "content": "                    forward_params = {}\n                    if has_img_embedding:\n                        forward_params['image_embed'] = img_emb.float()\n                    else:\n                        # Forward pass automatically generates embedding\n                        assert clip is not None\n                        img_embed, img_encoding = clip.embed_image(img)\n                        forward_params['image_embed'] = img_embed\n                    if condition_on_text_encodings:\n                        if has_text_embedding:\n                            forward_params['text_encodings'] = text_emb.float()\n                        else:\n                            # Then we need to pass the text instead\n                            assert clip is not None\n                            tokenized_texts = tokenize(txt, truncate=True).to(device=inference_device)\n                            assert tokenized_texts.shape[0] == len(img), f\"The number of texts ({tokenized_texts.shape[0]}) should be the same as the number of images ({len(img)})\"",
        "type": "code",
        "location": "/train_decoder.py:469-484"
    },
    "753": {
        "file_id": 23,
        "content": "This code segment checks if image and text embeddings are provided. If not, it automatically generates image embedding or passes the text instead based on the condition. It also asserts the number of texts should be equal to the number of images for consistency.",
        "type": "comment"
    },
    "754": {
        "file_id": 23,
        "content": "                            text_embed, text_encodings = clip.embed_text(tokenized_texts)\n                            forward_params['text_encodings'] = text_encodings\n                    loss = trainer.forward(img.float(), **forward_params, unet_number=unet, _device=inference_device)\n                    average_val_loss_tensor[0, unet-1] += loss\n                if i % VALID_CALC_LOSS_EVERY_ITERS == 0:\n                    samples_per_sec = (val_sample - last_val_sample) / timer.elapsed()\n                    timer.reset()\n                    last_val_sample = val_sample\n                    accelerator.print(f\"Epoch {epoch}/{epochs} Val Step {i} -  Sample {val_sample} - {samples_per_sec:.2f} samples/sec\")\n                    accelerator.print(f\"Loss: {(average_val_loss_tensor / (i+1))}\")\n                    accelerator.print(\"\")\n                if validation_samples is not None and val_sample >= validation_samples:\n                    break\n            print(f\"Rank {accelerator.state.process_index} finished validation after {i} steps\")",
        "type": "code",
        "location": "/train_decoder.py:485-500"
    },
    "755": {
        "file_id": 23,
        "content": "This code snippet is part of a larger model training process. It calculates the loss based on input images and text, updates the average validation loss, prints validation progress including samples per second and loss, and eventually breaks the loop when the specified number of validation samples have been processed. The code uses the PyTorch framework and the DALLE2 library for embedding text.",
        "type": "comment"
    },
    "756": {
        "file_id": 23,
        "content": "            accelerator.wait_for_everyone()\n            average_val_loss_tensor /= i+1\n            # Gather all the average loss tensors\n            all_average_val_losses = accelerator.gather(average_val_loss_tensor)\n            if is_master:\n                unet_average_val_loss = all_average_val_losses.mean(dim=0)\n                val_loss_map = { f\"Unet {index} Validation Loss\": loss.item() for index, loss in enumerate(unet_average_val_loss) if loss != 0 }\n                tracker.log(val_loss_map, step=step())\n            next_task = 'eval'\n        if next_task == 'eval':\n            if exists(evaluate_config):\n                accelerator.print(print_ribbon(f\"Starting Evaluation {epoch}\", repeat=40))\n                evaluation = evaluate_trainer(trainer, dataloaders[\"val\"], inference_device, first_trainable_unet, last_trainable_unet, clip=clip, inference_device=inference_device, **evaluate_config.model_dump(), condition_on_text_encodings=condition_on_text_encodings, cond_scale=cond_scale)\n                if is_master:",
        "type": "code",
        "location": "/train_decoder.py:501-515"
    },
    "757": {
        "file_id": 23,
        "content": "This code is used for averaging the validation losses and logging them during training. It also starts the evaluation process if it's time to do so, printing a message to indicate this. The average_val_loss_tensor is gathered by the accelerator, and then the mean of all the average loss tensors is calculated if the current task is 'eval'. If there are no zeros in the unet_average_val_loss, the validation losses are logged.",
        "type": "comment"
    },
    "758": {
        "file_id": 23,
        "content": "                    tracker.log(evaluation, step=step())\n            next_task = 'sample'\n            val_sample = 0\n        if next_task == 'sample':\n            if is_master:\n                # Generate examples and save the model if we are the master\n                # Generate sample images\n                print(print_ribbon(f\"Sampling Set {epoch}\", repeat=40))\n                test_images, test_captions = generate_grid_samples(trainer, test_example_data, clip, first_trainable_unet, last_trainable_unet, condition_on_text_encodings, cond_scale, inference_device, \"Test: \")\n                train_images, train_captions = generate_grid_samples(trainer, train_example_data, clip, first_trainable_unet, last_trainable_unet, condition_on_text_encodings, cond_scale, inference_device, \"Train: \")\n                tracker.log_images(test_images, captions=test_captions, image_section=\"Test Samples\", step=step())\n                tracker.log_images(train_images, captions=train_captions, image_section=\"Train Samples\", step=step())",
        "type": "code",
        "location": "/train_decoder.py:516-528"
    },
    "759": {
        "file_id": 23,
        "content": "The code is generating sample images and saving the model if it is the master process. It prints a ribbon and then generates grid samples from both train and test example data, conditioning on text encodings. Finally, it logs the generated images using the tracker, with labels indicating whether they are test or train samples.",
        "type": "comment"
    },
    "760": {
        "file_id": 23,
        "content": "                print(print_ribbon(f\"Starting Saving {epoch}\", repeat=40))\n                is_best = False\n                if all_average_val_losses is not None:\n                    average_loss = all_average_val_losses.mean(dim=0).sum() / sum(unet_training_mask)\n                    if len(validation_losses) == 0 or average_loss < min(validation_losses):\n                        is_best = True\n                    validation_losses.append(average_loss)\n                save_trainer(tracker, trainer, epoch, sample, next_task, validation_losses, samples_seen, is_best=is_best)\n            next_task = 'train'\ndef create_tracker(accelerator: Accelerator, config: TrainDecoderConfig, config_path: str, dummy: bool = False) -> Tracker:\n    tracker_config = config.tracker\n    accelerator_config = {\n        \"Distributed\": accelerator.distributed_type != accelerate_dataclasses.DistributedType.NO,\n        \"DistributedType\": accelerator.distributed_type,\n        \"NumProcesses\": accelerator.num_processes,\n        \"MixedPrecision\": accelerator.mixed_precision",
        "type": "code",
        "location": "/train_decoder.py:530-546"
    },
    "761": {
        "file_id": 23,
        "content": "The code checks if the average validation loss is lower than previous min, and saves the trainer if it's a new minimum. It's part of a function called create_tracker that creates a tracker object with accelerator, config, and dummy parameters.",
        "type": "comment"
    },
    "762": {
        "file_id": 23,
        "content": "    }\n    accelerator.wait_for_everyone()  # If nodes arrive at this point at different times they might try to autoresume the current run which makes no sense and will cause errors\n    tracker: Tracker = tracker_config.create(config, accelerator_config, dummy_mode=dummy)\n    tracker.save_config(config_path, config_name='decoder_config.json')\n    tracker.add_save_metadata(state_dict_key='config', metadata=config.model_dump())\n    return tracker\ndef initialize_training(config: TrainDecoderConfig, config_path):\n    # Make sure if we are not loading, distributed models are initialized to the same values\n    torch.manual_seed(config.seed)\n    # Set up accelerator for configurable distributed training\n    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=config.train.find_unused_parameters, static_graph=config.train.static_graph)\n    init_kwargs = InitProcessGroupKwargs(timeout=timedelta(seconds=60*60))\n    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs, init_kwargs])\n    if accelerator.num_processes > 1:",
        "type": "code",
        "location": "/train_decoder.py:547-563"
    },
    "763": {
        "file_id": 23,
        "content": "This code initializes distributed training for DALLE2, sets manual seed, and creates an accelerator for parallel processing with optional arguments. The function returns a tracker object to save configuration.",
        "type": "comment"
    },
    "764": {
        "file_id": 23,
        "content": "        # We are using distributed training and want to immediately ensure all can connect\n        accelerator.print(\"Waiting for all processes to connect...\")\n        accelerator.wait_for_everyone()\n        accelerator.print(\"All processes online and connected\")\n    # If we are in deepspeed fp16 mode, we must ensure learned variance is off\n    if accelerator.mixed_precision == \"fp16\" and accelerator.distributed_type == accelerate_dataclasses.DistributedType.DEEPSPEED and config.decoder.learned_variance:\n        raise ValueError(\"DeepSpeed fp16 mode does not support learned variance\")\n    # Set up data\n    all_shards = list(range(config.data.start_shard, config.data.end_shard + 1))\n    world_size = accelerator.num_processes\n    rank = accelerator.process_index\n    shards_per_process = len(all_shards) // world_size\n    assert shards_per_process > 0, \"Not enough shards to split evenly\"\n    my_shards = all_shards[rank * shards_per_process: (rank + 1) * shards_per_process]\n    dataloaders = create_dataloaders (",
        "type": "code",
        "location": "/train_decoder.py:564-581"
    },
    "765": {
        "file_id": 23,
        "content": "This code snippet is part of a distributed training process where it checks the accelerator settings, data sharding, and creates dataloaders for training. It ensures all processes are connected, handles DeepSpeed mixed precision mode without learned variance, splits data shards evenly across processes, and finally creates the necessary dataloaders for the training process.",
        "type": "comment"
    },
    "766": {
        "file_id": 23,
        "content": "        available_shards=my_shards,\n        img_preproc = config.data.img_preproc,\n        train_prop = config.data.splits.train,\n        val_prop = config.data.splits.val,\n        test_prop = config.data.splits.test,\n        n_sample_images=config.train.n_sample_images,\n        **config.data.model_dump(),\n        rank = rank,\n        seed = config.seed,\n    )\n    # If clip is in the model, we need to remove it for compatibility with deepspeed\n    clip = None\n    if config.decoder.clip is not None:\n        clip = config.decoder.clip.create()  # Of course we keep it to use it during training, just not in the decoder as that causes issues\n        config.decoder.clip = None\n    # Create the decoder model and print basic info\n    decoder = config.decoder.create()\n    get_num_parameters = lambda model, only_training=False: sum(p.numel() for p in model.parameters() if (p.requires_grad or not only_training))\n    # Create and initialize the tracker if we are the master\n    tracker = create_tracker(accelerator, config, config_path, dummy = rank!=0)",
        "type": "code",
        "location": "/train_decoder.py:582-603"
    },
    "767": {
        "file_id": 23,
        "content": "The code initializes the decoder model with specified parameters, removes clip if present for compatibility, and creates a tracker if the current rank is not the master. It also calculates the number of parameters in the model and prepares it for training.",
        "type": "comment"
    },
    "768": {
        "file_id": 23,
        "content": "    has_img_embeddings = config.data.img_embeddings_url is not None\n    has_text_embeddings = config.data.text_embeddings_url is not None\n    conditioning_on_text = any([unet.cond_on_text_encodings for unet in config.decoder.unets])\n    has_clip_model = clip is not None\n    data_source_string = \"\"\n    if has_img_embeddings:\n        data_source_string += \"precomputed image embeddings\"\n    elif has_clip_model:\n        data_source_string += \"clip image embeddings generation\"\n    else:\n        raise ValueError(\"No image embeddings source specified\")\n    if conditioning_on_text:\n        if has_text_embeddings:\n            data_source_string += \" and precomputed text embeddings\"\n        elif has_clip_model:\n            data_source_string += \" and clip text encoding generation\"\n        else:\n            raise ValueError(\"No text embeddings source specified\")\n    accelerator.print(print_ribbon(\"Loaded Config\", repeat=40))\n    accelerator.print(f\"Running training with {accelerator.num_processes} processes and {accelerator.distributed_type} distributed training\")",
        "type": "code",
        "location": "/train_decoder.py:605-627"
    },
    "769": {
        "file_id": 23,
        "content": "This code checks if image and/or text embeddings are available, either precomputed or generated using CLIP model. It then prints a message indicating the source of embeddings used for training.",
        "type": "comment"
    },
    "770": {
        "file_id": 23,
        "content": "    accelerator.print(f\"Training using {data_source_string}. {'conditioned on text' if conditioning_on_text else 'not conditioned on text'}\")\n    accelerator.print(f\"Number of parameters: {get_num_parameters(decoder)} total; {get_num_parameters(decoder, only_training=True)} training\")\n    for i, unet in enumerate(decoder.unets):\n        accelerator.print(f\"Unet {i} has {get_num_parameters(unet)} total; {get_num_parameters(unet, only_training=True)} training\")\n    train(dataloaders, decoder, accelerator,\n        clip=clip,\n        tracker=tracker,\n        inference_device=accelerator.device,\n        evaluate_config=config.evaluate,\n        condition_on_text_encodings=conditioning_on_text,\n        **config.train.model_dump(),\n    )\n# Create a simple click command line interface to load the config and start the training\n@click.command()\n@click.option(\"--config_file\", default=\"./train_decoder_config.json\", help=\"Path to config file\")\ndef main(config_file):\n    config_file_path = Path(config_file)\n    config = TrainDecoderConfig.from_json_path(str(config_file_path))",
        "type": "code",
        "location": "/train_decoder.py:628-647"
    },
    "771": {
        "file_id": 23,
        "content": "Training of the decoder is being executed using the specified data source, with or without conditioning on text. The number of parameters in total and for training are displayed, along with similar information for each Unet. The train function is called with dataloaders, decoder, accelerator, clip, tracker, inference_device, evaluate_config, and condition_on_text_encodings as arguments. A simple click command line interface is created to load the config and start training, using a default configuration file path and allowing for an alternative path to be specified with the --config_file option.",
        "type": "comment"
    },
    "772": {
        "file_id": 23,
        "content": "    initialize_training(config, config_path=config_file_path)\nif __name__ == \"__main__\":\n    main()",
        "type": "code",
        "location": "/train_decoder.py:648-651"
    },
    "773": {
        "file_id": 23,
        "content": "This code snippet initializes training and then calls the main function if the script is run directly. It ensures proper execution when running the script as a standalone program.",
        "type": "comment"
    },
    "774": {
        "file_id": 24,
        "content": "/train_diffusion_prior.py",
        "type": "filepath"
    },
    "775": {
        "file_id": 24,
        "content": "This code trains a Diffusion Prior model using PyTorch and DALLE2-pytorch library, with functions for creating the model, training, data loading, acceleration, evaluation, text-image similarity comparison, backpropagation, logging, saving best models, measuring speed, resetting validation timers, handling errors, saving models, and initializing training with data loaders and HFA setup.",
        "type": "summary"
    },
    "776": {
        "file_id": 24,
        "content": "import click\nimport torch\nfrom torch import nn\nfrom typing import List\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nfrom torch.utils.data import DataLoader\nfrom embedding_reader import EmbeddingReader\nfrom accelerate.utils import dataclasses as accelerate_dataclasses\nfrom dalle2_pytorch.utils import Timer\nfrom dalle2_pytorch.trackers import Tracker\nfrom dalle2_pytorch import DiffusionPriorTrainer\nfrom dalle2_pytorch.dataloaders import get_reader, make_splits\nfrom dalle2_pytorch.train_configs import (\n    DiffusionPriorConfig,\n    DiffusionPriorTrainConfig,\n    TrainDiffusionPriorConfig,\n)\n# helpers\ncos = nn.CosineSimilarity(dim=1, eps=1e-6)\ndef exists(val):\n    return val is not None\ndef all_between(values: list, lower_bound, upper_bound):\n    for value in values:\n        if value < lower_bound or value > upper_bound:\n            return False\n    return True\ndef make_model(\n    prior_config: DiffusionPriorConfig,\n    train_config: DiffusionPriorTrainConfig,\n    device: str = None,\n    accelerator: Accelerator = None,",
        "type": "code",
        "location": "/train_diffusion_prior.py:1-45"
    },
    "777": {
        "file_id": 24,
        "content": "This code is for training a Diffusion Prior model using PyTorch and the DALLE2-pytorch library. It defines functions to create the model, configure the training process, and load data. The cosine similarity function is used for comparison, and there are helper functions to check if values exist and if they fall within specified bounds. The code also uses accelerate for efficient training and allows for device specification (CPU or GPU) and an optional accelerator instance for further optimization.",
        "type": "comment"
    },
    "778": {
        "file_id": 24,
        "content": "):\n    # create model from config\n    diffusion_prior = prior_config.create()\n    # instantiate the trainer\n    trainer = DiffusionPriorTrainer(\n        diffusion_prior=diffusion_prior,\n        lr=train_config.lr,\n        wd=train_config.wd,\n        max_grad_norm=train_config.max_grad_norm,\n        amp=train_config.amp,\n        use_ema=train_config.use_ema,\n        device=device,\n        accelerator=accelerator,\n        warmup_steps=train_config.warmup_steps,\n    )\n    return trainer\ndef create_tracker(\n    accelerator: Accelerator,\n    config: TrainDiffusionPriorConfig,\n    config_path: str,\n    dummy: bool = False,\n) -> Tracker:\n    tracker_config = config.tracker\n    accelerator_config = {\n        \"Distributed\": accelerator.distributed_type\n        != accelerate_dataclasses.DistributedType.NO,\n        \"DistributedType\": accelerator.distributed_type,\n        \"NumProcesses\": accelerator.num_processes,\n        \"MixedPrecision\": accelerator.mixed_precision,\n    }\n    tracker: Tracker = tracker_config.create(\n        config, accelerator_config, dummy_mode=dummy",
        "type": "code",
        "location": "/train_diffusion_prior.py:46-83"
    },
    "779": {
        "file_id": 24,
        "content": "This code defines a function `create_trainer` that takes in a `prior_config`, and creates a `DiffusionPriorTrainer` object with specified parameters. It also defines the `create_tracker` function, which creates a `Tracker` object based on the provided configuration. The functions return the created objects.",
        "type": "comment"
    },
    "780": {
        "file_id": 24,
        "content": "    )\n    tracker.save_config(config_path, config_name=\"prior_config.json\")\n    return tracker\ndef pad_gather_reduce(trainer: DiffusionPriorTrainer, x, method=\"mean\"):\n    \"\"\"\n    pad a value or tensor across all processes and gather\n    params:\n        - trainer: a trainer that carries an accelerator object\n        - x: a number or torch tensor to reduce\n        - method: \"mean\", \"sum\", \"max\", \"min\"\n    return:\n        - the average tensor after maskin out 0's\n        - None if the gather resulted in an empty tensor\n    \"\"\"\n    assert method in [\n        \"mean\",\n        \"sum\",\n        \"max\",\n        \"min\",\n    ], \"This function has limited capabilities [sum, mean, max, min]\"\n    assert type(x) is not None, \"Cannot reduce a None type object\"\n    # wait for everyone to arrive here before gathering\n    if type(x) is not torch.Tensor:\n        x = torch.tensor([x])\n    # verify that the tensor is on the proper device\n    x = x.to(trainer.device)\n    # pad across processes\n    padded_x = trainer.accelerator.pad_across_processes(x, dim=0)",
        "type": "code",
        "location": "/train_diffusion_prior.py:84-122"
    },
    "781": {
        "file_id": 24,
        "content": "This function pads a value or tensor across all processes, gathers them and reduces them to a single average. It works with tensors of type \"mean\", \"sum\", \"max\", and \"min\". If the resulting tensor is empty, it returns None. It first waits for everyone to arrive before gathering, converts the input to a tensor if it's not already, and ensures that the tensor is on the proper device.",
        "type": "comment"
    },
    "782": {
        "file_id": 24,
        "content": "    # gather across all procesess\n    gathered_x = trainer.accelerator.gather(padded_x)\n    # mask out zeros\n    masked_x = gathered_x[gathered_x != 0]\n    # if the tensor is empty, warn and return None\n    if len(masked_x) == 0:\n        click.secho(\n            f\"The call to this method resulted in an empty tensor after masking out zeros. The gathered tensor was this: {gathered_x} and the original value passed was: {x}.\",\n            fg=\"red\",\n        )\n        return None\n    if method == \"mean\":\n        return torch.mean(masked_x)\n    elif method == \"sum\":\n        return torch.sum(masked_x)\n    elif method == \"max\":\n        return torch.max(masked_x)\n    elif method == \"min\":\n        return torch.min(masked_x)\ndef save_trainer(\n    tracker: Tracker,\n    trainer: DiffusionPriorTrainer,\n    is_latest: bool,\n    is_best: bool,\n    epoch: int,\n    samples_seen: int,\n    best_validation_loss: float,\n):\n    \"\"\"\n    Logs the model with an appropriate method depending on the tracker\n    \"\"\"\n    trainer.accelerator.wait_for_everyone()",
        "type": "code",
        "location": "/train_diffusion_prior.py:124-160"
    },
    "783": {
        "file_id": 24,
        "content": "The code gathers tensor data across all processes, masks out zeros, and handles empty tensors. It then calculates the mean, sum, maximum, or minimum of the masked tensor depending on the method specified. The save_trainer function logs the model with an appropriate method based on the tracker.",
        "type": "comment"
    },
    "784": {
        "file_id": 24,
        "content": "    if trainer.accelerator.is_main_process:\n        click.secho(\n            f\"RANK:{trainer.accelerator.process_index} | Saving Model | Best={is_best} | Latest={is_latest}\",\n            fg=\"magenta\",\n        )\n    tracker.save(\n        trainer=trainer,\n        is_best=is_best,\n        is_latest=is_latest,\n        epoch=int(epoch),\n        samples_seen=int(samples_seen),\n        best_validation_loss=best_validation_loss,\n    )\ndef recall_trainer(tracker: Tracker, trainer: DiffusionPriorTrainer):\n    \"\"\"\n    Loads the model with an appropriate method depending on the tracker\n    \"\"\"\n    if trainer.accelerator.is_main_process:\n        click.secho(f\"Loading model from {type(tracker.loader).__name__}\", fg=\"yellow\")\n    state_dict = tracker.recall()\n    trainer.load(state_dict, strict=True)\n    return (\n        int(state_dict.get(\"epoch\", 0)),\n        state_dict.get(\"best_validation_loss\", 0),\n        int(state_dict.get(\"samples_seen\", 0)),\n    )\n# eval functions\ndef report_validation_loss(\n    trainer: DiffusionPriorTrainer,",
        "type": "code",
        "location": "/train_diffusion_prior.py:162-201"
    },
    "785": {
        "file_id": 24,
        "content": "This code is part of a model training process. It saves the model at certain intervals and loads it later depending on the tracker type. The save function reports whether the saved model is best or latest, and the recall_trainer function loads the model with an appropriate method based on the tracker's loader type. Additionally, there are functions for evaluating validation loss.",
        "type": "comment"
    },
    "786": {
        "file_id": 24,
        "content": "    dataloader: DataLoader,\n    text_conditioned: bool,\n    use_ema: bool,\n    tracker: Tracker,\n    split: str,\n    tracker_folder: str,\n    loss_type: str,\n):\n    \"\"\"\n    Compute the validation loss on a given subset of data.\n    \"\"\"\n    if trainer.accelerator.is_main_process:\n        click.secho(\n            f\"Measuring performance on {use_ema}-{split} split\",\n            fg=\"green\",\n            blink=True,\n        )\n    total_loss = torch.zeros(1, dtype=torch.float, device=trainer.device)\n    for image_embeddings, text_data in dataloader:\n        image_embeddings = image_embeddings.to(trainer.device)\n        text_data = text_data.to(trainer.device)\n        input_args = dict(image_embed=image_embeddings)\n        if text_conditioned:\n            input_args = dict(**input_args, text=text_data)\n        else:\n            input_args = dict(**input_args, text_embed=text_data)\n        if use_ema:\n            loss = trainer.ema_diffusion_prior(**input_args)\n        else:\n            loss = trainer(**input_args)\n        total_loss += loss",
        "type": "code",
        "location": "/train_diffusion_prior.py:202-239"
    },
    "787": {
        "file_id": 24,
        "content": "This code measures validation loss on a given data subset, using an optional EMA model and text conditioning. It iterates through a dataloader, computes losses for each batch, accumulates them in total_loss, and finally returns the average loss. The progress is echoed if the process is the main one.",
        "type": "comment"
    },
    "788": {
        "file_id": 24,
        "content": "    # compute the average loss across all processes\n    avg_loss = pad_gather_reduce(trainer, total_loss, method=\"mean\")\n    stats = {f\"{tracker_folder}/{loss_type}-loss\": avg_loss}\n    # print and log results on main process\n    tracker.log(stats, step=trainer.step.item() + 1)\n    return avg_loss\ndef report_cosine_sims(\n    trainer: DiffusionPriorTrainer,\n    dataloader: DataLoader,\n    text_conditioned: bool,\n    tracker: Tracker,\n    split: str,\n    timesteps: int,\n    tracker_folder: str,\n):\n    trainer.eval()\n    if trainer.accelerator.is_main_process:\n        click.secho(\n            f\"Measuring Cosine-Similarity on {split} split with {timesteps} timesteps\",\n            fg=\"green\",\n            blink=True,\n        )\n    for test_image_embeddings, text_data in dataloader:\n        test_image_embeddings = test_image_embeddings.to(trainer.device)\n        text_data = text_data.to(trainer.device)\n        # we are text conditioned, we produce an embedding from the tokenized text\n        if text_conditioned:\n            text_embedding, text_encodings = trainer.embed_text(text_data)",
        "type": "code",
        "location": "/train_diffusion_prior.py:241-275"
    },
    "789": {
        "file_id": 24,
        "content": "This code measures the cosine similarity on a given split with specified timesteps. It first sets the trainer to evaluation mode and then iterates through each batch of data from the dataloader. Within this loop, it moves both test image embeddings and text data to the device used by the trainer. If the model is text-conditioned, it generates an embedding from the tokenized text using the `embed_text` function provided by the trainer. This information can be useful for understanding how this code measures cosine similarity in a given context.",
        "type": "comment"
    },
    "790": {
        "file_id": 24,
        "content": "            text_cond = dict(text_embed=text_embedding, text_encodings=text_encodings)\n        else:\n            text_embedding = text_data\n            text_cond = dict(text_embed=text_embedding)\n        # make a copy of the text embeddings for shuffling\n        text_embed_shuffled = text_embedding.clone()\n        # roll the text to simulate \"unrelated\" captions\n        rolled_idx = torch.roll(torch.arange(text_embedding.shape[0]), 1)\n        text_embed_shuffled = text_embed_shuffled[rolled_idx]\n        text_embed_shuffled = text_embed_shuffled / text_embed_shuffled.norm(\n            dim=1, keepdim=True\n        )\n        if text_conditioned:\n            text_encodings_shuffled = text_encodings[rolled_idx]\n        else:\n            text_encodings_shuffled = None\n        text_cond_shuffled = dict(\n            text_embed=text_embed_shuffled, text_encodings=text_encodings_shuffled\n        )\n        # prepare the text embedding\n        text_embed = text_embedding / text_embedding.norm(dim=1, keepdim=True)\n        # prepare image embeddings",
        "type": "code",
        "location": "/train_diffusion_prior.py:276-303"
    },
    "791": {
        "file_id": 24,
        "content": "This code shuffles text embeddings and encodings to simulate \"unrelated\" captions for training the diffusion model. If text-conditioned, it also shuffles the text condition. It prepares both text and image embeddings.",
        "type": "comment"
    },
    "792": {
        "file_id": 24,
        "content": "        test_image_embeddings = test_image_embeddings / test_image_embeddings.norm(\n            dim=1, keepdim=True\n        )\n        # predict on the unshuffled text embeddings\n        predicted_image_embeddings = trainer.p_sample_loop(\n            test_image_embeddings.shape,\n            text_cond,\n            timesteps=timesteps,\n        )\n        predicted_image_embeddings = (\n            predicted_image_embeddings\n            / predicted_image_embeddings.norm(dim=1, keepdim=True)\n        )\n        # predict on the shuffled embeddings\n        predicted_unrelated_embeddings = trainer.p_sample_loop(\n            test_image_embeddings.shape,\n            text_cond_shuffled,\n            timesteps=timesteps,\n        )\n        predicted_unrelated_embeddings = (\n            predicted_unrelated_embeddings\n            / predicted_unrelated_embeddings.norm(dim=1, keepdim=True)\n        )\n        # calculate similarities\n        orig_sim = pad_gather_reduce(\n            trainer, cos(text_embed, test_image_embeddings), method=\"mean\"",
        "type": "code",
        "location": "/train_diffusion_prior.py:304-334"
    },
    "793": {
        "file_id": 24,
        "content": "This code calculates the similarity between text embeddings and image embeddings, then shuffles the text embeddings to create unrelated pairs. It uses diffusion models for prediction and normalizes the embeddings. The final step is calculating the similarities using cosine similarity and mean reduction method.",
        "type": "comment"
    },
    "794": {
        "file_id": 24,
        "content": "        )\n        pred_sim = pad_gather_reduce(\n            trainer, cos(text_embed, predicted_image_embeddings), method=\"mean\"\n        )\n        unrel_sim = pad_gather_reduce(\n            trainer, cos(text_embed, predicted_unrelated_embeddings), method=\"mean\"\n        )\n        pred_img_sim = pad_gather_reduce(\n            trainer,\n            cos(test_image_embeddings, predicted_image_embeddings),\n            method=\"mean\",\n        )\n        stats = {\n            f\"{tracker_folder}/baseline similarity [steps={timesteps}]\": orig_sim,\n            f\"{tracker_folder}/similarity with text [steps={timesteps}]\": pred_sim,\n            f\"{tracker_folder}/similarity with original image [steps={timesteps}]\": pred_img_sim,\n            f\"{tracker_folder}/similarity with unrelated caption [steps={timesteps}]\": unrel_sim,\n            f\"{tracker_folder}/difference from baseline similarity [steps={timesteps}]\": pred_sim\n            - orig_sim,\n        }\n        tracker.log(stats, step=trainer.step.item() + 1)\ndef eval_model(",
        "type": "code",
        "location": "/train_diffusion_prior.py:335-360"
    },
    "795": {
        "file_id": 24,
        "content": "This code calculates similarity scores between embeddings of text, predicted images, and original images. It then logs these scores for various steps in the training process to track progress.",
        "type": "comment"
    },
    "796": {
        "file_id": 24,
        "content": "    trainer: DiffusionPriorTrainer,\n    dataloader: DataLoader,\n    text_conditioned: bool,\n    split: str,\n    tracker: Tracker,\n    use_ema: bool,\n    report_cosine: bool,\n    report_loss: bool,\n    timesteps: List[int],\n    loss_type: str = None,\n):\n    \"\"\"\n    Run evaluation on a model and track metrics\n    returns: loss if requested\n    \"\"\"\n    trainer.eval()\n    use_ema = \"ema\" if use_ema else \"online\"\n    tracker_folder = f\"metrics/{use_ema}-{split}\"\n    # detemine if valid timesteps are passed\n    min_timesteps = trainer.accelerator.unwrap_model(\n        trainer.diffusion_prior\n    ).sample_timesteps\n    max_timesteps = trainer.accelerator.unwrap_model(\n        trainer.diffusion_prior\n    ).noise_scheduler.num_timesteps\n    assert all_between(\n        timesteps, lower_bound=min_timesteps, upper_bound=max_timesteps\n    ), f\"all timesteps values must be between {min_timesteps} and {max_timesteps}: got {timesteps}\"\n    # measure cosine metrics across various eta and timesteps\n    if report_cosine:\n        for timestep in timesteps:",
        "type": "code",
        "location": "/train_diffusion_prior.py:361-398"
    },
    "797": {
        "file_id": 24,
        "content": "This function runs evaluation on a model, tracks metrics, and returns the loss if requested. It uses DiffusionPriorTrainer and DataLoader. The use_ema parameter is used to differentiate between an Exponential Moving Average (EMA) model and an online (current) model. It checks whether the timesteps are valid for the model's noise scheduler. It also measures cosine metrics across various eta and timesteps if report_cosine is set to True.",
        "type": "comment"
    },
    "798": {
        "file_id": 24,
        "content": "            report_cosine_sims(\n                trainer,\n                dataloader=dataloader,\n                text_conditioned=text_conditioned,\n                tracker=tracker,\n                split=split,\n                timesteps=timestep,\n                tracker_folder=tracker_folder,\n            )\n    # measure loss on a seperate split of data\n    if report_loss:\n        loss = report_validation_loss(\n            trainer=trainer,\n            dataloader=dataloader,\n            text_conditioned=text_conditioned,\n            use_ema=use_ema,\n            tracker=tracker,\n            split=split,\n            tracker_folder=tracker_folder,\n            loss_type=loss_type,\n        )\n        return loss\n# training script\ndef train(\n    trainer: DiffusionPriorTrainer,\n    tracker: Tracker,\n    train_loader: DataLoader,\n    eval_loader: DataLoader,\n    test_loader: DataLoader,\n    config: DiffusionPriorTrainConfig,\n):\n    # init timers\n    save_timer = Timer()  # when to save\n    samples_timer = Timer()  # samples/sec\n    validation_profiler = Timer()  # how long is validation taking",
        "type": "code",
        "location": "/train_diffusion_prior.py:399-440"
    },
    "799": {
        "file_id": 24,
        "content": "This code measures cosine similarity on a separate dataset and reports the loss on another split of data in a training script. It also initializes timers for saving, measuring samples per second, and tracking validation time.",
        "type": "comment"
    }
}