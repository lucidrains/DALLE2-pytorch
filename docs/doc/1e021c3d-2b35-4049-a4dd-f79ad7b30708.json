{
    "summary": "The code creates a dataloader for image embedding datasets and sets up training, evaluation, and testing splits for three ranks using the provided config TRAIN_ARGS. It uses img2dataset, clip-retrieval, and embedding-dataset-reordering tools to load images and embeddings without resampling.",
    "details": [
        {
            "comment": "This code snippet describes the usage of general dataloaders for efficient data loading and training portions of the network, particularly focusing on the decoder. It supports two types of datasets: a webdataset containing .jpg and .npy files in .tar formats or an external source where .npy files correspond to .jpg filenames from the webdataset.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/README.md\":0-4",
            "content": "## Dataloaders\nIn order to make loading data simple and efficient, we include some general dataloaders that can be used to train portions of the network.\n### Decoder: Image Embedding Dataset\nWhen training the decoder (and up samplers if training together) in isolation, you will need to load images and corresponding image embeddings. This dataset can read two similar types of datasets. First, it can read a [webdataset](https://github.com/webdataset/webdataset) that contains `.jpg` and `.npy` files in the `.tar`s that contain the images and associated image embeddings respectively. Alternatively, you can also specify a source for the embeddings outside of the webdataset. In this case, the path to the embeddings should contain `.npy` files with the same shard numbers as the webdataset and there should be a correspondence between the filename of the `.jpg` and the index of the embedding in the `.npy`. So, for example, `0001.tar` from the webdataset with image `00010509.jpg` (the first 4 digit"
        },
        {
            "comment": "This code demonstrates how to create a dataloader for an image embedding dataset. It utilizes three separate tools: img2dataset, clip-retrieval, and embedding-dataset-reordering. The user must provide the appropriate URLs for the webdataset and embeddings folder in order to generate the dataloader. The code snippet also highlights the usage of create_image_embedding_dataloader function which takes in URL parameters and returns a dataloader object.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/README.md\":4-18",
            "content": "s are the shard number and the last 4 are the index) in it should be paralleled by a `img_emb_0001.npy` which contains a NumPy array with the embedding at index 509.\nGenerating a dataset of this type:\n1. Use [img2dataset](https://github.com/rom1504/img2dataset) to generate a webdataset.\n2. Use [clip-retrieval](https://github.com/rom1504/clip-retrieval) to convert the images to embeddings.\n3. Use [embedding-dataset-reordering](https://github.com/Veldrovive/embedding-dataset-reordering) to reorder the embeddings into the expected format.\nUsage:\n```python\nfrom dalle2_pytorch.dataloaders import ImageEmbeddingDataset, create_image_embedding_dataloader\n# Create a dataloader directly.\ndataloader = create_image_embedding_dataloader(\n    tar_url=\"/path/or/url/to/webdataset/{0000..9999}.tar\", # Uses bracket expanding notation. This specifies to read all tars from 0000.tar to 9999.tar\n    embeddings_url=\"path/or/url/to/embeddings/folder\",     # Included if .npy files are not in webdataset. Left out or set to None otherwise"
        },
        {
            "comment": "This code initializes a dataloader with parameters such as number of workers, batch size, shard width, and shuffle settings. It loads images and their corresponding embeddings from webdataset files. The images' shapes are printed for a single epoch. An ImageEmbeddingDataset is also created without a loader for manual configuration.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/README.md\":19-36",
            "content": "    num_workers=4,\n    batch_size=32,\n    shard_width=4,                                         # If a file in the webdataset shard 3 is named 0003039.jpg, we know the shard width is 4 and the last three digits are the index\n    shuffle_num=200,                                       # Does a shuffle of the data with a buffer size of 200\n    shuffle_shards=True,                                   # Shuffle the order the shards are read in\n    resample_shards=False,                                 # Sample shards with replacement. If true, an epoch will be infinite unless stopped manually\n)\nfor img, emb in dataloader:\n    print(img.shape)  # torch.Size([32, 3, 256, 256])\n    print(emb.shape)  # torch.Size([32, 512])\n    # Train decoder only as shown above\n# Or create a dataset without a loader so you can configure it manually\ndataset = ImageEmbeddingDataset(\n    urls=\"/path/or/url/to/webdataset/{0000..9999}.tar\",\n    embedding_folder_url=\"path/or/url/to/embeddings/folder\",\n    shard_width=4,\n    shuffle_shards=True,"
        },
        {
            "comment": "The `resample=False` argument is used to disable resampling when processing the embeddings in the Prior Embedding Dataset. This ensures that the embeddings are not recomputed and can be efficiently used for both embedding-only and text-conditioned prior training.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/README.md\":37-52",
            "content": "    resample=False\n)\n```\n### Diffusion Prior: Prior Embedding Dataset\nWhen training the prior it is much more efficient to work with pre-computed embeddings. The `PriorEmbeddingDataset` class enables you to leverage the same script (with minimal modification) for both embedding-only and text-conditioned prior training. This saves you from having to worry about a lot of the boilerplate code.\nTo utilize the `PriorEmbeddingDataset`, all you need to do is make a single call to `get_reader()` which will create `EmbeddingReader` object(s) for you. Afterwards, you can utilize `make_splits()` to cleanly create DataLoader objects from for your training run.\nIf you are training in a distributed manner, `make_splits()` accepts `rank` and `world_size` arguments to properly distribute to each process. The defaults for these values are `rank=0` and `world_size=1`, so single-process training can safely ignore these parameters.\nUsage:\n```python\nfrom dalle2_pytorch.dataloaders import get_reader, make_splits\n# grab embeddings from some specified location"
        },
        {
            "comment": "The code sets up training, evaluation, and testing splits for three different ranks (0, 1, 2) using the provided config TRAIN_ARGS. It uses the get_reader function to load image and metadata from specified URLs, and the make_splits function to divide the data into train, eval, and test sets for distributed training.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/README.md\":53-74",
            "content": "IMG_URL = \"data/img_emb/\"\nMETA_URL = \"data/meta/\"\nreader = get_reader(text_conditioned=True, img_url=IMG_URL, meta_url=META_URL)\n# some config for training\nTRAIN_ARGS = {\n    \"world_size\": 3,\n    \"text_conditioned\": True,\n    \"start\": 0,\n    \"num_data_points\": 10000,\n    \"batch_size\": 2,\n    \"train_split\": 0.5,\n    \"eval_split\": 0.25,\n    \"image_reader\": reader,\n}\n# specifying a rank will handle allocation internally\nrank0_train, rank0_eval, rank0_test = make_splits(rank=0, **TRAIN_ARGS)\nrank1_train, rank1_eval, rank1_test = make_splits(rank=1, **TRAIN_ARGS)\nrank2_train, rank2_eval, rank2_test = make_splits(rank=2, **TRAIN_ARGS)\n```"
        }
    ]
}