{
    "summary": "The code initializes trackers and loggers, provides methods for logging data, saving configurations, and metadata. It saves states and models, manages loading/saving checkpoints, and handles errors with a \"recall()\" function.",
    "details": [
        {
            "comment": "This code is from the \"trackers.py\" file in the DALLE2-pytorch library, containing a class for base logger objects that can log data with optional data storage path and verbosity control. The class initializes with specified parameters like data_path, resume, auto_resume, and verbose. It uses Pathlib for path manipulation and supports temporary data storage.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":0-34",
            "content": "import urllib.request\nimport os\nimport json\nfrom pathlib import Path\nimport shutil\nfrom itertools import zip_longest\nfrom typing import Any, Optional, List, Union\nfrom pydantic import BaseModel\nimport torch\nfrom dalle2_pytorch.dalle2_pytorch import Decoder, DiffusionPrior\nfrom dalle2_pytorch.utils import import_or_print_error\nfrom dalle2_pytorch.trainer import DecoderTrainer, DiffusionPriorTrainer\nfrom dalle2_pytorch.version import __version__\nfrom packaging import version\n# constants\nDEFAULT_DATA_PATH = './.tracker-data'\n# helper functions\ndef exists(val):\n    return val is not None\nclass BaseLogger:\n    \"\"\"\n    An abstract class representing an object that can log data.\n    Parameters:\n        data_path (str): A file path for storing temporary data.\n        verbose (bool): Whether of not to always print logs to the console.\n    \"\"\"\n    def __init__(self, data_path: str, resume: bool = False, auto_resume: bool = False, verbose: bool = False, **kwargs):\n        self.data_path = Path(data_path)\n        self.resume = resume"
        },
        {
            "comment": "The code defines a logger class with methods for logging different types of data, and an initialization method to set up the logger. The logger raises a NotImplementedError for each method, which means they need to be implemented in child classes. The get_resume_data method sets tracker attributes used to resume training if needed.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":35-61",
            "content": "        self.auto_resume = auto_resume\n        self.verbose = verbose\n    def init(self, full_config: BaseModel, extra_config: dict, **kwargs) -> None:\n        \"\"\"\n        Initializes the logger.\n        Errors if the logger is invalid.\n        full_config is the config file dict while extra_config is anything else from the script that is not defined the config file.\n        \"\"\"\n        raise NotImplementedError\n    def log(self, log, **kwargs) -> None:\n        raise NotImplementedError\n    def log_images(self, images, captions=[], image_section=\"images\", **kwargs) -> None:\n        raise NotImplementedError\n    def log_file(self, file_path, **kwargs) -> None:\n        raise NotImplementedError\n    def log_error(self, error_string, **kwargs) -> None:\n        raise NotImplementedError\n    def get_resume_data(self, **kwargs) -> dict:\n        \"\"\"\n        Sets tracker attributes that along with { \"resume\": True } will be used to resume training.\n        It is assumed that after init is called this data will be complete."
        },
        {
            "comment": "This code defines two logger classes, ConsoleLogger and WandbLogger, which inherit from the BaseLogger class. The ConsoleLogger logs to the console while the WandbLogger logs data to a Weights & Biases (WandB) run. Both loggers have methods for logging different types of data such as logs, images, files, and errors. The ConsoleLogger returns an empty dictionary if resuming is not supported, whereas the WandbLogger requires additional parameters like wandb_entity, wandb_project, wandb_run_id, and wandb_run_name for proper functioning.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":62-93",
            "content": "        If the logger does not have any resume functionality, it should return an empty dict.\n        \"\"\"\n        raise NotImplementedError\nclass ConsoleLogger(BaseLogger):\n    def init(self, full_config: BaseModel, extra_config: dict, **kwargs) -> None:\n        print(\"Logging to console\")\n    def log(self, log, **kwargs) -> None:\n        print(log)\n    def log_images(self, images, captions=[], image_section=\"images\", **kwargs) -> None:\n        pass\n    def log_file(self, file_path, **kwargs) -> None:\n        pass\n    def log_error(self, error_string, **kwargs) -> None:\n        print(error_string)\n    def get_resume_data(self, **kwargs) -> dict:\n        return {}\nclass WandbLogger(BaseLogger):\n    \"\"\"\n    Logs to a wandb run.\n    Parameters:\n        data_path (str): A file path for storing temporary data.\n        wandb_entity (str): The wandb entity to log to.\n        wandb_project (str): The wandb project to log to.\n        wandb_run_id (str): The wandb run id to resume.\n        wandb_run_name (str): The wandb run name to use."
        },
        {
            "comment": "This code is a Python class for creating and initializing a WandB logger. It requires a data path, WandB entity, and project parameters. The class also supports additional configuration options. If the WandB entity or project are not specified, an error will be raised.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":94-119",
            "content": "    \"\"\"\n    def __init__(self,\n        data_path: str,\n        wandb_entity: str,\n        wandb_project: str,\n        wandb_run_id: Optional[str] = None,\n        wandb_run_name: Optional[str] = None,\n        **kwargs\n    ):\n        super().__init__(data_path, **kwargs)\n        self.entity = wandb_entity\n        self.project = wandb_project\n        self.run_id = wandb_run_id\n        self.run_name = wandb_run_name\n    def init(self, full_config: BaseModel, extra_config: dict, **kwargs) -> None:\n        assert self.entity is not None, \"wandb_entity must be specified for wandb logger\"\n        assert self.project is not None, \"wandb_project must be specified for wandb logger\"\n        self.wandb = import_or_print_error('wandb', '`pip install wandb` to use the wandb logger')\n        os.environ[\"WANDB_SILENT\"] = \"true\"\n        # Initializes the wandb run\n        init_object = {\n            \"entity\": self.entity,\n            \"project\": self.project,\n            \"config\": {**full_config.dict(), **extra_config}\n        }"
        },
        {
            "comment": "This code initializes a Wandb tracker, allowing for easy logging of data to a specific run. If `run_id` is provided and `wandb_resume` is True, the run is resumed with a warning about renaming. The code then logs various types of data including logs, images with captions, using the Wandb API. Verbose output is also supported for logs.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":120-142",
            "content": "        if self.run_name is not None:\n            init_object['name'] = self.run_name\n        if self.resume:\n            assert self.run_id is not None, '`wandb_run_id` must be provided if `wandb_resume` is True'\n            if self.run_name is not None:\n                print(\"You are renaming a run. I hope that is what you intended.\")\n            init_object['resume'] = 'must'\n            init_object['id'] = self.run_id\n        self.wandb.init(**init_object)\n        print(f\"Logging to wandb run {self.wandb.run.path}-{self.wandb.run.name}\")\n    def log(self, log, **kwargs) -> None:\n        if self.verbose:\n            print(log)\n        self.wandb.log(log, **kwargs)\n    def log_images(self, images, captions=[], image_section=\"images\", **kwargs) -> None:\n        \"\"\"\n        Takes a tensor of images and a list of captions and logs them to wandb.\n        \"\"\"\n        wandb_images = [self.wandb.Image(image, caption=caption) for image, caption in zip_longest(images, captions)]\n        self.wandb.log({ image_section: wandb_images }, **kwargs)"
        },
        {
            "comment": "The code defines a class with three methods: `log_file`, `log_error`, and `get_resume_data`. The `log_file` method logs a file path, `log_error` logs an error string, and `get_resume_data` returns a dictionary containing essential resume information. Additionally, there is a function `create_logger` which creates a logger of type 'console' or 'wandb'. For now, custom loggers are not supported.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":144-169",
            "content": "    def log_file(self, file_path, base_path: Optional[str] = None, **kwargs) -> None:\n        if base_path is None:\n            # Then we take the basepath as the parent of the file_path\n            base_path = Path(file_path).parent\n        self.wandb.save(str(file_path), base_path = str(base_path))\n    def log_error(self, error_string, step=None, **kwargs) -> None:\n        if self.verbose:\n            print(error_string)\n        self.wandb.log({\"error\": error_string, **kwargs}, step=step)\n    def get_resume_data(self, **kwargs) -> dict:\n        # In order to resume, we need wandb_entity, wandb_project, and wandb_run_id\n        return {\n            \"entity\": self.entity,\n            \"project\": self.project,\n            \"run_id\": self.wandb.run.id\n        }\nlogger_type_map = {\n    'console': ConsoleLogger,\n    'wandb': WandbLogger,\n}\ndef create_logger(logger_type: str, data_path: str, **kwargs) -> BaseLogger:\n    if logger_type == 'custom':\n        raise NotImplementedError('Custom loggers are not supported yet. Please use a different logger type.')"
        },
        {
            "comment": "Function tries to create an instance of a logger class based on the given type, otherwise it raises a ValueError. BaseLoader is an abstract class that can be used to load model checkpoints with data_path and optionally other parameters. UrlLoader extends BaseLoader by allowing loading files from URLs instead of local file paths.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":170-199",
            "content": "    try:\n        logger_class = logger_type_map[logger_type]\n    except KeyError:\n        raise ValueError(f'Unknown logger type: {logger_type}. Must be one of {list(logger_type_map.keys())}')\n    return logger_class(data_path, **kwargs)\nclass BaseLoader:\n    \"\"\"\n    An abstract class representing an object that can load a model checkpoint.\n    Parameters:\n        data_path (str): A file path for storing temporary data.\n    \"\"\"\n    def __init__(self, data_path: str, only_auto_resume: bool = False, **kwargs):\n        self.data_path = Path(data_path)\n        self.only_auto_resume = only_auto_resume\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        raise NotImplementedError\n    def recall() -> dict:\n        raise NotImplementedError\nclass UrlLoader(BaseLoader):\n    \"\"\"\n    A loader that downloads the file from a url and loads it\n    Parameters:\n        data_path (str): A file path for storing temporary data.\n        url (str): The url to download the file from.\n    \"\"\"\n    def __init__(self, data_path: str, url: str, **kwargs):"
        },
        {
            "comment": "The code defines a base class, \"BaseLoader\", which is responsible for loading files from a given data path. It initializes the class by setting the URL and has an init method to check if the file exists. The \"recall\" method downloads the file and loads it into memory. Additionally, there is a subclass called \"LocalLoader\" that loads files from local paths, checking if the file exists before loading it.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":200-228",
            "content": "        super().__init__(data_path, **kwargs)\n        self.url = url\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        # Makes sure the file exists to be downloaded\n        pass  # TODO: Actually implement that\n    def recall(self) -> dict:\n        # Download the file\n        save_path = self.data_path / 'loaded_checkpoint.pth'\n        urllib.request.urlretrieve(self.url, str(save_path))\n        # Load the file\n        return torch.load(str(save_path), map_location='cpu')\nclass LocalLoader(BaseLoader):\n    \"\"\"\n    A loader that loads a file from a local path\n    Parameters:\n        data_path (str): A file path for storing temporary data.\n        file_path (str): The path to the file to load.\n    \"\"\"\n    def __init__(self, data_path: str, file_path: str, **kwargs):\n        super().__init__(data_path, **kwargs)\n        self.file_path = Path(file_path)\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        # Makes sure the file exists to be loaded\n        if not self.file_path.exists() and not self.only_auto_resume:"
        },
        {
            "comment": "This code defines a class `WandbLoader` that loads a model from an existing W&B (Weights & Biases) run. It requires a data path, a file path within the W&B run, and optionally a W&B run path. The `__init__` method initializes the object, the `init` method ensures the file can be downloaded, and the `recall` method loads the model using `torch.load`. If a W&B run is available but the run path is not specified, it sets the run path to the current run's path. The code also imports the 'wandb' library if it is missing.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":229-250",
            "content": "            raise FileNotFoundError(f'Model not found at {self.file_path}')\n    def recall(self) -> dict:\n        # Load the file\n        return torch.load(str(self.file_path), map_location='cpu')\nclass WandbLoader(BaseLoader):\n    \"\"\"\n    A loader that loads a model from an existing wandb run\n    \"\"\"\n    def __init__(self, data_path: str, wandb_file_path: str, wandb_run_path: Optional[str] = None, **kwargs):\n        super().__init__(data_path, **kwargs)\n        self.run_path = wandb_run_path\n        self.file_path = wandb_file_path\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        self.wandb = import_or_print_error('wandb', '`pip install wandb` to use the wandb recall function')\n        # Make sure the file can be downloaded\n        if self.wandb.run is not None and self.run_path is None:\n            self.run_path = self.wandb.run.path\n            assert self.run_path is not None, 'wandb run was not found to load from. If not using the wandb logger must specify the `wandb_run_path`.'\n        assert self.run_path is not None, '`wandb_run_path` must be provided for the wandb loader'"
        },
        {
            "comment": "This code defines a `BaseSaver` class with an optional parameter for saving the latest data to a specified location. It also includes a function `create_loader()` that creates different types of loaders (url, local, wandb) based on the provided loader type and data path. The WandbLoader is used to restore data from a specified file path using Weights & Biases environment.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":251-277",
            "content": "        assert self.file_path is not None, '`wandb_file_path` must be provided for the wandb loader'\n        os.environ[\"WANDB_SILENT\"] = \"true\"\n        pass  # TODO: Actually implement that\n    def recall(self) -> dict:\n        file_reference = self.wandb.restore(self.file_path, run_path=self.run_path)\n        return torch.load(file_reference.name, map_location='cpu')\nloader_type_map = {\n    'url': UrlLoader,\n    'local': LocalLoader,\n    'wandb': WandbLoader,\n}\ndef create_loader(loader_type: str, data_path: str, **kwargs) -> BaseLoader:\n    if loader_type == 'custom':\n        raise NotImplementedError('Custom loaders are not supported yet. Please use a different loader type.')\n    try:\n        loader_class = loader_type_map[loader_type]\n    except KeyError:\n        raise ValueError(f'Unknown loader type: {loader_type}. Must be one of {list(loader_type_map.keys())}')\n    return loader_class(data_path, **kwargs)\nclass BaseSaver:\n    def __init__(self,\n        data_path: str,\n        save_latest_to: Optional[Union[str, bool]] = None,"
        },
        {
            "comment": "This code defines a tracker class that handles saving of data to specified locations. It allows saving the latest, best, and meta information, with options for file type and paths. The `save_file` method is used to save files with optional flags for best and latest status. An assertion ensures that the save type is either 'checkpoint' or 'model'. A final assertion requires at least one saving option to be specified.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":278-298",
            "content": "        save_best_to: Optional[Union[str, bool]] = None,\n        save_meta_to: Optional[str] = None,\n        save_type: str = 'checkpoint',\n        **kwargs\n    ):\n        self.data_path = Path(data_path)\n        self.save_latest_to = save_latest_to\n        self.saving_latest = save_latest_to is not None and save_latest_to is not False\n        self.save_best_to = save_best_to\n        self.saving_best = save_best_to is not None and save_best_to is not False\n        self.save_meta_to = save_meta_to\n        self.saving_meta = save_meta_to is not None\n        self.save_type = save_type\n        assert save_type in ['checkpoint', 'model'], '`save_type` must be one of `checkpoint` or `model`'\n        assert self.saving_latest or self.saving_best or self.saving_meta, 'At least one saving option must be specified'\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        raise NotImplementedError\n    def save_file(self, local_path: Path, save_path: str, is_best=False, is_latest=False, **kwargs) -> None:\n        \"\"\""
        },
        {
            "comment": "This code defines two classes, LocalSaver and WandbSaver, which inherit from BaseSaver. Both classes are responsible for saving files in different locations. The LocalSaver saves files locally to a specified data_path, ensuring the directory exists beforehand. The WandbSaver is optional and requires a wandb_run_path parameter.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":299-327",
            "content": "        Save a general file under save_meta_to\n        \"\"\"\n        raise NotImplementedError\nclass LocalSaver(BaseSaver):\n    def __init__(self,\n        data_path: str,\n        **kwargs\n    ):\n        super().__init__(data_path, **kwargs)\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        # Makes sure the directory exists to be saved to\n        print(f\"Saving {self.save_type} locally\")\n        if not self.data_path.exists():\n            self.data_path.mkdir(parents=True)\n    def save_file(self, local_path: str, save_path: str, **kwargs) -> None:\n        # Copy the file to save_path\n        save_path_file_name = Path(save_path).name\n        # Make sure parent directory exists\n        save_path_parent = Path(save_path).parent\n        if not save_path_parent.exists():\n            save_path_parent.mkdir(parents=True)\n        print(f\"Saving {save_path_file_name} {self.save_type} to local path {save_path}\")\n        shutil.copy(local_path, save_path)\nclass WandbSaver(BaseSaver):\n    def __init__(self, data_path: str, wandb_run_path: Optional[str] = None, **kwargs):"
        },
        {
            "comment": "This code initializes a W&B run based on the `wandb_run_path` provided. It imports the W&B library, sets up the environment for uploading to W&B runs, and checks if the user has access to save files in the specified W&B run path.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":328-345",
            "content": "        super().__init__(data_path, **kwargs)\n        self.run_path = wandb_run_path\n    def init(self, logger: BaseLogger, **kwargs) -> None:\n        self.wandb = import_or_print_error('wandb', '`pip install wandb` to use the wandb logger')\n        os.environ[\"WANDB_SILENT\"] = \"true\"\n        # Makes sure that the user can upload tot his run\n        if self.run_path is not None:\n            entity, project, run_id = self.run_path.split(\"/\")\n            self.run = self.wandb.init(entity=entity, project=project, id=run_id)\n        else:\n            assert self.wandb.run is not None, 'You must be using the wandb logger if you are saving to wandb and have not set `wandb_run_path`'\n            self.run = self.wandb.run\n        # TODO: Now actually check if upload is possible\n        print(f\"Saving to wandb run {self.run.path}-{self.run.name}\")\n    def save_file(self, local_path: Path, save_path: str, **kwargs) -> None:\n        # In order to log something in the correct place in wandb, we need to have the same file structure here"
        },
        {
            "comment": "This code defines a `HuggingfaceSaver` class that saves files to a Hugging Face repository. It initializes the instance with a data path, Hugging Face repo, and optional token path. The `init` method checks if the user is logged in to the Hugging Face hub and then saves the file specified by `save_path` using `self.hub.upload`.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":346-364",
            "content": "        save_path_file_name = Path(save_path).name\n        print(f\"Saving {save_path_file_name} {self.save_type} to wandb run {self.run.path}-{self.run.name}\")\n        save_path = Path(self.data_path) / save_path\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copy(local_path, save_path)\n        self.run.save(str(save_path), base_path = str(self.data_path), policy='now')\nclass HuggingfaceSaver(BaseSaver):\n    def __init__(self, data_path: str, huggingface_repo: str, token_path: Optional[str] = None, **kwargs):\n        super().__init__(data_path, **kwargs)\n        self.huggingface_repo = huggingface_repo\n        self.token_path = token_path\n    def init(self, logger: BaseLogger, **kwargs):\n        # Makes sure this user can upload to the repo\n        self.hub = import_or_print_error('huggingface_hub', '`pip install huggingface_hub` to use the huggingface saver')\n        try:\n            identity = self.hub.whoami()  # Errors if not logged in\n            # Then we are logged in"
        },
        {
            "comment": "This code handles saving a file to the HuggingFace repo. If not logged in, it checks for a token path and uses it if available, or throws an exception. It then prints the saving path, logs in with the token (if provided), and finally uploads the file to the specified HuggingFace repo.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":365-381",
            "content": "        except:\n            # We are not logged in. Use the token_path to set the token.\n            if not os.path.exists(self.token_path):\n                raise Exception(\"Not logged in to huggingface and no token_path specified. Please login with `huggingface-cli login` or if that does not work set the token_path.\")\n            with open(self.token_path, \"r\") as f:\n                token = f.read().strip()\n            self.hub.HfApi.set_access_token(token)\n            identity = self.hub.whoami()\n        print(f\"Saving to huggingface repo {self.huggingface_repo}\")\n    def save_file(self, local_path: Path, save_path: str, **kwargs) -> None:\n        # Saving to huggingface is easy, we just need to upload the file with the correct name\n        save_path_file_name = Path(save_path).name\n        print(f\"Saving {save_path_file_name} {self.save_type} to huggingface repo {self.huggingface_repo}\")\n        self.hub.upload_file(\n            path_or_fileobj=str(local_path),\n            path_in_repo=str(save_path),"
        },
        {
            "comment": "Function create_saver takes a saver type and data path, returns a BaseSaver object. It supports 'local', 'wandb', and 'huggingface' saver types. If the saver type is 'custom', it raises an error since custom savers aren't supported yet. Tracker initializes with optional data_path, overwrite_data_path (to overwrite existing path), and dummy_mode (if running in simulation mode). If not in dummy mode, asserts that the data path doesn't exist unless overwrite_data_path is True.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":382-406",
            "content": "            repo_id=self.huggingface_repo\n        )\nsaver_type_map = {\n    'local': LocalSaver,\n    'wandb': WandbSaver,\n    'huggingface': HuggingfaceSaver\n}\ndef create_saver(saver_type: str, data_path: str, **kwargs) -> BaseSaver:\n    if saver_type == 'custom':\n        raise NotImplementedError('Custom savers are not supported yet. Please use a different saver type.')\n    try:\n        saver_class = saver_type_map[saver_type]\n    except KeyError:\n        raise ValueError(f'Unknown saver type: {saver_type}. Must be one of {list(saver_type_map.keys())}')\n    return saver_class(data_path, **kwargs)\nclass Tracker:\n    def __init__(self, data_path: Optional[str] = DEFAULT_DATA_PATH, overwrite_data_path: bool = False, dummy_mode: bool = False):\n        self.data_path = Path(data_path)\n        if not dummy_mode:\n            if not overwrite_data_path:\n                assert not self.data_path.exists(), f'Data path {self.data_path} already exists. Set overwrite_data_path to True to overwrite.'\n                if not self.data_path.exists():"
        },
        {
            "comment": "This code initializes a tracker object, handling the data path creation, base logger and loader setup, saving list initialization, and dummy mode. It also includes a method to load auto-resume configuration if it exists, printing warnings for first run or removing the file if auto-resume is not enabled.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":407-426",
            "content": "                    self.data_path.mkdir(parents=True)\n        self.logger: BaseLogger = None\n        self.loader: Optional[BaseLoader] = None\n        self.savers: List[BaseSaver]= []\n        self.dummy_mode = dummy_mode\n    def _load_auto_resume(self) -> bool:\n        # If the file does not exist, we return False. If autoresume is enabled we print a warning so that the user can know that this is the first run.\n        if not self.auto_resume_path.exists():\n            if self.logger.auto_resume:\n                print(\"Auto_resume is enabled but no auto_resume.json file exists. Assuming this is the first run.\")\n            return False\n        # Now we know that the autoresume file exists, but if we are not auto resuming we should remove it so that we don't accidentally load it next time\n        if not self.logger.auto_resume:\n            print(f'Removing auto_resume.json because auto_resume is not enabled in the config')\n            self.auto_resume_path.unlink()\n            return False\n        # Otherwise we read the json into a dictionary will will override parts of logger.__dict__"
        },
        {
            "comment": "This code reads a previously saved state from the \"auto_resume_path\" and checks if the logger type matches the current logger. If they don't match, it raises an exception with instructions on how to proceed. Otherwise, it updates the logger with the auto-resume data and returns True.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":427-441",
            "content": "        with open(self.auto_resume_path, 'r') as f:\n            auto_resume_dict = json.load(f)\n        # Check if the logger is of the same type as the autoresume save\n        if auto_resume_dict[\"logger_type\"] != self.logger.__class__.__name__:\n            raise Exception(f'The logger type in the auto_resume file is {auto_resume_dict[\"logger_type\"]} but the current logger is {self.logger.__class__.__name__}. Either use the original logger type, set `auto_resume` to `False`, or delete your existing tracker-data folder.')\n        # Then we are ready to override the logger with the autoresume save\n        self.logger.__dict__[\"resume\"] = True\n        print(f\"Updating {self.logger.__dict__} with {auto_resume_dict}\")\n        self.logger.__dict__.update(auto_resume_dict)\n        return True\n    def _save_auto_resume(self):\n        # Gets the autoresume dict from the logger and adds \"logger_type\" to it then saves it to the auto_resume file\n        auto_resume_dict = self.logger.get_resume_data()\n        auto_resume_dict['logger_type'] = self.logger.__class__.__name__"
        },
        {
            "comment": "This code is initializing a tracker object. It sets the auto_resume path, checks for resuming the run and prints a warning if it was automatically resumed. The save_metadata dictionary is created with version information and some keys are blacklisted from being saved as metadata to avoid errors during saving. The logger must be set before calling init method.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":442-458",
            "content": "        with open(self.auto_resume_path, 'w') as f:\n            json.dump(auto_resume_dict, f)\n    def init(self, full_config: BaseModel, extra_config: dict):\n        self.auto_resume_path = self.data_path / 'auto_resume.json'\n        # Check for resuming the run\n        self.did_auto_resume = self._load_auto_resume()\n        if self.did_auto_resume:\n            print(f'\\n\\nWARNING: RUN HAS BEEN AUTO-RESUMED WITH THE LOGGER TYPE {self.logger.__class__.__name__}.\\nIf this was not your intention, stop this run and set `auto_resume` to `False` in the config.\\n\\n')\n            print(f\"New logger config: {self.logger.__dict__}\")\n        self.save_metadata = dict(\n            version = version.parse(__version__)\n        )  # Data that will be saved alongside the checkpoint or model\n        self.blacklisted_checkpoint_metadata_keys = ['scaler', 'optimizer', 'model', 'version', 'step', 'steps']  # These keys would cause us to error if we try to save them as metadata\n        assert self.logger is not None, '`logger` must be set before `init` is called'"
        },
        {
            "comment": "This code initializes trackers by first checking if in dummy mode, then initializing loaders and savers. The logger is initialized only if the `savers` list has items, and if `auto_resume` is enabled, it saves an autoresume file. The `add_logger`, `add_loader`, `add_saver`, and `log` methods are provided to interact with trackers' components.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":459-488",
            "content": "        if self.dummy_mode:\n            # The only thing we need is a loader\n            if self.loader is not None:\n                self.loader.init(self.logger)\n            return\n        assert len(self.savers) > 0, '`savers` must be set before `init` is called'\n        self.logger.init(full_config, extra_config)\n        if self.loader is not None:\n            self.loader.init(self.logger)\n        for saver in self.savers:\n            saver.init(self.logger)\n        if self.logger.auto_resume:\n            # Then we need to save the autoresume file. It is assumed after logger.init is called that the logger is ready to be saved.\n            self._save_auto_resume()\n    def add_logger(self, logger: BaseLogger):\n        self.logger = logger\n    def add_loader(self, loader: BaseLoader):\n        self.loader = loader\n    def add_saver(self, saver: BaseSaver):\n        self.savers.append(saver)\n    def log(self, *args, **kwargs):\n        if self.dummy_mode:\n            return\n        self.logger.log(*args, **kwargs)"
        },
        {
            "comment": "This code is from the DALLE2-pytorch library and it contains several methods for logging images, files, saving configurations, and adding save metadata. The dummy_mode check prevents unnecessary actions when in a test mode. The save_config method copies the current config file to the root folder of the data_path and saves it remotely if specified by the saver. The add_save_metadata method adds new metadata that will be saved along with the model or decoder.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":490-516",
            "content": "    def log_images(self, *args, **kwargs):\n        if self.dummy_mode:\n            return\n        self.logger.log_images(*args, **kwargs)\n    def log_file(self, *args, **kwargs):\n        if self.dummy_mode:\n            return\n        self.logger.log_file(*args, **kwargs)\n    def save_config(self, current_config_path: str, config_name = 'config.json'):\n        if self.dummy_mode:\n            return\n        # Save the config under config_name in the root folder of data_path\n        shutil.copy(current_config_path, self.data_path / config_name)\n        for saver in self.savers:\n            if saver.saving_meta:\n                remote_path = Path(saver.save_meta_to) / config_name\n                saver.save_file(current_config_path, str(remote_path))\n    def add_save_metadata(self, state_dict_key: str, metadata: Any):\n        \"\"\"\n        Adds a new piece of metadata that will be saved along with the model or decoder.\n        \"\"\"\n        self.save_metadata[state_dict_key] = metadata\n    def _save_state_dict(self,"
        },
        {
            "comment": "This function saves the trainer's state dict, depending on the 'save_type' parameter. If 'checkpoint', it saves the entire trainer state without blacklisted metadata keys. If 'model', it saves only the model state if the trainer is a DiffusionPriorTrainer.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":516-530",
            "content": " trainer: Union[DiffusionPriorTrainer, DecoderTrainer], save_type: str, file_path: str, **kwargs) -> Path:\n        \"\"\"\n        Gets the state dict to be saved and writes it to file_path.\n        If save_type is 'checkpoint', we save the entire trainer state dict.\n        If save_type is 'model', we save only the model state dict.\n        \"\"\"\n        assert save_type in ['checkpoint', 'model']\n        if save_type == 'checkpoint':\n            # Create a metadata dict without the blacklisted keys so we do not error when we create the state dict\n            metadata = {k: v for k, v in self.save_metadata.items() if k not in self.blacklisted_checkpoint_metadata_keys}\n            trainer.save(file_path, overwrite=True, **kwargs, **metadata)\n        elif save_type == 'model':\n            if isinstance(trainer, DiffusionPriorTrainer):\n                prior = trainer.ema_diffusion_prior.ema_model if trainer.use_ema else trainer.diffusion_prior\n                prior: DiffusionPrior = trainer.accelerator.unwrap_model(prior)"
        },
        {
            "comment": "This code checks the type of trainer and removes CLIP from the model if it is part of it. It then saves the state dictionary for the model, and optionally swaps EMA unets in or out depending on the use_ema flag. Finally, it restores the original CLIP state.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":531-550",
            "content": "                # Remove CLIP if it is part of the model\n                original_clip = prior.clip\n                prior.clip = None\n                model_state_dict = prior.state_dict()\n                prior.clip = original_clip\n            elif isinstance(trainer, DecoderTrainer):\n                decoder: Decoder = trainer.accelerator.unwrap_model(trainer.decoder)\n                # Remove CLIP if it is part of the model\n                original_clip = decoder.clip\n                decoder.clip = None\n                if trainer.use_ema:\n                    trainable_unets = decoder.unets\n                    decoder.unets = trainer.unets  # Swap EMA unets in\n                    model_state_dict = decoder.state_dict()\n                    decoder.unets = trainable_unets  # Swap back\n                else:\n                    model_state_dict = decoder.state_dict()\n                decoder.clip = original_clip\n            else:\n                raise NotImplementedError('Saving this type of model with EMA mode enabled is not yet implemented. Actually, how did you get here?')"
        },
        {
            "comment": "This code saves the model and checkpoint to specified file paths. If not in dummy mode, it checks if the 'is_best' or 'is_latest' flag is set before proceeding with saving the state dictionary for 'checkpoint' and 'model'. It then prints a message confirming the saved cached models. Lastly, it calls save methods on savers, considering the 'saving_latest' flag and appropriate file paths.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":551-574",
            "content": "            state_dict = {\n                **self.save_metadata,\n                'model': model_state_dict\n            }\n            torch.save(state_dict, file_path)\n        return Path(file_path)\n    def save(self, trainer, is_best: bool, is_latest: bool, **kwargs):\n        if self.dummy_mode:\n            return\n        if not is_best and not is_latest:\n            # Nothing to do\n            return\n        # Save the checkpoint and model to data_path\n        checkpoint_path = self.data_path / 'checkpoint.pth'\n        self._save_state_dict(trainer, 'checkpoint', checkpoint_path, **kwargs)\n        model_path = self.data_path / 'model.pth'\n        self._save_state_dict(trainer, 'model', model_path, **kwargs)\n        print(\"Saved cached models\")\n        # Call the save methods on the savers\n        for saver in self.savers:\n            local_path = checkpoint_path if saver.save_type == 'checkpoint' else model_path\n            if saver.saving_latest and is_latest:\n                latest_checkpoint_path = saver.save_latest_to.format(**kwargs)"
        },
        {
            "comment": "This code appears to be part of a class that manages loading and saving checkpoints for a model. It has a property called \"can_recall\" which determines if a recall (loading a previously saved checkpoint) can be performed based on whether the loader is not None and certain conditions about the loader's properties. If a recall is possible, the \"recall()\" function is called to perform the actual recall. Any errors that occur during saving are logged and printed.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":575-597",
            "content": "                try:\n                    saver.save_file(local_path, latest_checkpoint_path, is_latest=True, **kwargs)\n                except Exception as e:\n                    self.logger.log_error(f'Error saving checkpoint: {e}', **kwargs)\n                    print(f'Error saving checkpoint: {e}')\n            if saver.saving_best and is_best:\n                best_checkpoint_path = saver.save_best_to.format(**kwargs)\n                try:\n                    saver.save_file(local_path, best_checkpoint_path, is_best=True, **kwargs)\n                except Exception as e:\n                    self.logger.log_error(f'Error saving checkpoint: {e}', **kwargs)\n                    print(f'Error saving checkpoint: {e}')\n    @property\n    def can_recall(self):\n        # Defines whether a recall can be performed.\n        return self.loader is not None and (not self.loader.only_auto_resume or self.did_auto_resume)\n    def recall(self):\n        if self.can_recall:\n            return self.loader.recall()\n        else:\n"
        },
        {
            "comment": "Raises an error when no loader is set and auto-resume was not performed.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/trackers.py\":597-597",
            "content": "            raise ValueError('Tried to recall, but no loader was set or auto-resume was not performed.')"
        }
    ]
}