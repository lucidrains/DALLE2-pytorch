{
    "summary": "The code defines functions for retrieving embeddings, combining image and text embeddings, creating image embedding datasets, and handling exceptions in webdataset tar files. It also includes support for preprocessing, resampling, shuffling, package checks, and dataloaders.",
    "details": [
        {
            "comment": "This code defines three functions: `get_shard`, `get_example_file`, and `embedding_inserter`. The first function extracts the shard number from a filename. The second function returns an example file given a file system and a file format. Lastly, the third function inserts embeddings into a dataset, given samples, embedding URL, index width, sample key, and a handler to handle exceptions.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":0-28",
            "content": "import os\nimport webdataset as wds\nimport torch\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport fsspec\nimport shutil\ndef get_shard(filename):\n    \"\"\"\n    Filenames with shards in them have a consistent structure that we can take advantage of\n    Standard structure: path/to/file/prefix_string_00001.ext\n    \"\"\"\n    try:\n        return filename.split(\"_\")[-1].split(\".\")[0]\n    except ValueError:\n        raise RuntimeError(f\"Could not find shard for filename {filename}\")\ndef get_example_file(fs, path, file_format):\n    \"\"\"\n    Given a file system and a file extension, return the example file\n    \"\"\"\n    return fs.glob(os.path.join(path, f\"*.{file_format}\"))[0]\ndef embedding_inserter(samples, embeddings_url, index_width, sample_key='npy', handler=wds.handlers.reraise_exception):\n    \"\"\"Given a datum of {\"__key__\": str, \"__url__\": str, ...} adds the cooresponding embedding and yields\"\"\"\n    previous_tar_url = None\n    current_embeddings = None\n    # Get a reference to an abstract file system where the embeddings are stored"
        },
        {
            "comment": "This code segment retrieves and loads embeddings from a webdataset tar file using the given URL. It identifies the correct npy file containing the embeddings by extracting the shard number from the URL, then opens and loads the data into a torch tensor.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":29-48",
            "content": "    embeddings_fs, embeddings_path = fsspec.core.url_to_fs(embeddings_url)\n    example_embedding_file = get_example_file(embeddings_fs, embeddings_path, \"npy\")\n    example_embedding_shard = get_shard(example_embedding_file)\n    emb_shard_width = len(example_embedding_shard)\n    # Easier to get the basename without the shard once than search through for the correct file every time\n    embedding_file_basename = '_'.join(example_embedding_file.split(\"_\")[:-1]) + \"_\"\n    def load_corresponding_embeds(tar_url):\n      \"\"\"Finds and reads the npy files that contains embeddings for the given webdataset tar\"\"\"\n      shard = int(tar_url.split(\"/\")[-1].split(\".\")[0])\n      embedding_url = embedding_file_basename + str(shard).zfill(emb_shard_width) + '.npy'\n      with embeddings_fs.open(embedding_url) as f:\n        data = np.load(f)\n      return torch.from_numpy(data)\n    for sample in samples:\n        try:\n            tar_url = sample[\"__url__\"]\n            key = sample[\"__key__\"]\n            if tar_url != previous_tar_url:"
        },
        {
            "comment": "The code checks if a tar file changed and loads corresponding embeddings. If the sample has no embedding, it raises an error. The insert_embedding variable is assigned a pipeline filter with the embedding inserter function.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":49-66",
            "content": "                # If the tar changed, we need to download new embeddings\n                # This means if we shuffle before inserting it will load many more files than we expect and be very inefficient.\n                previous_tar_url = tar_url\n                current_embeddings = load_corresponding_embeds(tar_url)\n            embedding_index = int(key[-index_width:])\n            embedding = current_embeddings[embedding_index]\n            # We need to check if this sample is nonzero. If it is, this embedding is not valid and we should continue to the next loop\n            if torch.count_nonzero(embedding) == 0:\n                raise RuntimeError(f\"Webdataset had a sample, but no embedding was found. ImgShard: {key[:-index_width]} - Index: {key[-index_width:]}\")\n            sample[sample_key] = embedding\n            yield sample\n        except Exception as exn:  # From wds implementation\n            if handler(exn):\n                continue\n            else:\n                break\ninsert_embedding = wds.filters.pipelinefilter(embedding_inserter)"
        },
        {
            "comment": "This function checks if there are corresponding embeddings for the given tarfiles. It first retrieves a set of embedding shards from the embeddings_url, then iterates through the tarfiles. If a tarfile's shard is in the set of embedding shards, it yields the tarfile. Otherwise, it will continue to iterate until it finds a matching shard. Exceptions are handled using the provided handler function.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":68-83",
            "content": "def unassociated_shard_skipper(tarfiles, embeddings_url, handler=wds.handlers.reraise_exception):\n    \"\"\"Finds if the is a corresponding embedding for the tarfile at { url: [URL] }\"\"\"\n    embeddings_fs, embeddings_path = fsspec.core.url_to_fs(embeddings_url)\n    embedding_files = embeddings_fs.ls(embeddings_path)\n    get_embedding_shard = lambda embedding_file: int(embedding_file.split(\"_\")[-1].split(\".\")[0])\n    embedding_shards = set([get_embedding_shard(filename) for filename in embedding_files])  # Sets have O(1) check for member\n    get_tar_shard = lambda tar_file: int(tar_file.split(\"/\")[-1].split(\".\")[0])\n    for tarfile in tarfiles:\n        try:\n            webdataset_shard = get_tar_shard(tarfile[\"url\"])\n            # If this shard has an associated embeddings file, we pass it through. Otherwise we iterate until we do have one\n            if webdataset_shard in embedding_shards:\n                yield tarfile\n        except Exception as exn:  # From wds implementation\n            if handler(exn):"
        },
        {
            "comment": "The code defines two functions: `join_embeddings()` and `verify_keys()`. The first function combines the `img_emb` and `text_emb` keys into a single \"emb\" key in each sample, only including existing embeddings. The second function ensures that both image and embedding are present in each sample. If not, it either continues or breaks depending on the exception handler.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":84-110",
            "content": "                continue\n            else:\n                break\nskip_unassociated_shards = wds.filters.pipelinefilter(unassociated_shard_skipper)\ndef join_embeddings(samples, handler=wds.handlers.reraise_exception):\n    \"\"\"\n    Takes the img_emb and text_emb keys and turns them into one key \"emb\": { \"text\": text_emb, \"img\": img_emb }\n    either or both of text_emb and img_emb may not be in the sample so we only add the ones that exist\n    \"\"\"\n    for sample in samples:\n        try:\n            sample['emb'] = {}\n            if 'text_emb' in sample:\n                sample['emb']['text'] = sample['text_emb']\n            if 'img_emb' in sample:\n                sample['emb']['img'] = sample['img_emb']\n            yield sample\n        except Exception as exn:  # From wds implementation\n            if handler(exn):\n                continue\n            else:\n                break\ndef verify_keys(samples, required_keys, handler=wds.handlers.reraise_exception):\n    \"\"\"\n    Requires that both the image and embedding are present in the sample"
        },
        {
            "comment": "This code checks if required keys are present in each sample, asserts if missing and yields the sample. It uses a key_verifier filter and a fluid interface for DataPipeline to return image embedding pairs. Embeddings can be read from webdataset or inserted from an alternate source based on embedding_folder_url.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":111-135",
            "content": "    This is important to do as a user may forget they do not have embeddings in their webdataset and neglect to add them using the embedding_folder_url parameter.\n    \"\"\"\n    for sample in samples:\n        try:\n            for key in required_keys:\n                assert key in sample, f\"Sample {sample['__key__']} missing {key}. Has keys {sample.keys()}\"\n            yield sample\n        except Exception as exn:  # From wds implementation\n            if handler(exn):\n                continue\n            else:\n                break\nkey_verifier = wds.filters.pipelinefilter(verify_keys)\nclass ImageEmbeddingDataset(wds.DataPipeline, wds.compat.FluidInterface):\n    \"\"\"\n    A fluid interface wrapper for DataPipline that returns image embedding pairs\n    Reads embeddings as npy files from the webdataset if they exist. If embedding_folder_url is set, they will be inserted in from the alternate source.\n    \"\"\"\n    def __init__(\n            self,\n            urls,\n            img_embedding_folder_url=None,\n            text_embedding_folder_url=None,"
        },
        {
            "comment": "The code defines a function to load data from webdatasets and embeddings for a model. It takes URLs as input, where each URL points to tar files of the webdataset. If embeddings are not included in the dataset, an embedding_folder_URL is required. The index width specifies the number of digits in the index, used to align image and embedding indices. The handler handles exceptions, while resample can be set for resampling data. The shuffle_shards flag determines whether to shuffle shards during loading.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":136-150",
            "content": "            index_width=None,\n            img_preproc=None,\n            extra_keys=[],\n            handler=wds.handlers.reraise_exception,\n            resample=False,\n            shuffle_shards=True\n    ):\n        \"\"\"\n        Modeled directly off of the WebDataset constructor\n        :param urls: A url pointing to the tar files of the webdataset formatted as /path/to/webdataset/{0000..9999}.tar\n        :param embedding_folder_url: Required if webdataset does not contain embeddings. A url pointing to the npy files of the embeddings. Should have the same number of shards as the webdataset.\n            Webdataset image keys should align with the index of the embedding. This means missing image indices must have a corresponding embedding of all zeros.\n        :param index_width: The number of digits in the index. This is used to align the embedding index with the image index.\n            For example, if a file in the webdataset shard 3 is named 0003039.jpg, we know the shard is 4 digits and the last 3 digits are the index_width."
        },
        {
            "comment": "This function is a webdataset handler that takes parameters for img_preproc, resample, and shuffle_shards. It initializes the keys for data loading and maps them to their respective indices. If img_embedding_folder_url or text_embedding_folder_url is not None, \"img_emb\" and \"text_emb\" will be added as keys. The function also checks if s3fs and s3cmd are installed, and handles data piping.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":151-168",
            "content": "        :param img_preproc: This function is run on the img before it is batched and returned. Useful for data augmentation or converting to torch tensor.\n        :param handler: A webdataset handler.\n        :param resample: If true, resample webdataset shards with replacement. You need to set your own epoch size if this is true since it will resample infinitely.\n        :param shuffle_shards: If true, shuffle the shards before resampling. This cannot be true if resample is true.\n        \"\"\"\n        super().__init__()\n        keys = [\"jpg\", \"emb\"] + extra_keys\n        # if img_embedding_folder_url is not None:\n        #     keys.append(\"img_emb\")\n        # if text_embedding_folder_url is not None:\n        #     keys.append(\"text_emb\")\n        # keys.extend(extra_keys)\n        self.key_map = {key: i for i, key in enumerate(keys)}\n        self.resampling = resample\n        self.img_preproc = img_preproc\n        # If s3, check if s3fs is installed and s3cmd is installed and check if the data is piped instead of straight up"
        },
        {
            "comment": "Code checks if the URLs provided for webdataset contain \"s3:\" indicating S3 links. If so, it requires 's3cmd' and 's3fs' packages to be installed or raises an error. It also adds shardList and allows resampling or shuffling of shards based on user input.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":169-184",
            "content": "        if (isinstance(urls, str) and \"s3:\" in urls) or (isinstance(urls, list) and any([\"s3:\" in url for url in urls])):\n            # Then this has an s3 link for the webdataset and we need extra packages\n            if shutil.which(\"s3cmd\") is None:\n                raise RuntimeError(\"s3cmd is required for s3 webdataset\")\n        if (img_embedding_folder_url is not None and \"s3:\" in img_embedding_folder_url) or (text_embedding_folder_url is not None and \"s3:\" in text_embedding_folder_url):\n            # Then the embeddings are being loaded from s3 and fsspec requires s3fs\n            try:\n                import s3fs\n            except ImportError:\n                raise RuntimeError(\"s3fs is required to load embeddings from s3\")\n        # Add the shardList and randomize or resample if requested\n        if resample:\n            assert not shuffle_shards, \"Cannot both resample and shuffle\"\n            self.append(wds.ResampledShards(urls))\n        else:\n            self.append(wds.SimpleShardList(urls))"
        },
        {
            "comment": "The code configures a decoder loader for DALLE2-pytorch. It shuffles 1000 filters and skips unassociated shards if necessary, loads embeddings from URLs, converts to samples, and decodes images as PILRGB.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":185-199",
            "content": "            if shuffle_shards:\n                self.append(wds.filters.shuffle(1000))\n        if img_embedding_folder_url is not None:\n            # There may be webdataset shards that do not have a embedding shard associated with it. If we do not skip these, they would cause issues.\n            self.append(skip_unassociated_shards(embeddings_url=img_embedding_folder_url, handler=handler))\n        if text_embedding_folder_url is not None:\n            self.append(skip_unassociated_shards(embeddings_url=text_embedding_folder_url, handler=handler))\n        self.append(wds.tarfile_to_samples(handler=handler))\n        self.append(wds.decode(\"pilrgb\", handler=handler))\n        if img_embedding_folder_url is not None:\n            # Then we are loading image embeddings for a remote source\n            assert index_width is not None, \"Reading embeddings separately requires index width length to be given\"\n            self.append(insert_embedding(embeddings_url=img_embedding_folder_url, index_width=index_width, sample_key='img_emb', handler=handler))"
        },
        {
            "comment": "This code creates an image embedding dataloader. If a text embedding folder URL is provided, it loads image embeddings for remote sources based on the given index width. It then applies preprocessing and joins the embeddings before returning the tuple of keys. The preproc function applies image preprocessing if available.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":200-224",
            "content": "        if text_embedding_folder_url is not None:\n            # Then we are loading image embeddings for a remote source\n            assert index_width is not None, \"Reading embeddings separately requires index width length to be given\"\n            self.append(insert_embedding(embeddings_url=text_embedding_folder_url, index_width=index_width, sample_key='text_emb', handler=handler))\n        self.append(join_embeddings)\n        self.append(key_verifier(required_keys=keys, handler=handler))\n        # Apply preprocessing\n        self.append(wds.map(self.preproc))\n        self.append(wds.to_tuple(*keys))\n    def preproc(self, sample):\n        \"\"\"Applies the preprocessing for images\"\"\"\n        if self.img_preproc is not None:\n            sample[\"jpg\"] = self.img_preproc(sample[\"jpg\"])\n        return sample\ndef create_image_embedding_dataloader(\n    tar_url,\n    num_workers,\n    batch_size,\n    img_embeddings_url=None,\n    text_embeddings_url=None,\n    index_width=None,\n    shuffle_num = None,\n    shuffle_shards = True,"
        },
        {
            "comment": "This code creates an image embedding dataset and dataloader in one line, accepting parameters such as tar_url, num_workers, batch_size, embeddings_url, and index_width. The function is designed for webdataset format and requires the same number of shards for both the webdataset images and their corresponding embeddings. It also supports handling exceptions using a specified handler.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":225-239",
            "content": "    resample_shards = False, \n    img_preproc=None,\n    extra_keys=[],\n    handler=wds.handlers.reraise_exception#warn_and_continue\n):\n    \"\"\"\n    Convenience function to create an image embedding dataseta and dataloader in one line\n    :param tar_url: A url pointing to the tar files of the webdataset formatted as /path/to/webdataset/{0000..9999}.tar\n    :param num_workers: The number of workers to use for the dataloader\n    :param batch_size: The batch size to use for the dataloader\n    :param embeddings_url: Required if webdataset does not contain embeddings. A url pointing to the npy files of the embeddings. Should have the same number of shards as the webdataset.\n        Webdataset image keys should align with the index of the embedding. This means missing image indices must have a corresponding embedding of all zeros.\n    :param index_width: The number of digits in the index. This is used to align the embedding index with the image index.\n            For example, if a file in the webdataset sh"
        },
        {
            "comment": "This code defines a function that takes in parameters like tar_url, img_embedding_folder_url, text_embeddings_url, index_width, extra_keys, img_preproc, and handler. It creates an ImageEmbeddingDataset and optionally shuffles it based on the given shuffle_num. Then, it returns a DataLoader for further processing.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":239-260",
            "content": "ard 3 is named 0003039.jpg, we know the shard is 4 digits and the last 3 digits are the index_width.\n    :param shuffle_num: If not None, shuffle the dataset with this size buffer after sampling.\n    :param shuffle_shards: If true, shuffle the shards before sampling. This cannot be true if resample is true.\n    :param resample_shards: If true, resample webdataset shards with replacement. You need to set your own epoch size if this is true since it will resample infinitely.\n    :param handler: A webdataset handler.\n    \"\"\"\n    ds = ImageEmbeddingDataset(\n        tar_url,\n        img_embedding_folder_url=img_embeddings_url,\n        text_embedding_folder_url=text_embeddings_url,\n        index_width=index_width,\n        shuffle_shards=shuffle_shards,\n        resample=resample_shards,\n        extra_keys=extra_keys,\n        img_preproc=img_preproc,\n        handler=handler\n    )\n    if shuffle_num is not None and shuffle_num > 0:\n        ds.shuffle(1000)\n    return DataLoader(\n        ds,\n        num_workers=num_workers,"
        },
        {
            "comment": "This code creates a data loader for the decoder model. It sets batch size, prefetch factor (for efficient loading), pin memory (for faster GPU transfers), and disables shuffling.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dataloaders/decoder_loader.py\":261-265",
            "content": "        batch_size=batch_size,\n        prefetch_factor=2,  # This might be good to have high so the next npy file is prefetched\n        pin_memory=True,\n        shuffle=False\n    )"
        }
    ]
}