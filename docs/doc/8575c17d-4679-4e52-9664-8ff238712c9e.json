{
    "summary": "This code divides shards, initializes training, and trains UNet models for DALL-E 2 using PyTorch. It also supports distributed training and executes as a standalone program.",
    "details": [
        {
            "comment": "This code imports various modules and defines constants for training a decoder model in the DALLE2-pytorch framework. It uses DecoderTrainer, dataloaders, trackers, train configs, utilities, and models from the dalle2_pytorch package. It also includes metrics such as FrechetInceptionDistance, InceptionScore, KernelInceptionDistance, and LearnedPerceptualImagePatchSimilarity for evaluation. Accelerate is used for accelerated training, and webdataset is used for data loading.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":0-27",
            "content": "from pathlib import Path\nfrom typing import List\nfrom datetime import timedelta\nfrom dalle2_pytorch.trainer import DecoderTrainer\nfrom dalle2_pytorch.dataloaders import create_image_embedding_dataloader\nfrom dalle2_pytorch.trackers import Tracker\nfrom dalle2_pytorch.train_configs import DecoderConfig, TrainDecoderConfig\nfrom dalle2_pytorch.utils import Timer, print_ribbon\nfrom dalle2_pytorch.dalle2_pytorch import Decoder, resize_image_to\nfrom clip import tokenize\nimport torchvision\nimport torch\nfrom torch import nn\nfrom torchmetrics.image.fid import FrechetInceptionDistance\nfrom torchmetrics.image.inception import InceptionScore\nfrom torchmetrics.image.kid import KernelInceptionDistance\nfrom torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\nfrom accelerate import Accelerator, DistributedDataParallelKwargs, InitProcessGroupKwargs\nfrom accelerate.utils import dataclasses as accelerate_dataclasses\nimport webdataset as wds\nimport click\n# constants\nTRAIN_CALC_LOSS_EVERY_ITERS = 10\nVALID_CALC_LOSS_EVERY_ITERS = 10"
        },
        {
            "comment": "This function takes available shards, URLs for embeddings, and other parameters to randomly split them into train, validation, and test sets, then returns dataloaders for each. It asserts that the proportions of splits sum up to 1, calculates the actual number of samples in each split based on the proportion, and checks if the sum of splits matches the total number of available shards.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":29-63",
            "content": "# helpers functions\ndef exists(val):\n    return val is not None\n# main functions\ndef create_dataloaders(\n    available_shards,\n    webdataset_base_url,\n    img_embeddings_url=None,\n    text_embeddings_url=None,\n    shard_width=6,\n    num_workers=4,\n    batch_size=32,\n    n_sample_images=6,\n    shuffle_train=True,\n    resample_train=False,\n    img_preproc = None,\n    index_width=4,\n    train_prop = 0.75,\n    val_prop = 0.15,\n    test_prop = 0.10,\n    seed = 0,\n    **kwargs\n):\n    \"\"\"\n    Randomly splits the available shards into train, val, and test sets and returns a dataloader for each\n    \"\"\"\n    assert train_prop + test_prop + val_prop == 1\n    num_train = round(train_prop*len(available_shards))\n    num_test = round(test_prop*len(available_shards))\n    num_val = len(available_shards) - num_train - num_test\n    assert num_train + num_test + num_val == len(available_shards), f\"{num_train} + {num_test} + {num_val} = {num_train + num_test + num_val} != {len(available_shards)}\"\n    train_split, test_split, val_split ="
        },
        {
            "comment": "This code randomly splits available shards into training, testing, and validation sets. It then generates corresponding URLs for each set by zero-padding the shard numbers to match the filename format. A lambda function is created to handle creating a dataloader for image embeddings using these URLs, considering various parameters like batch size and number of workers.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":63-79",
            "content": " torch.utils.data.random_split(available_shards, [num_train, num_test, num_val], generator=torch.Generator().manual_seed(seed))\n    # The shard number in the webdataset file names has a fixed width. We zero pad the shard numbers so they correspond to a filename.\n    train_urls = [webdataset_base_url.format(str(shard).zfill(shard_width)) for shard in train_split]\n    test_urls = [webdataset_base_url.format(str(shard).zfill(shard_width)) for shard in test_split]\n    val_urls = [webdataset_base_url.format(str(shard).zfill(shard_width)) for shard in val_split]\n    create_dataloader = lambda tar_urls, shuffle=False, resample=False, for_sampling=False: create_image_embedding_dataloader(\n        tar_url=tar_urls,\n        num_workers=num_workers,\n        batch_size=batch_size if not for_sampling else n_sample_images,\n        img_embeddings_url=img_embeddings_url,\n        text_embeddings_url=text_embeddings_url,\n        index_width=index_width,\n        shuffle_num = None,\n        extra_keys= [\"txt\"],\n        shuffle_shards = shuffle,"
        },
        {
            "comment": "The code creates multiple data loaders for training, validation, and testing datasets. It returns a dictionary with each dataset's corresponding dataloader. The `get_dataset_keys` function extracts the real dataloader if the input is a WebLoader.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":80-102",
            "content": "        resample_shards = resample, \n        img_preproc=img_preproc,\n        handler=wds.handlers.warn_and_continue\n    )\n    train_dataloader = create_dataloader(train_urls, shuffle=shuffle_train, resample=resample_train)\n    train_sampling_dataloader = create_dataloader(train_urls, shuffle=False, for_sampling=True)\n    val_dataloader = create_dataloader(val_urls, shuffle=False)\n    test_dataloader = create_dataloader(test_urls, shuffle=False)\n    test_sampling_dataloader = create_dataloader(test_urls, shuffle=False, for_sampling=True)\n    return {\n        \"train\": train_dataloader,\n        \"train_sampling\": train_sampling_dataloader,\n        \"val\": val_dataloader,\n        \"test\": test_dataloader,\n        \"test_sampling\": test_sampling_dataloader\n    }\ndef get_dataset_keys(dataloader):\n    \"\"\"\n    It is sometimes neccesary to get the keys the dataloader is returning. Since the dataset is burried in the dataloader, we need to do a process to recover it.\n    \"\"\"\n    # If the dataloader is actually a WebLoader, we need to extract the real dataloader"
        },
        {
            "comment": "The code samples the dataloader and returns a zipped list of examples. It iterates through each image, extracts its embedding, converts it to the device's format, extends the respective lists for images and text embeddings, and finally returns them.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":103-129",
            "content": "    if isinstance(dataloader, wds.WebLoader):\n        dataloader = dataloader.pipeline[0]\n    return dataloader.dataset.key_map\ndef get_example_data(dataloader, device, n=5):\n    \"\"\"\n    Samples the dataloader and returns a zipped list of examples\n    \"\"\"\n    images = []\n    img_embeddings = []\n    text_embeddings = []\n    captions = []\n    for img, emb, txt in dataloader:\n        img_emb, text_emb = emb.get('img'), emb.get('text')\n        if img_emb is not None:\n            img_emb = img_emb.to(device=device, dtype=torch.float)\n            img_embeddings.extend(list(img_emb))\n        else:\n            # Then we add None img.shape[0] times\n            img_embeddings.extend([None]*img.shape[0])\n        if text_emb is not None:\n            text_emb = text_emb.to(device=device, dtype=torch.float)\n            text_embeddings.extend(list(text_emb))\n        else:\n            # Then we add None img.shape[0] times\n            text_embeddings.extend([None]*img.shape[0])\n        img = img.to(device=device, dtype=torch.float)"
        },
        {
            "comment": "This function generates samples by taking example data and creating real images, generated images, and captions. If image embeddings are None, it generates them using the clip model. It returns three lists: real images, generated images, and captions.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":130-151",
            "content": "        images.extend(list(img))\n        captions.extend(list(txt))\n        if len(images) >= n:\n            break\n    return list(zip(images[:n], img_embeddings[:n], text_embeddings[:n], captions[:n]))\ndef generate_samples(trainer, example_data, clip=None, start_unet=1, end_unet=None, condition_on_text_encodings=False, cond_scale=1.0, device=None, text_prepend=\"\", match_image_size=True):\n    \"\"\"\n    Takes example data and generates images from the embeddings\n    Returns three lists: real images, generated images, and captions\n    \"\"\"\n    real_images, img_embeddings, text_embeddings, txts = zip(*example_data)\n    sample_params = {}\n    if img_embeddings[0] is None:\n        # Generate image embeddings from clip\n        imgs_tensor = torch.stack(real_images)\n        assert clip is not None, \"clip is None, but img_embeddings is None\"\n        imgs_tensor.to(device=device)\n        img_embeddings, img_encoding = clip.embed_image(imgs_tensor)\n        sample_params[\"image_embed\"] = img_embeddings\n    else:\n        # Then we are using precomputed image embeddings"
        },
        {
            "comment": "This code is responsible for preparing training samples by stacking image and text embeddings, setting parameters for start and stop U-net layers, and handling the case where real images are provided. If real images exist, it stacks them as part of the sample. The code also considers whether to generate text embeddings or use precomputed ones and ensures everything is on the specified device.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":152-171",
            "content": "        img_embeddings = torch.stack(img_embeddings)\n        sample_params[\"image_embed\"] = img_embeddings\n    if condition_on_text_encodings:\n        if text_embeddings[0] is None:\n            # Generate text embeddings from text\n            assert clip is not None, \"clip is None, but text_embeddings is None\"\n            tokenized_texts = tokenize(txts, truncate=True).to(device=device)\n            text_embed, text_encodings = clip.embed_text(tokenized_texts)\n            sample_params[\"text_encodings\"] = text_encodings\n        else:\n            # Then we are using precomputed text embeddings\n            text_embeddings = torch.stack(text_embeddings)\n            sample_params[\"text_encodings\"] = text_embeddings\n    sample_params[\"start_at_unet_number\"] = start_unet\n    sample_params[\"stop_at_unet_number\"] = end_unet\n    if start_unet > 1:\n        # If we are only training upsamplers\n        sample_params[\"image\"] = torch.stack(real_images)\n    if device is not None:\n        sample_params[\"_device\"] = device"
        },
        {
            "comment": "This function generates samples, combines them with real images in a grid format for easy viewing. It first calls `generate_samples` to get the real and generated images along with their corresponding captions. Then it uses `torchvision.utils.make_grid` to create grids of original and generated images.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":172-185",
            "content": "    samples = trainer.sample(**sample_params, _cast_deepspeed_precision=False)  # At sampling time we don't want to cast to FP16\n    generated_images = list(samples)\n    captions = [text_prepend + txt for txt in txts]\n    if match_image_size:\n        generated_image_size = generated_images[0].shape[-1]\n        real_images = [resize_image_to(image, generated_image_size, clamp_range=(0, 1)) for image in real_images]\n    return real_images, generated_images, captions\ndef generate_grid_samples(trainer, examples, clip=None, start_unet=1, end_unet=None, condition_on_text_encodings=False, cond_scale=1.0, device=None, text_prepend=\"\"):\n    \"\"\"\n    Generates samples and uses torchvision to put them in a side by side grid for easy viewing\n    \"\"\"\n    real_images, generated_images, captions = generate_samples(trainer, examples, clip, start_unet, end_unet, condition_on_text_encodings, cond_scale, device, text_prepend)\n    grid_images = [torchvision.utils.make_grid([original_image, generated_image]) for original_image, generated_image in zip(real_images, generated_images)]"
        },
        {
            "comment": "This function computes evaluation metrics for a decoder. It prepares data, generates samples using the trainer and start/end unets, converts images from [0, 1] to [0, 255], and types them as uint8. The generated and real images are then stored in variables for further evaluation metrics calculations.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":186-202",
            "content": "    return grid_images, captions\ndef evaluate_trainer(trainer, dataloader, device, start_unet, end_unet, clip=None, condition_on_text_encodings=False, cond_scale=1.0, inference_device=None, n_evaluation_samples=1000, FID=None, IS=None, KID=None, LPIPS=None):\n    \"\"\"\n    Computes evaluation metrics for the decoder\n    \"\"\"\n    metrics = {}\n    # Prepare the data\n    examples = get_example_data(dataloader, device, n_evaluation_samples)\n    if len(examples) == 0:\n        print(\"No data to evaluate. Check that your dataloader has shards.\")\n        return metrics\n    real_images, generated_images, captions = generate_samples(trainer, examples, clip, start_unet, end_unet, condition_on_text_encodings, cond_scale, inference_device)\n    real_images = torch.stack(real_images).to(device=device, dtype=torch.float)\n    generated_images = torch.stack(generated_images).to(device=device, dtype=torch.float)\n    # Convert from [0, 1] to [0, 255] and from torch.float to torch.uint8\n    int_real_images = real_images.mul(255).add(0.5).clamp(0, 255).type(torch.uint8)"
        },
        {
            "comment": "This code calculates and stores metrics for the quality of generated images, including Frechet Inception Distance (FID), Inception Score (IS), and Kernel Inception Distance (KID). It first scales the generated images, then checks if specific configuration files exist for each metric. If they do, it creates an instance of the corresponding metric class, sets it up on the device, updates with real and generated images, and computes the metric values. The computed metrics are stored in the \"metrics\" dictionary.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":203-226",
            "content": "    int_generated_images = generated_images.mul(255).add(0.5).clamp(0, 255).type(torch.uint8)\n    def null_sync(t, *args, **kwargs):\n        return [t]\n    if exists(FID):\n        fid = FrechetInceptionDistance(**FID, dist_sync_fn=null_sync)\n        fid.to(device=device)\n        fid.update(int_real_images, real=True)\n        fid.update(int_generated_images, real=False)\n        metrics[\"FID\"] = fid.compute().item()\n    if exists(IS):\n        inception = InceptionScore(**IS, dist_sync_fn=null_sync)\n        inception.to(device=device)\n        inception.update(int_real_images)\n        is_mean, is_std = inception.compute()\n        metrics[\"IS_mean\"] = is_mean.item()\n        metrics[\"IS_std\"] = is_std.item()\n    if exists(KID):\n        kernel_inception = KernelInceptionDistance(**KID, dist_sync_fn=null_sync)\n        kernel_inception.to(device=device)\n        kernel_inception.update(int_real_images, real=True)\n        kernel_inception.update(int_generated_images, real=False)\n        kid_mean, kid_std = kernel_inception.compute()"
        },
        {
            "comment": "This code calculates metrics such as KID and LPIPS for a model's performance. It stores the values in a dictionary, normalizes the images if LPIPS is present, applies the LearnedPerceptualImagePatchSimilarity function, and syncs the calculated metrics across processes using accelerator functions.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":227-246",
            "content": "        metrics[\"KID_mean\"] = kid_mean.item()\n        metrics[\"KID_std\"] = kid_std.item()\n    if exists(LPIPS):\n        # Convert from [0, 1] to [-1, 1]\n        renorm_real_images = real_images.mul(2).sub(1).clamp(-1,1)\n        renorm_generated_images = generated_images.mul(2).sub(1).clamp(-1,1)\n        lpips = LearnedPerceptualImagePatchSimilarity(**LPIPS, dist_sync_fn=null_sync)\n        lpips.to(device=device)\n        lpips.update(renorm_real_images, renorm_generated_images)\n        metrics[\"LPIPS\"] = lpips.compute().item()\n    if trainer.accelerator.num_processes > 1:\n        # Then we should sync the metrics\n        metrics_order = sorted(metrics.keys())\n        metrics_tensor = torch.zeros(1, len(metrics), device=device, dtype=torch.float)\n        for i, metric_name in enumerate(metrics_order):\n            metrics_tensor[0, i] = metrics[metric_name]\n        metrics_tensor = trainer.accelerator.gather(metrics_tensor)\n        metrics_tensor = metrics_tensor.mean(dim=0)\n        for i, metric_name in enumerate(metrics_order):"
        },
        {
            "comment": "This code contains three functions: 1) `train_decoder`, which updates metrics based on the current metric; 2) `save_trainer`, which logs the model using an appropriate method according to the tracker; and 3) `recall_trainer`, which loads the model using the tracker. The code is part of a larger system that likely involves training a machine learning model, tracking its progress, and recalling it for further use or evaluation.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":247-263",
            "content": "            metrics[metric_name] = metrics_tensor[i].item()\n    return metrics\ndef save_trainer(tracker: Tracker, trainer: DecoderTrainer, epoch: int, sample: int, next_task: str, validation_losses: List[float], samples_seen: int, is_latest=True, is_best=False):\n    \"\"\"\n    Logs the model with an appropriate method depending on the tracker\n    \"\"\"\n    tracker.save(trainer, is_best=is_best, is_latest=is_latest, epoch=epoch, sample=sample, next_task=next_task, validation_losses=validation_losses, samples_seen=samples_seen)\ndef recall_trainer(tracker: Tracker, trainer: DecoderTrainer):\n    \"\"\"\n    Loads the model with an appropriate method depending on the tracker\n    \"\"\"\n    trainer.accelerator.print(print_ribbon(f\"Loading model from {type(tracker.loader).__name__}\"))\n    state_dict = tracker.recall()\n    trainer.load_state_dict(state_dict, only_model=False, strict=True)\n    return state_dict.get(\"epoch\", 0), state_dict.get(\"validation_losses\", []), state_dict.get(\"next_task\", \"train\"), state_dict.get(\"sample\", 0), state_dict.get(\"samples_seen\", 0)"
        },
        {
            "comment": "The function trains a decoder on a dataset, using the specified dataloaders, Decoder instance, and Accelerator. It also has optional arguments for clip, evaluate_config, epoch_samples, validation_samples, save_immediately, epochs, n_sample_images, save_every_n_samples, unet_training_mask, condition_on_text_encodings, and cond_scale. The function checks if the unet_training_mask exists and asserts that its length matches the number of unets in the decoder. It also assigns trainable unet numbers to a list.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":265-293",
            "content": "def train(\n    dataloaders,\n    decoder: Decoder,\n    accelerator: Accelerator,\n    tracker: Tracker,\n    inference_device,\n    clip=None,\n    evaluate_config=None,\n    epoch_samples = None,  # If the training dataset is resampling, we have to manually stop an epoch\n    validation_samples = None,\n    save_immediately=False,\n    epochs = 20,\n    n_sample_images = 5,\n    save_every_n_samples = 100000,\n    unet_training_mask=None,\n    condition_on_text_encodings=False,\n    cond_scale=1.0,\n    **kwargs\n):\n    \"\"\"\n    Trains a decoder on a dataset.\n    \"\"\"\n    is_master = accelerator.process_index == 0\n    if not exists(unet_training_mask):\n        # Then the unet mask should be true for all unets in the decoder\n        unet_training_mask = [True] * len(decoder.unets)\n    assert len(unet_training_mask) == len(decoder.unets), f\"The unet training mask should be the same length as the number of unets in the decoder. Got {len(unet_training_mask)} and {trainer.num_unets}\"\n    trainable_unet_numbers = [i+1 for i, trainable in enumerate(unet_training_mask) if trainable]"
        },
        {
            "comment": "The code is removing non-trainable UNet modules and setting up a trainer for the given task. It also checks if the state can be recalled from a previous training session and updates relevant variables accordingly.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":294-321",
            "content": "    first_trainable_unet = trainable_unet_numbers[0]\n    last_trainable_unet = trainable_unet_numbers[-1]\n    def move_unets(unet_training_mask):\n        for i in range(len(decoder.unets)):\n            if not unet_training_mask[i]:\n                # Replace the unet from the module list with a nn.Identity(). This training script never uses unets that aren't being trained so this is fine.\n                decoder.unets[i] = nn.Identity().to(inference_device)\n    # Remove non-trainable unets\n    move_unets(unet_training_mask)\n    trainer = DecoderTrainer(\n        decoder=decoder,\n        accelerator=accelerator,\n        dataloaders=dataloaders,\n        **kwargs\n    )\n    # Set up starting model and parameters based on a recalled state dict\n    start_epoch = 0\n    validation_losses = []\n    next_task = 'train'\n    sample = 0\n    samples_seen = 0\n    val_sample = 0\n    step = lambda: int(trainer.num_steps_taken(unet_number=first_trainable_unet))\n    if tracker.can_recall:\n        start_epoch, validation_losses, next_task, recalled_sample, samples_seen = recall_trainer(tracker, trainer)"
        },
        {
            "comment": "The code loads a model and starts training from the specified task, either 'train' or 'val'. It prints the details of the loaded model, including epoch, samples seen, and minimum validation loss. The trainer is moved to the inference device. Example data for both training and testing is generated using get_example_data function with the specified number of sample images.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":322-336",
            "content": "        if next_task == 'train':\n            sample = recalled_sample\n        if next_task == 'val':\n            val_sample = recalled_sample\n        accelerator.print(f\"Loaded model from {type(tracker.loader).__name__} on epoch {start_epoch} having seen {samples_seen} samples with minimum validation loss {min(validation_losses) if len(validation_losses) > 0 else 'N/A'}\")\n        accelerator.print(f\"Starting training from task {next_task} at sample {sample} and validation sample {val_sample}\")\n    trainer.to(device=inference_device)\n    accelerator.print(print_ribbon(\"Generating Example Data\", repeat=40))\n    accelerator.print(\"This can take a while to load the shard lists...\")\n    if is_master:\n        train_example_data = get_example_data(dataloaders[\"train_sampling\"], inference_device, n_sample_images)\n        accelerator.print(\"Generated training examples\")\n        test_example_data = get_example_data(dataloaders[\"test_sampling\"], inference_device, n_sample_images)\n        accelerator.print(\"Generated testing examples\")"
        },
        {
            "comment": "Iterating over epochs in training mode, counting the total number of samples across all processes. Gathering sample length tensors using accelerator's gather function and summing them up to get the total samples seen. Updating sample and samples_seen variables accordingly.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":338-356",
            "content": "    send_to_device = lambda arr: [x.to(device=inference_device, dtype=torch.float) for x in arr]\n    sample_length_tensor = torch.zeros(1, dtype=torch.int, device=inference_device)\n    unet_losses_tensor = torch.zeros(TRAIN_CALC_LOSS_EVERY_ITERS, trainer.num_unets, dtype=torch.float, device=inference_device)\n    for epoch in range(start_epoch, epochs):\n        accelerator.print(print_ribbon(f\"Starting epoch {epoch}\", repeat=40))\n        timer = Timer()\n        last_sample = sample\n        last_snapshot = sample\n        if next_task == 'train':\n            for i, (img, emb, txt) in enumerate(dataloaders[\"train\"]):\n                # We want to count the total number of samples across all processes\n                sample_length_tensor[0] = len(img)\n                all_samples = accelerator.gather(sample_length_tensor)  # TODO: accelerator.reduce is broken when this was written. If it is fixed replace this.\n                total_samples = all_samples.sum().item()\n                sample += total_samples\n                samples_seen += total_samples"
        },
        {
            "comment": "This code checks if there are image or text embeddings available, sends them to the device, and then trains a model. It also performs a forward pass for image embedding generation if necessary.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":357-379",
            "content": "                img_emb = emb.get('img')\n                has_img_embedding = img_emb is not None\n                if has_img_embedding:\n                    img_emb, = send_to_device((img_emb,))\n                text_emb = emb.get('text')\n                has_text_embedding = text_emb is not None\n                if has_text_embedding:\n                    text_emb, = send_to_device((text_emb,))\n                img, = send_to_device((img,))\n                trainer.train()\n                for unet in range(1, trainer.num_unets+1):\n                    # Check if this is a unet we are training\n                    if not unet_training_mask[unet-1]: # Unet index is the unet number - 1\n                        continue\n                    forward_params = {}\n                    if has_img_embedding:\n                        forward_params['image_embed'] = img_emb\n                    else:\n                        # Forward pass automatically generates embedding\n                        assert clip is not None\n                        img_embed, img_encoding = clip.embed_image(img)"
        },
        {
            "comment": "This code chunk is for training the DALL-E 2 model's decoder. It first checks if image and text embeddings are provided, and if not, it tokenizes the text and generates text embeddings using the CLIP model. Then, it passes the required parameters to the trainer and updates the model, storing the loss for each unit in the unet_losses_tensor array.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":380-393",
            "content": "                        forward_params['image_embed'] = img_embed\n                    if condition_on_text_encodings:\n                        if has_text_embedding:\n                            forward_params['text_encodings'] = text_emb\n                        else:\n                            # Then we need to pass the text instead\n                            assert clip is not None\n                            tokenized_texts = tokenize(txt, truncate=True).to(inference_device)\n                            assert tokenized_texts.shape[0] == len(img), f\"The number of texts ({tokenized_texts.shape[0]}) should be the same as the number of images ({len(img)})\"\n                            text_embed, text_encodings = clip.embed_text(tokenized_texts)\n                            forward_params['text_encodings'] = text_encodings\n                    loss = trainer.forward(img, **forward_params, unet_number=unet, _device=inference_device)\n                    trainer.update(unet_number=unet)\n                    unet_losses_tensor[i % TRAIN_CALC_LOSS_EVERY_ITERS, unet-1] = loss"
        },
        {
            "comment": "This code is calculating the samples per second and resetting timers, then averaging the losses across all processes for a UNet model. It gathers the decay rate on each UNet, logs epoch, sample, and step information.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":395-412",
            "content": "                samples_per_sec = (sample - last_sample) / timer.elapsed()\n                timer.reset()\n                last_sample = sample\n                if i % TRAIN_CALC_LOSS_EVERY_ITERS == 0:\n                    # We want to average losses across all processes\n                    unet_all_losses = accelerator.gather(unet_losses_tensor)\n                    mask = unet_all_losses != 0\n                    unet_average_loss = (unet_all_losses * mask).sum(dim=0) / mask.sum(dim=0)\n                    loss_map = { f\"Unet {index} Training Loss\": loss.item() for index, loss in enumerate(unet_average_loss) if unet_training_mask[index] }\n                    # gather decay rate on each UNet\n                    ema_decay_list = {f\"Unet {index} EMA Decay\": ema_unet.get_current_decay() for index, ema_unet in enumerate(trainer.ema_unets) if unet_training_mask[index]}\n                    log_data = {\n                        \"Epoch\": epoch,\n                        \"Sample\": sample,\n                        \"Step\": i,"
        },
        {
            "comment": "This code snippet is logging data and saving a snapshot of the model at specific intervals. It logs samples per second, samples seen, EMA decay parameters, and loss metrics. The snapshot is saved if the current sample meets certain conditions or every time an immediate save command is issued. The code prints \"Saving snapshot\" when a snapshot is taken.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":413-430",
            "content": "                        \"Samples per second\": samples_per_sec,\n                        \"Samples Seen\": samples_seen,\n                        **ema_decay_list,\n                        **loss_map\n                    }\n                    if is_master:\n                        tracker.log(log_data, step=step())\n                if is_master and (last_snapshot + save_every_n_samples < sample or (save_immediately and i == 0)):  # This will miss by some amount every time, but it's not a big deal... I hope\n                    # It is difficult to gather this kind of info on the accelerator, so we have to do it on the master\n                    print(\"Saving snapshot\")\n                    last_snapshot = sample\n                    # We need to know where the model should be saved\n                    save_trainer(tracker, trainer, epoch, sample, next_task, validation_losses, samples_seen)\n                    if exists(n_sample_images) and n_sample_images > 0:\n                        trainer.eval()\n             "
        },
        {
            "comment": "This code is used for training a model and validating it. It generates samples from the training dataset, logs them, checks if it should stop based on sample count, switches to validation mode, and initializes variables for validation.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":430-447",
            "content": "           train_images, train_captions = generate_grid_samples(trainer, train_example_data, clip, first_trainable_unet, last_trainable_unet, condition_on_text_encodings, cond_scale, inference_device, \"Train: \")\n                        tracker.log_images(train_images, captions=train_captions, image_section=\"Train Samples\", step=step())\n                if epoch_samples is not None and sample >= epoch_samples:\n                    break\n            next_task = 'val'\n            sample = 0\n        all_average_val_losses = None\n        if next_task == 'val':\n            trainer.eval()\n            accelerator.print(print_ribbon(f\"Starting Validation {epoch}\", repeat=40))\n            last_val_sample = val_sample\n            val_sample_length_tensor = torch.zeros(1, dtype=torch.int, device=inference_device)\n            average_val_loss_tensor = torch.zeros(1, trainer.num_unets, dtype=torch.float, device=inference_device)\n            timer = Timer()\n            accelerator.wait_for_everyone()\n            i = 0"
        },
        {
            "comment": "This code is part of the DALLE2-pytorch training process. It iterates over the validation dataloader, gathers sample lengths, calculates total samples, and checks for image and text embeddings. If available, it sends these embeddings along with images to the device for further processing. This code ensures that all necessary data is properly prepared and sent to the device for evaluation.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":448-466",
            "content": "            for i, (img, emb, txt) in enumerate(dataloaders['val']):  # Use the accelerate prepared loader\n                val_sample_length_tensor[0] = len(img)\n                all_samples = accelerator.gather(val_sample_length_tensor)\n                total_samples = all_samples.sum().item()\n                val_sample += total_samples\n                img_emb = emb.get('img')\n                has_img_embedding = img_emb is not None\n                if has_img_embedding:\n                    img_emb, = send_to_device((img_emb,))\n                text_emb = emb.get('text')\n                has_text_embedding = text_emb is not None\n                if has_text_embedding:\n                    text_emb, = send_to_device((text_emb,))\n                img, = send_to_device((img,))\n                for unet in range(1, len(decoder.unets)+1):\n                    if not unet_training_mask[unet-1]: # Unet index is the unet number - 1\n                        # No need to evaluate an unchanging unet\n                        continue"
        },
        {
            "comment": "This code segment checks if image and text embeddings are provided. If not, it automatically generates image embedding or passes the text instead based on the condition. It also asserts the number of texts should be equal to the number of images for consistency.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":468-483",
            "content": "                    forward_params = {}\n                    if has_img_embedding:\n                        forward_params['image_embed'] = img_emb.float()\n                    else:\n                        # Forward pass automatically generates embedding\n                        assert clip is not None\n                        img_embed, img_encoding = clip.embed_image(img)\n                        forward_params['image_embed'] = img_embed\n                    if condition_on_text_encodings:\n                        if has_text_embedding:\n                            forward_params['text_encodings'] = text_emb.float()\n                        else:\n                            # Then we need to pass the text instead\n                            assert clip is not None\n                            tokenized_texts = tokenize(txt, truncate=True).to(device=inference_device)\n                            assert tokenized_texts.shape[0] == len(img), f\"The number of texts ({tokenized_texts.shape[0]}) should be the same as the number of images ({len(img)})\""
        },
        {
            "comment": "This code snippet is part of a larger model training process. It calculates the loss based on input images and text, updates the average validation loss, prints validation progress including samples per second and loss, and eventually breaks the loop when the specified number of validation samples have been processed. The code uses the PyTorch framework and the DALLE2 library for embedding text.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":484-499",
            "content": "                            text_embed, text_encodings = clip.embed_text(tokenized_texts)\n                            forward_params['text_encodings'] = text_encodings\n                    loss = trainer.forward(img.float(), **forward_params, unet_number=unet, _device=inference_device)\n                    average_val_loss_tensor[0, unet-1] += loss\n                if i % VALID_CALC_LOSS_EVERY_ITERS == 0:\n                    samples_per_sec = (val_sample - last_val_sample) / timer.elapsed()\n                    timer.reset()\n                    last_val_sample = val_sample\n                    accelerator.print(f\"Epoch {epoch}/{epochs} Val Step {i} -  Sample {val_sample} - {samples_per_sec:.2f} samples/sec\")\n                    accelerator.print(f\"Loss: {(average_val_loss_tensor / (i+1))}\")\n                    accelerator.print(\"\")\n                if validation_samples is not None and val_sample >= validation_samples:\n                    break\n            print(f\"Rank {accelerator.state.process_index} finished validation after {i} steps\")"
        },
        {
            "comment": "This code is used for averaging the validation losses and logging them during training. It also starts the evaluation process if it's time to do so, printing a message to indicate this. The average_val_loss_tensor is gathered by the accelerator, and then the mean of all the average loss tensors is calculated if the current task is 'eval'. If there are no zeros in the unet_average_val_loss, the validation losses are logged.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":500-514",
            "content": "            accelerator.wait_for_everyone()\n            average_val_loss_tensor /= i+1\n            # Gather all the average loss tensors\n            all_average_val_losses = accelerator.gather(average_val_loss_tensor)\n            if is_master:\n                unet_average_val_loss = all_average_val_losses.mean(dim=0)\n                val_loss_map = { f\"Unet {index} Validation Loss\": loss.item() for index, loss in enumerate(unet_average_val_loss) if loss != 0 }\n                tracker.log(val_loss_map, step=step())\n            next_task = 'eval'\n        if next_task == 'eval':\n            if exists(evaluate_config):\n                accelerator.print(print_ribbon(f\"Starting Evaluation {epoch}\", repeat=40))\n                evaluation = evaluate_trainer(trainer, dataloaders[\"val\"], inference_device, first_trainable_unet, last_trainable_unet, clip=clip, inference_device=inference_device, **evaluate_config.model_dump(), condition_on_text_encodings=condition_on_text_encodings, cond_scale=cond_scale)\n                if is_master:"
        },
        {
            "comment": "The code is generating sample images and saving the model if it is the master process. It prints a ribbon and then generates grid samples from both train and test example data, conditioning on text encodings. Finally, it logs the generated images using the tracker, with labels indicating whether they are test or train samples.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":515-527",
            "content": "                    tracker.log(evaluation, step=step())\n            next_task = 'sample'\n            val_sample = 0\n        if next_task == 'sample':\n            if is_master:\n                # Generate examples and save the model if we are the master\n                # Generate sample images\n                print(print_ribbon(f\"Sampling Set {epoch}\", repeat=40))\n                test_images, test_captions = generate_grid_samples(trainer, test_example_data, clip, first_trainable_unet, last_trainable_unet, condition_on_text_encodings, cond_scale, inference_device, \"Test: \")\n                train_images, train_captions = generate_grid_samples(trainer, train_example_data, clip, first_trainable_unet, last_trainable_unet, condition_on_text_encodings, cond_scale, inference_device, \"Train: \")\n                tracker.log_images(test_images, captions=test_captions, image_section=\"Test Samples\", step=step())\n                tracker.log_images(train_images, captions=train_captions, image_section=\"Train Samples\", step=step())"
        },
        {
            "comment": "The code checks if the average validation loss is lower than previous min, and saves the trainer if it's a new minimum. It's part of a function called create_tracker that creates a tracker object with accelerator, config, and dummy parameters.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":529-545",
            "content": "                print(print_ribbon(f\"Starting Saving {epoch}\", repeat=40))\n                is_best = False\n                if all_average_val_losses is not None:\n                    average_loss = all_average_val_losses.mean(dim=0).sum() / sum(unet_training_mask)\n                    if len(validation_losses) == 0 or average_loss < min(validation_losses):\n                        is_best = True\n                    validation_losses.append(average_loss)\n                save_trainer(tracker, trainer, epoch, sample, next_task, validation_losses, samples_seen, is_best=is_best)\n            next_task = 'train'\ndef create_tracker(accelerator: Accelerator, config: TrainDecoderConfig, config_path: str, dummy: bool = False) -> Tracker:\n    tracker_config = config.tracker\n    accelerator_config = {\n        \"Distributed\": accelerator.distributed_type != accelerate_dataclasses.DistributedType.NO,\n        \"DistributedType\": accelerator.distributed_type,\n        \"NumProcesses\": accelerator.num_processes,\n        \"MixedPrecision\": accelerator.mixed_precision"
        },
        {
            "comment": "This code initializes distributed training for DALLE2, sets manual seed, and creates an accelerator for parallel processing with optional arguments. The function returns a tracker object to save configuration.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":546-562",
            "content": "    }\n    accelerator.wait_for_everyone()  # If nodes arrive at this point at different times they might try to autoresume the current run which makes no sense and will cause errors\n    tracker: Tracker = tracker_config.create(config, accelerator_config, dummy_mode=dummy)\n    tracker.save_config(config_path, config_name='decoder_config.json')\n    tracker.add_save_metadata(state_dict_key='config', metadata=config.model_dump())\n    return tracker\ndef initialize_training(config: TrainDecoderConfig, config_path):\n    # Make sure if we are not loading, distributed models are initialized to the same values\n    torch.manual_seed(config.seed)\n    # Set up accelerator for configurable distributed training\n    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=config.train.find_unused_parameters, static_graph=config.train.static_graph)\n    init_kwargs = InitProcessGroupKwargs(timeout=timedelta(seconds=60*60))\n    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs, init_kwargs])\n    if accelerator.num_processes > 1:"
        },
        {
            "comment": "This code snippet is part of a distributed training process where it checks the accelerator settings, data sharding, and creates dataloaders for training. It ensures all processes are connected, handles DeepSpeed mixed precision mode without learned variance, splits data shards evenly across processes, and finally creates the necessary dataloaders for the training process.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":563-580",
            "content": "        # We are using distributed training and want to immediately ensure all can connect\n        accelerator.print(\"Waiting for all processes to connect...\")\n        accelerator.wait_for_everyone()\n        accelerator.print(\"All processes online and connected\")\n    # If we are in deepspeed fp16 mode, we must ensure learned variance is off\n    if accelerator.mixed_precision == \"fp16\" and accelerator.distributed_type == accelerate_dataclasses.DistributedType.DEEPSPEED and config.decoder.learned_variance:\n        raise ValueError(\"DeepSpeed fp16 mode does not support learned variance\")\n    # Set up data\n    all_shards = list(range(config.data.start_shard, config.data.end_shard + 1))\n    world_size = accelerator.num_processes\n    rank = accelerator.process_index\n    shards_per_process = len(all_shards) // world_size\n    assert shards_per_process > 0, \"Not enough shards to split evenly\"\n    my_shards = all_shards[rank * shards_per_process: (rank + 1) * shards_per_process]\n    dataloaders = create_dataloaders ("
        },
        {
            "comment": "The code initializes the decoder model with specified parameters, removes clip if present for compatibility, and creates a tracker if the current rank is not the master. It also calculates the number of parameters in the model and prepares it for training.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":581-602",
            "content": "        available_shards=my_shards,\n        img_preproc = config.data.img_preproc,\n        train_prop = config.data.splits.train,\n        val_prop = config.data.splits.val,\n        test_prop = config.data.splits.test,\n        n_sample_images=config.train.n_sample_images,\n        **config.data.model_dump(),\n        rank = rank,\n        seed = config.seed,\n    )\n    # If clip is in the model, we need to remove it for compatibility with deepspeed\n    clip = None\n    if config.decoder.clip is not None:\n        clip = config.decoder.clip.create()  # Of course we keep it to use it during training, just not in the decoder as that causes issues\n        config.decoder.clip = None\n    # Create the decoder model and print basic info\n    decoder = config.decoder.create()\n    get_num_parameters = lambda model, only_training=False: sum(p.numel() for p in model.parameters() if (p.requires_grad or not only_training))\n    # Create and initialize the tracker if we are the master\n    tracker = create_tracker(accelerator, config, config_path, dummy = rank!=0)"
        },
        {
            "comment": "This code checks if image and/or text embeddings are available, either precomputed or generated using CLIP model. It then prints a message indicating the source of embeddings used for training.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":604-626",
            "content": "    has_img_embeddings = config.data.img_embeddings_url is not None\n    has_text_embeddings = config.data.text_embeddings_url is not None\n    conditioning_on_text = any([unet.cond_on_text_encodings for unet in config.decoder.unets])\n    has_clip_model = clip is not None\n    data_source_string = \"\"\n    if has_img_embeddings:\n        data_source_string += \"precomputed image embeddings\"\n    elif has_clip_model:\n        data_source_string += \"clip image embeddings generation\"\n    else:\n        raise ValueError(\"No image embeddings source specified\")\n    if conditioning_on_text:\n        if has_text_embeddings:\n            data_source_string += \" and precomputed text embeddings\"\n        elif has_clip_model:\n            data_source_string += \" and clip text encoding generation\"\n        else:\n            raise ValueError(\"No text embeddings source specified\")\n    accelerator.print(print_ribbon(\"Loaded Config\", repeat=40))\n    accelerator.print(f\"Running training with {accelerator.num_processes} processes and {accelerator.distributed_type} distributed training\")"
        },
        {
            "comment": "Training of the decoder is being executed using the specified data source, with or without conditioning on text. The number of parameters in total and for training are displayed, along with similar information for each Unet. The train function is called with dataloaders, decoder, accelerator, clip, tracker, inference_device, evaluate_config, and condition_on_text_encodings as arguments. A simple click command line interface is created to load the config and start training, using a default configuration file path and allowing for an alternative path to be specified with the --config_file option.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":627-646",
            "content": "    accelerator.print(f\"Training using {data_source_string}. {'conditioned on text' if conditioning_on_text else 'not conditioned on text'}\")\n    accelerator.print(f\"Number of parameters: {get_num_parameters(decoder)} total; {get_num_parameters(decoder, only_training=True)} training\")\n    for i, unet in enumerate(decoder.unets):\n        accelerator.print(f\"Unet {i} has {get_num_parameters(unet)} total; {get_num_parameters(unet, only_training=True)} training\")\n    train(dataloaders, decoder, accelerator,\n        clip=clip,\n        tracker=tracker,\n        inference_device=accelerator.device,\n        evaluate_config=config.evaluate,\n        condition_on_text_encodings=conditioning_on_text,\n        **config.train.model_dump(),\n    )\n# Create a simple click command line interface to load the config and start the training\n@click.command()\n@click.option(\"--config_file\", default=\"./train_decoder_config.json\", help=\"Path to config file\")\ndef main(config_file):\n    config_file_path = Path(config_file)\n    config = TrainDecoderConfig.from_json_path(str(config_file_path))"
        },
        {
            "comment": "This code snippet initializes training and then calls the main function if the script is run directly. It ensures proper execution when running the script as a standalone program.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/train_decoder.py\":647-650",
            "content": "    initialize_training(config, config_path=config_file_path)\nif __name__ == \"__main__\":\n    main()"
        }
    ]
}