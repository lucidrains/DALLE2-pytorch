{
    "summary": "The code uses VQGAN-VAE, CLIP, and CoCa libraries for image generation, and includes helper functions, PyTorch CLIP model, neural networks, DALL-E 2 architecture, self-attention layers with normalization and dropout regularization. It initializes efficient DALL-E 2 and Imagen models, utilizes diffusion models for denoising and inpainting images, and incorporates conditional sampling from DALLE2-pytorch model for low-resolution image generation.",
    "details": [
        {
            "comment": "This code imports various libraries and defines functions for data processing, including image resizing, Gaussian blurring, and rotary embeddings. It also utilizes the VQGAN-VAE, CLIP model, and CoCa. The code contains namedtuples, helper functions, and constants relevant to the tasks of image generation and language modeling.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":0-48",
            "content": "import math\nimport random\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager\nfrom collections import namedtuple\nfrom pathlib import Path\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\nfrom torch import nn, einsum\nimport torchvision.transforms as T\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange\nfrom kornia.filters import gaussian_blur2d\nimport kornia.augmentation as K\nfrom dalle2_pytorch.tokenizer import tokenizer\nfrom dalle2_pytorch.vqgan_vae import NullVQGanVAE, VQGanVAE\nfrom resize_right import resize\n# rotary embeddings\nfrom rotary_embedding_torch import RotaryEmbedding\n# use x-clip\nfrom x_clip import CLIP\nfrom coca_pytorch import CoCa\n# constants\nNAT = 1. / math.log(2.)\nUnetOutput = namedtuple('UnetOutput', ['pred', 'var_interp_frac_unnormalized'])\n# helper functions\ndef exists(val):\n    return val is not None\ndef identity(t, *args, **kwargs):\n    return t\ndef first(arr, d = None):"
        },
        {
            "comment": "Function 'if len(arr) == 0: return d' checks if the array is empty and returns the value 'd' if it is.\n'maybe(fn)' function creates a decorator that checks if the input exists, returning it if it does not.\n'default(val, d)' function returns the provided value 'val' if it exists; otherwise, it returns the default value 'd'.\n'cast_tuple(val, length=None, validate=True)' casts its argument to a tuple and optionally checks its length.\n'module_device(module)' retrieves the device of the module, defaulting to CPU for certain types like nn.Identity.\n'zero_init_(m)' initializes the weights and biases of the given module 'm' with zeros.\n'null_context(*args, **kwargs)' is a context manager that does nothing.\n'eval_decorator(fn)' wraps a function to evaluate the model before executing it.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":49-95",
            "content": "    if len(arr) == 0:\n        return d\n    return arr[0]\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x, *args, **kwargs):\n        if not exists(x):\n            return x\n        return fn(x, *args, **kwargs)\n    return inner\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\ndef cast_tuple(val, length = None, validate = True):\n    if isinstance(val, list):\n        val = tuple(val)\n    out = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n    if exists(length) and validate:\n        assert len(out) == length\n    return out\ndef module_device(module):\n    if isinstance(module, nn.Identity):\n        return 'cpu' # It doesn't matter\n    return next(module.parameters()).device\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n@contextmanager\ndef null_context(*args, **kwargs):\n    yield\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)"
        },
        {
            "comment": "This code defines several helper functions for processing lists of strings, padding tuples to a specific length, and creating checkpointable versions of Python functions. It also includes a function to determine if a given dtype is a floating point type, and a conditional wrapper for creating a checkpointable version of a function or module list.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":96-132",
            "content": "        model.train(was_training)\n        return out\n    return inner\ndef is_float_dtype(dtype):\n    return any([dtype == float_dtype for float_dtype in (torch.float64, torch.float32, torch.float16, torch.bfloat16)])\ndef is_list_str(x):\n    if not isinstance(x, (list, tuple)):\n        return False\n    return all([type(el) == str for el in x])\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n# checkpointing helper function\ndef make_checkpointable(fn, **kwargs):\n    if isinstance(fn, nn.ModuleList):\n        return [maybe(make_checkpointable)(el, **kwargs) for el in fn]\n    condition = kwargs.pop('condition', None)\n    if exists(condition) and not condition(fn):\n        return fn\n    @wraps(fn)\n    def inner(*args):\n        input_needs_grad = any([isinstance(el, torch.Tensor) and el.requires_grad for el in args])\n        if not input_needs_grad:\n            return fn(*args)\n        return checkpoint(fn, *args)"
        },
        {
            "comment": "The code defines functions for controlling the gradient flow in a module, freezing all layers in a model, and making it evaluate only. It also includes helper functions to log a tensor, normalize a tensor using L2 norm, and resize an image to the specified size with optional interpolation method.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":134-178",
            "content": "    return inner\n# for controlling freezing of CLIP\ndef set_module_requires_grad_(module, requires_grad):\n    for param in module.parameters():\n        param.requires_grad = requires_grad\ndef freeze_all_layers_(module):\n    set_module_requires_grad_(module, False)\ndef unfreeze_all_layers_(module):\n    set_module_requires_grad_(module, True)\ndef freeze_model_and_make_eval_(model):\n    model.eval()\n    freeze_all_layers_(model)\n# tensor helpers\ndef log(t, eps = 1e-12):\n    return torch.log(t.clamp(min = eps))\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    nearest = False,\n    **kwargs\n):\n    orig_image_size = image.shape[-1]\n    if orig_image_size == target_image_size:\n        return image\n    if not nearest:\n        scale_factors = target_image_size / orig_image_size\n        out = resize(image, scale_factors = scale_factors, **kwargs)\n    else:\n        out = F.interpolate(image, target_image_size, mode = 'nearest')\n    if exists(clamp_range):"
        },
        {
            "comment": "This code defines a function for normalizing an image to the range of -1 to 1, and another for unnormalizing it back to the 0 to 1 range. It also includes a namedtuple for returning embedded text and image data along with their encodings. The code further defines a base class for clip adapters that takes a CLIP model as an argument and provides methods for validating and resizing images to match CLIP's requirements.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":179-213",
            "content": "        out = out.clamp(*clamp_range)\n    return out\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n# but CLIP may otherwise\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n# clip related adapters\nEmbeddedText = namedtuple('EmbedTextReturn', ['text_embed', 'text_encodings'])\nEmbeddedImage = namedtuple('EmbedImageReturn', ['image_embed', 'image_encodings'])\nclass BaseClipAdapter(nn.Module):\n    def __init__(self, clip, **kwargs):\n        super().__init__()\n        self.clip = clip\n        self.overrides = kwargs\n    def validate_and_resize_image(self, image):\n        image_size = image.shape[-1]\n        assert image_size >= self.image_size, f'you are passing in an image of size {image_size} but CLIP requires the image size to be at least {self.image_size}'\n        return resize_image_to(image, self.image_size)\n    @property\n    def dim_latent(self):\n        raise NotImplementedError\n    @property"
        },
        {
            "comment": "This code defines a base class `BaseClipAdapter` with four methods that must be implemented by derived classes. The `XClipAdapter` class inherits from `BaseClipAdapter` and provides implementations for the properties of the underlying `clip` object, which is an instance of some clip model. The `embed_text` method takes a text input, truncates it to fit the maximum text length defined by `max_text_len`, applies a text transformer from the `clip` object, and returns the embeddings.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":214-256",
            "content": "    def image_size(self):\n        raise NotImplementedError\n    @property\n    def image_channels(self):\n        raise NotImplementedError\n    @property\n    def max_text_len(self):\n        raise NotImplementedError\n    def embed_text(self, text):\n        raise NotImplementedError\n    def embed_image(self, image):\n        raise NotImplementedError\nclass XClipAdapter(BaseClipAdapter):\n    @property\n    def dim_latent(self):\n        return self.clip.dim_latent\n    @property\n    def image_size(self):\n        return self.clip.image_size\n    @property\n    def image_channels(self):\n        return self.clip.image_channels\n    @property\n    def max_text_len(self):\n        return self.clip.text_seq_len\n    @torch.no_grad()\n    def embed_text(self, text):\n        text = text[..., :self.max_text_len]\n        text_mask = text != 0\n        encoder_output = self.clip.text_transformer(text)\n        encoder_output_is_cls = encoder_output.ndim == 3\n        text_cls, text_encodings = (encoder_output[:, 0], encoder_output[:, 1:]) if encoder_output_is_cls else (encoder_output, None)"
        },
        {
            "comment": "This code snippet defines a class called CoCaAdapter, which is a base adapter for the DALL-E 2 PyTorch model. It contains methods to embed text and images, with optional overrides for image size and channels. The dim_latent property returns the dimension of the latent space, while max_text_len is used to set the maximum length for text inputs.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":257-288",
            "content": "        text_embed = self.clip.to_text_latent(text_cls)\n        if exists(text_encodings):\n            text_encodings = text_encodings.masked_fill(~text_mask[..., None], 0.)\n        return EmbeddedText(l2norm(text_embed), text_encodings)\n    @torch.no_grad()\n    def embed_image(self, image):\n        image = self.validate_and_resize_image(image)\n        encoder_output = self.clip.visual_transformer(image)\n        image_cls, image_encodings = encoder_output[:, 0], encoder_output[:, 1:]\n        image_embed = self.clip.to_visual_latent(image_cls)\n        return EmbeddedImage(l2norm(image_embed), image_encodings)\nclass CoCaAdapter(BaseClipAdapter):\n    @property\n    def dim_latent(self):\n        return self.clip.dim\n    @property\n    def image_size(self):\n        assert 'image_size' in self.overrides\n        return self.overrides['image_size']\n    @property\n    def image_channels(self):\n        assert 'image_channels' in self.overrides\n        return self.overrides['image_channels']\n    @property\n    def max_text_len(self):"
        },
        {
            "comment": "This code is for a text-to-image model that uses CLIP as its base. It has functions to embed texts and images, with the ability to handle maximum text length. It initializes an OpenAIClipAdapter class using CLIP's 'ViT-B/32' model and finds the layer for text attention final output.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":289-318",
            "content": "        assert 'max_text_len' in self.overrides\n        return self.overrides['max_text_len']\n    @torch.no_grad()\n    def embed_text(self, text):\n        text = text[..., :self.max_text_len]\n        text_mask = text != 0\n        text_embed, text_encodings = self.clip.embed_text(text)\n        text_encodings = text_encodings.masked_fill(~text_mask[..., None], 0.)\n        return EmbeddedText(text_embed, text_encodings)\n    @torch.no_grad()\n    def embed_image(self, image):\n        image = self.validate_and_resize_image(image)\n        image_embed, image_encodings = self.clip.embed_image(image)\n        return EmbeddedImage(image_embed, image_encodings)\nclass OpenAIClipAdapter(BaseClipAdapter):\n    def __init__(\n        self,\n        name = 'ViT-B/32'\n    ):\n        import clip\n        openai_clip, preprocess = clip.load(name)\n        super().__init__(openai_clip)\n        self.eos_id = 49407 # for handling 0 being also '!'\n        text_attention_final = self.find_layer('ln_final')\n        self.dim_latent_ = text_attention_final.weight.shape[0]"
        },
        {
            "comment": "This code is part of a neural network model for text-to-image generation using PyTorch. It includes functions to handle text attention, clear the internal state, and embed input text. The class has properties such as `dim_latent`, `image_size`, `image_channels`, `max_text_len` which are used to define the network's structure and behavior.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":319-359",
            "content": "        self.handle = text_attention_final.register_forward_hook(self._hook)\n        self.clip_normalize = preprocess.transforms[-1]\n        self.cleared = False\n    def find_layer(self,  layer):\n        modules = dict([*self.clip.named_modules()])\n        return modules.get(layer, None)\n    def clear(self):\n        if self.cleared:\n            return\n        self.handle()\n    def _hook(self, _, inputs, outputs):\n        self.text_encodings = outputs\n    @property\n    def dim_latent(self):\n        return self.dim_latent_\n    @property\n    def image_size(self):\n        return self.clip.visual.input_resolution\n    @property\n    def image_channels(self):\n        return 3\n    @property\n    def max_text_len(self):\n        return self.clip.context_length\n    @torch.no_grad()\n    def embed_text(self, text):\n        text = text[..., :self.max_text_len]\n        is_eos_id = (text == self.eos_id)\n        text_mask_excluding_eos = is_eos_id.cumsum(dim = -1) == 0\n        text_mask = F.pad(text_mask_excluding_eos, (1, -1), value = True)"
        },
        {
            "comment": "Method to embed text using CLIP model by encoding the input text, applying a mask on text encodings, and returning EmbeddedText object with L2 normalized text embedding and float text encodings.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":360-389",
            "content": "        text_mask = text_mask & (text != 0)\n        assert not self.cleared\n        text_embed = self.clip.encode_text(text)\n        text_encodings = self.text_encodings\n        text_encodings = text_encodings.masked_fill(~text_mask[..., None], 0.)\n        del self.text_encodings\n        return EmbeddedText(l2norm(text_embed.float()), text_encodings.float())\n    @torch.no_grad()\n    def embed_image(self, image):\n        assert not self.cleared\n        image = self.validate_and_resize_image(image)\n        image = self.clip_normalize(image)\n        image_embed = self.clip.encode_image(image)\n        return EmbeddedImage(l2norm(image_embed.float()), None)\nclass OpenClipAdapter(BaseClipAdapter):\n    def __init__(\n        self,\n        name = 'ViT-B/32',\n        pretrained = 'laion400m_e32'\n    ):\n        import open_clip\n        clip, _, preprocess = open_clip.create_model_and_transforms(name, pretrained = pretrained)\n        super().__init__(clip)\n        self.eos_id = 49407\n        text_attention_final = self.find_layer('ln_final')"
        },
        {
            "comment": "The code represents a class that appears to be a part of a larger model. It has methods for embedding text, clearing internal state, finding layers in the network, and retrieving properties like latent dimension and maximum text length. The class relies on other components such as `preprocess`, `clip`, and `image_size`.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":390-432",
            "content": "        self._dim_latent = text_attention_final.weight.shape[0]\n        self.handle = text_attention_final.register_forward_hook(self._hook)\n        self.clip_normalize = preprocess.transforms[-1]\n        self.cleared = False\n    def find_layer(self,  layer):\n        modules = dict([*self.clip.named_modules()])\n        return modules.get(layer, None)\n    def clear(self):\n        if self.cleared:\n            return\n        self.handle()\n    def _hook(self, _, inputs, outputs):\n        self.text_encodings = outputs\n    @property\n    def dim_latent(self):\n        return self._dim_latent\n    @property\n    def image_size(self):\n        image_size = self.clip.visual.image_size\n        if isinstance(image_size, tuple):\n            return max(image_size)\n        return image_size\n    @property\n    def image_channels(self):\n        return 3\n    @property\n    def max_text_len(self):\n        return self.clip.context_length\n    @torch.no_grad()\n    def embed_text(self, text):\n        text = text[..., :self.max_text_len]\n        is_eos_id = (text == self.eos_id)"
        },
        {
            "comment": "This function takes in a text input and returns an EmbeddedText object containing the embedded text representation and a corresponding mask. It first creates a mask excluding the end of sentence (EOS) token, pads it, and applies the mask to the original mask. Then, it encodes the text using CLIP's encode_text function, and finally normalizes the resulting embeddings. The classifier free guidance functions return a probability mask based on the given probability value for a specific shape and device.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":433-458",
            "content": "        text_mask_excluding_eos = is_eos_id.cumsum(dim = -1) == 0\n        text_mask = F.pad(text_mask_excluding_eos, (1, -1), value = True)\n        text_mask = text_mask & (text != 0)\n        assert not self.cleared\n        text_embed = self.clip.encode_text(text)\n        text_encodings = self.text_encodings\n        text_encodings = text_encodings.masked_fill(~text_mask[..., None], 0.)\n        del self.text_encodings\n        return EmbeddedText(l2norm(text_embed.float()), text_encodings.float())\n    @torch.no_grad()\n    def embed_image(self, image):\n        assert not self.cleared\n        image = self.validate_and_resize_image(image)\n        image = self.clip_normalize(image)\n        image_embed = self.clip.encode_image(image)\n        return EmbeddedImage(l2norm(image_embed.float()), None)\n# classifier free guidance functions\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)"
        },
        {
            "comment": "This code defines several helper functions used in the DALLE2-pytorch model. These functions are involved in tasks such as extracting values, calculating normal KL divergence, approximating the standard normal cumulative distribution function, and computing the discretized Gaussian log likelihood. The code also includes error handling for potential nan gradients when using deepspeed fp16.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":459-487",
            "content": "    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n# gaussian diffusion helper functions\ndef extract(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\ndef meanflat(x):\n    return x.mean(dim = tuple(range(1, len(x.shape))))\ndef normal_kl(mean1, logvar1, mean2, logvar2):\n    return 0.5 * (-1.0 + logvar2 - logvar1 + torch.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * torch.exp(-logvar2))\ndef approx_standard_normal_cdf(x):\n    return 0.5 * (1.0 + torch.tanh(((2.0 / math.pi) ** 0.5) * (x + 0.044715 * (x ** 3))))\ndef discretized_gaussian_log_likelihood(x, *, means, log_scales, thres = 0.999):\n    assert x.shape == means.shape == log_scales.shape\n    # attempting to correct nan gradients when learned variance is turned on\n    # in the setting of deepspeed fp16\n    eps = 1e-12 if x.dtype == torch.float32 else 1e-3\n    centered_x = x - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_x + 1. / 255.)"
        },
        {
            "comment": "Function at line 488-518 calculates log probabilities for a given input x, using an adaptive quantile regression approach with a cosine or linear schedule. The cosine_beta_schedule function generates a sequence of beta values using a cosine schedule, and the linear_beta_schedule function generates a sequence of beta values linearly.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":488-518",
            "content": "    cdf_plus = approx_standard_normal_cdf(plus_in)\n    min_in = inv_stdv * (centered_x - 1. / 255.)\n    cdf_min = approx_standard_normal_cdf(min_in)\n    log_cdf_plus = log(cdf_plus, eps = eps)\n    log_one_minus_cdf_min = log(1. - cdf_min, eps = eps)\n    cdf_delta = cdf_plus - cdf_min\n    log_probs = torch.where(x < -thres,\n        log_cdf_plus,\n        torch.where(x > thres,\n            log_one_minus_cdf_min,\n            log(cdf_delta, eps = eps)))\n    return log_probs\ndef cosine_beta_schedule(timesteps, s = 0.008):\n    \"\"\"\n    cosine schedule\n    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n    \"\"\"\n    steps = timesteps + 1\n    x = torch.linspace(0, timesteps, steps, dtype = torch.float64)\n    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / first(alphas_cumprod)\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return torch.clip(betas, 0, 0.999)\ndef linear_beta_schedule(timesteps):\n    scale = 1000 / timesteps\n    beta_start = scale * 0.0001"
        },
        {
            "comment": "This code defines three beta scheduling functions (linear, quadratic, cosine) and a class for the NoiseScheduler. The scheduler initializes with a selected beta schedule and timesteps. The beta_schedule parameter determines which function to use for generating the betas, which represent noise scaling factors in the model's training process.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":519-547",
            "content": "    beta_end = scale * 0.02\n    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\ndef quadratic_beta_schedule(timesteps):\n    scale = 1000 / timesteps\n    beta_start = scale * 0.0001\n    beta_end = scale * 0.02\n    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps, dtype = torch.float64) ** 2\ndef sigmoid_beta_schedule(timesteps):\n    scale = 1000 / timesteps\n    beta_start = scale * 0.0001\n    beta_end = scale * 0.02\n    betas = torch.linspace(-6, 6, timesteps, dtype = torch.float64)\n    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\nclass NoiseScheduler(nn.Module):\n    def __init__(self, *, beta_schedule, timesteps, loss_type, p2_loss_weight_gamma = 0., p2_loss_weight_k = 1):\n        super().__init__()\n        if beta_schedule == \"cosine\":\n            betas = cosine_beta_schedule(timesteps)\n        elif beta_schedule == \"linear\":\n            betas = linear_beta_schedule(timesteps)\n        elif beta_schedule == \"quadratic\":\n            betas = quadratic_beta_schedule(timesteps)"
        },
        {
            "comment": "This code sets the beta schedule and alpha values based on user input, then selects a loss function according to the specified type. The code also registers buffer helper functions for 'betas' and 'alphas_cumprod'.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":548-579",
            "content": "        elif beta_schedule == \"jsd\":\n            betas = 1.0 / torch.linspace(timesteps, 1, timesteps)\n        elif beta_schedule == \"sigmoid\":\n            betas = sigmoid_beta_schedule(timesteps)\n        else:\n            raise NotImplementedError()\n        alphas = 1. - betas\n        alphas_cumprod = torch.cumprod(alphas, axis = 0)\n        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n        # register buffer helper function to cast double back to float\n        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n        register_buffer('betas', betas)\n        register_buffer('alphas_cumprod', alphas_cumprod)"
        },
        {
            "comment": "The code is registering various buffers for computations related to diffusion. It calculates the posterior variance and clips the log of the posterior variance to avoid numerical instability at the beginning of the diffusion chain.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":580-600",
            "content": "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        register_buffer('posterior_variance', posterior_variance)\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))"
        },
        {
            "comment": "In this code segment, the author is computing posterior means for a model, performing loss reweighting, generating random times, and calculating posterior values. The posterior means are calculated based on betas and alphas, while the loss reweighting considers p2_loss_weight_gamma. Random times are sampled for a batch of inputs using torch.randint. The q_posterior function calculates posterior mean, variance, and log-variance clipped from these computed values.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":601-619",
            "content": "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n        # p2 loss reweighting\n        self.has_p2_loss_reweighting = p2_loss_weight_gamma > 0.\n        register_buffer('p2_loss_weight', (p2_loss_weight_k + alphas_cumprod / (1 - alphas_cumprod)) ** -p2_loss_weight_gamma)\n    def sample_random_times(self, batch):\n        return torch.randint(0, self.num_timesteps, (batch,), device = self.betas.device, dtype = torch.long)\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped"
        },
        {
            "comment": "The code defines three functions: `q_sample`, `calculate_v`, and `q_sample_from_to`. These functions are part of a neural network for generating images. `q_sample` combines alpha and noise values to generate a sample, while `calculate_v` calculates the difference between an alpha-blended noise and a one minus alpha-blended image start. The `q_sample_from_to` function samples from one timestep to another by interpolating alphas and sigmas.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":621-644",
            "content": "    def q_sample(self, x_start, t, noise = None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n    def calculate_v(self, x_start, t, noise = None):\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n        )\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape = x_from.shape\n        noise = default(noise, lambda: torch.randn_like(x_from))\n        alpha = extract(self.sqrt_alphas_cumprod, from_t, shape)\n        sigma = extract(self.sqrt_one_minus_alphas_cumprod, from_t, shape)\n        alpha_next = extract(self.sqrt_alphas_cumprod, to_t, shape)\n        sigma_next = extract(self.sqrt_one_minus_alphas_cumprod, to_t, shape)\n        return x_from * (alpha_next / alpha) + noise * (sigma_next * alpha - sigma * alpha_next) / alpha"
        },
        {
            "comment": "The code defines three methods for predicting values from different inputs, including v and noise. It also includes a method to reweight loss using p2_loss_weight and a class to rearrange images into sequences.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":646-676",
            "content": "    def predict_start_from_v(self, x_t, t, v):\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n        )\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n    def predict_noise_from_start(self, x_t, t, x0):\n        return (\n            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n        )\n    def p2_reweigh_loss(self, loss, times):\n        if not self.has_p2_loss_reweighting:\n            return loss\n        return loss * extract(self.p2_loss_weight, times, loss.shape)\n# rearrange image to sequence\nclass RearrangeToSequence(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x):"
        },
        {
            "comment": "This function is applying layer normalization to input tensor 'x' and returning the normalized output. The 'LayerNorm' class is a type of layer normalization, while 'ChanLayerNorm' is a channel-wise version. The code includes settings for epsilon, float precision, and stability options.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":677-710",
            "content": "        x = rearrange(x, 'b c ... -> b ... c')\n        x, ps = pack([x], 'b * c')\n        x = self.fn(x)\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b ... c -> b c ...')\n        return x\n# diffusion prior\nclass LayerNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-5, fp16_eps = 1e-3, stable = False):\n        super().__init__()\n        self.eps = eps\n        self.fp16_eps = fp16_eps\n        self.stable = stable\n        self.g = nn.Parameter(torch.ones(dim))\n    def forward(self, x):\n        eps = self.eps if x.dtype == torch.float32 else self.fp16_eps\n        if self.stable:\n            x = x / x.amax(dim = -1, keepdim = True).detach()\n        var = torch.var(x, dim = -1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = -1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\nclass ChanLayerNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-5, fp16_eps = 1e-3, stable = False):\n        super().__init__()\n        self.eps = eps\n        self.fp16_eps = fp16_eps"
        },
        {
            "comment": "This code defines a Residual class that wraps a function and adds it to the input. It also contains an MLP (Multi-Layer Perceptron) class with optional normalization and activation functions, followed by a series of fully connected layers. The forward method in DALLE2_PyTorch performs normalization, calculates mean and variance, then applies element-wise transformations before returning the output.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":711-749",
            "content": "        self.stable = stable\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n    def forward(self, x):\n        eps = self.eps if x.dtype == torch.float32 else self.fp16_eps\n        if self.stable:\n            x = x / x.amax(dim = 1, keepdim = True).detach()\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n# mlp\nclass MLP(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        *,\n        expansion_factor = 2.,\n        depth = 2,\n        norm = False,\n    ):\n        super().__init__()\n        hidden_dim = int(expansion_factor * dim_out)\n        norm_fn = lambda: nn.LayerNorm(hidden_dim) if norm else nn.Identity()\n        layers = [nn.Sequential(\n            nn.Linear(dim_in, hidden_dim),"
        },
        {
            "comment": "This code defines a neural network architecture for the DALL-E 2 model. It includes a sequential layer with multiple linear layers, SiLU activation function, and normalization. The forward method performs inference on input data. Another class is defined for relative positional bias in causal transformer. The RelPosBias class initializes an embedding layer to calculate the relative position between elements for attention mechanism. It uses the concept of buckets, where each bucket represents a range of distances between two elements, and computes the relative position bucket based on input data.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":750-791",
            "content": "            nn.SiLU(),\n            norm_fn()\n        )]\n        for _ in range(depth - 1):\n            layers.append(nn.Sequential(\n                nn.Linear(hidden_dim, hidden_dim),\n                nn.SiLU(),\n                norm_fn()\n            ))\n        layers.append(nn.Linear(hidden_dim, dim_out))\n        self.net = nn.Sequential(*layers)\n    def forward(self, x):\n        return self.net(x.float())\n# relative positional bias for causal transformer\nclass RelPosBias(nn.Module):\n    def __init__(\n        self,\n        heads = 8,\n        num_buckets = 32,\n        max_distance = 128,\n    ):\n        super().__init__()\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n    @staticmethod\n    def _relative_position_bucket(\n        relative_position,\n        num_buckets = 32,\n        max_distance = 128\n    ):\n        n = -relative_position\n        n = torch.max(n, torch.zeros_like(n))\n        max_exact = num_buckets // 2\n        is_small = n < max_exact"
        },
        {
            "comment": "This code snippet defines a class for DALLE2-pytorch, containing a method to calculate relative position buckets and an attention layer. The attention layer uses the SwiGLU activation function in its FeedForward module. The purpose of this code is to facilitate the calculation and application of positional embeddings in a transformer model.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":793-815",
            "content": "        val_if_large = max_exact + (torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)).long()\n        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n        return torch.where(is_small, n, val_if_large)\n    def forward(self, i, j, *, device):\n        q_pos = torch.arange(i, dtype = torch.long, device = device)\n        k_pos = torch.arange(j, dtype = torch.long, device = device)\n        rel_pos = rearrange(k_pos, 'j -> 1 j') - rearrange(q_pos, 'i -> i 1')\n        rp_bucket = self._relative_position_bucket(rel_pos, num_buckets = self.num_buckets, max_distance = self.max_distance)\n        values = self.relative_attention_bias(rp_bucket)\n        return rearrange(values, 'i j h -> h i j')\n# feedforward\nclass SwiGLU(nn.Module):\n    \"\"\" used successfully in https://arxiv.org/abs/2204.0231 \"\"\"\n    def forward(self, x):\n        x, gate = x.chunk(2, dim = -1)\n        return x * F.silu(gate)\ndef FeedForward(\n    dim,\n    mult = 4,"
        },
        {
            "comment": "The code defines a module that applies post-activation normalization. It also includes a nested Attention class that performs multi-head attention with optional causal masking and rotary embedding. The main components include layer normalization, dropout regularization, and linear transformations for dimensionality adjustments. The cosine similarity calculation is utilized if specified.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":816-857",
            "content": "    dropout = 0.,\n    post_activation_norm = False\n):\n    \"\"\" post-activation norm https://arxiv.org/abs/2110.09456 \"\"\"\n    inner_dim = int(mult * dim)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, inner_dim * 2, bias = False),\n        SwiGLU(),\n        LayerNorm(inner_dim) if post_activation_norm else nn.Identity(),\n        nn.Dropout(dropout),\n        nn.Linear(inner_dim, dim, bias = False)\n    )\n# attention\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        dropout = 0.,\n        causal = False,\n        rotary_emb = None,\n        cosine_sim = True,\n        cosine_sim_scale = 16\n    ):\n        super().__init__()\n        self.scale = cosine_sim_scale if cosine_sim else (dim_head ** -0.5)\n        self.cosine_sim = cosine_sim\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.causal = causal\n        self.norm = LayerNorm(dim)\n        self.dropout = nn.Dropout(dropout)\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))"
        },
        {
            "comment": "This code defines a self-attention layer for DALL\u00b7E 2, initializing linear layers and including the option to use rotary embeddings. It also allows for classifier free guidance by adding null key/value pairs and using cosine similarity if enabled.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":858-890",
            "content": "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n        self.rotary_emb = rotary_emb\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n    def forward(self, x, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n        x = self.norm(x)\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n        q = q * self.scale\n        # rotary embeddings\n        if exists(self.rotary_emb):\n            q, k = map(self.rotary_emb.rotate_queries_or_keys, (q, k))\n        # add null key / value for classifier free guidance in prior net\n        nk, nv = map(lambda t: repeat(t, 'd -> b 1 d', b = b), self.null_kv.unbind(dim = -2))\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n        # whether to use cosine sim\n        if self.cosine_sim:"
        },
        {
            "comment": "This code snippet performs multi-head attention by first normalizing the query and key tensors, calculating their similarities, adding relative positional encoding if available, masking irrelevant values based on a given mask, applying causal masking if specified, and finally computing the attention weights and aggregating the corresponding values.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":891-927",
            "content": "            q, k = map(l2norm, (q, k))\n        q, k = map(lambda t: t * math.sqrt(self.scale), (q, k))\n        # calculate query / key similarities\n        sim = einsum('b h i d, b j d -> b h i j', q, k)\n        # relative positional encoding (T5 style)\n        if exists(attn_bias):\n            sim = sim + attn_bias\n        # masking\n        max_neg_value = -torch.finfo(sim.dtype).max\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n        if self.causal:\n            i, j = sim.shape[-2:]\n            causal_mask = torch.ones((i, j), dtype = torch.bool, device = device).triu(j - i + 1)\n            sim = sim.masked_fill(causal_mask, max_neg_value)\n        # attention\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.type(sim.dtype)\n        attn = self.dropout(attn)\n        # aggregate values\n        out = einsum('b h i j, b j d -> b h i d', attn, v)"
        },
        {
            "comment": "This code defines a `CausalTransformer` class for natural language processing tasks. The class initializes several modules such as LayerNorm, RelPosBias, RotaryEmbedding, and Attention. It also includes a FeedForward layer with configurable parameters like `dim`, `depth`, `dim_head`, `heads`, `ff_mult`, `attn_dropout`, `ff_dropout`, `norm_in`, `norm_out`, `final_proj`, and `rotary_emb`. The code snippet you provided is responsible for rearranging the tensor dimensions and returning it after processing by the `CausalTransformer` model.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":929-960",
            "content": "        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\nclass CausalTransformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        ff_mult = 4,\n        norm_in = False,\n        norm_out = True,\n        attn_dropout = 0.,\n        ff_dropout = 0.,\n        final_proj = True,\n        normformer = False,\n        rotary_emb = True\n    ):\n        super().__init__()\n        self.init_norm = LayerNorm(dim) if norm_in else nn.Identity() # from latest BLOOM model and Yandex's YaLM\n        self.rel_pos_bias = RelPosBias(heads = heads)\n        rotary_emb = RotaryEmbedding(dim = min(32, dim_head)) if rotary_emb else None\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, causal = True, dim_head = dim_head, heads = heads, dropout = attn_dropout, rotary_emb = rotary_emb),\n                FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout, post_activation_norm = normformer)"
        },
        {
            "comment": "The code initializes a DiffusionPriorNetwork model with multiple layers, including attention and feed-forward modules. It also includes layer normalization and the option to project the output. The network takes in input of varying dimensions and can condition on time, image, and/or text embeddings. The self_cond parameter determines whether or not to use self-conditioning.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":961-992",
            "content": "            ]))\n        self.norm = LayerNorm(dim, stable = True) if norm_out else nn.Identity()  # unclear in paper whether they projected after the classic layer norm for the final denoised image embedding, or just had the transformer output it directly: plan on offering both options\n        self.project_out = nn.Linear(dim, dim, bias = False) if final_proj else nn.Identity()\n    def forward(self, x):\n        n, device = x.shape[1], x.device\n        x = self.init_norm(x)\n        attn_bias = self.rel_pos_bias(n, n + 1, device = device)\n        for attn, ff in self.layers:\n            x = attn(x, attn_bias = attn_bias) + x\n            x = ff(x) + x\n        out = self.norm(x)\n        return self.project_out(out)\nclass DiffusionPriorNetwork(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_timesteps = None,\n        num_time_embeds = 1,\n        num_image_embeds = 1,\n        num_text_embeds = 1,\n        max_text_len = 256,\n        self_cond = False,\n        **kwargs\n    ):\n        super().__init__()"
        },
        {
            "comment": "This code defines a class with parameters for dimensionality, number of time, image, and text embeddings. It initializes layers to transform input into text, time, and image embeddings. The \"learned_query\" is a learned parameter for the model.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":993-1016",
            "content": "        self.dim = dim\n        self.num_time_embeds = num_time_embeds\n        self.num_image_embeds = num_image_embeds\n        self.num_text_embeds = num_text_embeds\n        self.to_text_embeds = nn.Sequential(\n            nn.Linear(dim, dim * num_text_embeds) if num_text_embeds > 1 else nn.Identity(),\n            Rearrange('b (n d) -> b n d', n = num_text_embeds)\n        )\n        self.continuous_embedded_time = not exists(num_timesteps)\n        self.to_time_embeds = nn.Sequential(\n            nn.Embedding(num_timesteps, dim * num_time_embeds) if exists(num_timesteps) else nn.Sequential(SinusoidalPosEmb(dim), MLP(dim, dim * num_time_embeds)), # also offer a continuous version of timestep embeddings, with a 2 layer MLP\n            Rearrange('b (n d) -> b n d', n = num_time_embeds)\n        )\n        self.to_image_embeds = nn.Sequential(\n            nn.Linear(dim, dim * num_image_embeds) if num_image_embeds > 1 else nn.Identity(),\n            Rearrange('b (n d) -> b n d', n = num_image_embeds)\n        )\n        self.learned_query = nn.Parameter(torch.randn(dim))"
        },
        {
            "comment": "The code defines a model with a causal transformer and includes parameters for padding strategy, self-conditioning, and a function to perform forward calculations. The `forward_with_cond_scale` method takes conditional scaling as input and returns the scaled logits by combining original logits with null logits at 100% condition drop probabilities.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1017-1051",
            "content": "        self.causal_transformer = CausalTransformer(dim = dim, **kwargs)\n        # dalle1 learned padding strategy\n        self.max_text_len = max_text_len\n        self.null_text_encodings = nn.Parameter(torch.randn(1, max_text_len, dim))\n        self.null_text_embeds = nn.Parameter(torch.randn(1, num_text_embeds, dim))\n        self.null_image_embed = nn.Parameter(torch.randn(1, dim))\n        # whether to use self conditioning, Hinton's group's new ddpm technique\n        self.self_cond = self_cond\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n        if cond_scale == 1:\n            return logits\n        null_logits = self.forward(*args, text_cond_drop_prob = 1., image_cond_drop_prob = 1, **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n    def forward(\n        self,\n        image_embed,\n        diffusion_timesteps,\n        *,\n        text_embed,\n        text_encodings = None,"
        },
        {
            "comment": "This code initializes a model's parameters based on the given image_embed. It sets up self-conditioning if necessary, converts text and image embeddings to the appropriate format, and creates classifier free guidance masks for both text and image inputs. The model will use these embeddings and masks for prediction.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1052-1075",
            "content": "        self_cond = None,\n        text_cond_drop_prob = 0.,\n        image_cond_drop_prob = 0.\n    ):\n        batch, dim, device, dtype = *image_embed.shape, image_embed.device, image_embed.dtype\n        num_time_embeds, num_image_embeds, num_text_embeds = self.num_time_embeds, self.num_image_embeds, self.num_text_embeds\n        # setup self conditioning\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros(batch, self.dim, device = device, dtype = dtype))\n            self_cond = rearrange(self_cond, 'b d -> b 1 d')\n        # in section 2.2, last paragraph\n        # \"... consisting of encoded text, CLIP text embedding, diffusion timestep embedding, noised CLIP image embedding, final embedding for prediction\"\n        text_embed = self.to_text_embeds(text_embed)\n        image_embed = self.to_image_embeds(image_embed)\n        # classifier free guidance masks\n        text_keep_mask = prob_mask_like((batch,), 1 - text_cond_drop_prob, device = device)\n        text_keep_mask = rearrange(text_keep_mask, 'b -> b 1 1')"
        },
        {
            "comment": "This code snippet is preparing the input data for a DALL-E 2 model by handling text encodings. It creates an image_keep_mask, makes text encodings optional based on their existence, applies masking to remove padding or null encodings, and ensures that the length of text_encodings matches the expected maximum length.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1077-1102",
            "content": "        image_keep_mask = prob_mask_like((batch,), 1 - image_cond_drop_prob, device = device)\n        image_keep_mask = rearrange(image_keep_mask, 'b -> b 1 1')\n        # make text encodings optional\n        # although the paper seems to suggest it is present <--\n        if not exists(text_encodings):\n            text_encodings = torch.empty((batch, 0, dim), device = device, dtype = dtype)\n        mask = torch.any(text_encodings != 0., dim = -1)\n        # replace any padding in the text encodings with learned padding tokens unique across position\n        text_encodings = text_encodings[:, :self.max_text_len]\n        mask = mask[:, :self.max_text_len]\n        text_len = text_encodings.shape[-2]\n        remainder = self.max_text_len - text_len\n        if remainder > 0:\n            text_encodings = F.pad(text_encodings, (0, 0, 0, remainder), value = 0.)\n            mask = F.pad(mask, (0, remainder), value = False)\n        # mask out text encodings with null encodings\n        null_text_encodings = self.null_text_encodings.to(text_encodings.dtype)"
        },
        {
            "comment": "This code section is applying masking to text, image, and null embeddings based on the `text_keep_mask` and `image_keep_mask`. It uses these masks to decide which embeddings to keep or replace with null embeddings. The embeddings are also being converted to appropriate data types. Additionally, there's a conditional check for continuous embedded time.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1104-1133",
            "content": "        text_encodings = torch.where(\n            rearrange(mask, 'b n -> b n 1').clone() & text_keep_mask,\n            text_encodings,\n            null_text_encodings\n        )\n        # mask out text embeddings with null text embeddings\n        null_text_embeds = self.null_text_embeds.to(text_embed.dtype)\n        text_embed = torch.where(\n            text_keep_mask,\n            text_embed,\n            null_text_embeds\n        )\n        # mask out image embeddings with null image embeddings\n        null_image_embed = self.null_image_embed.to(image_embed.dtype)\n        image_embed = torch.where(\n            image_keep_mask,\n            image_embed,\n            null_image_embed\n        )\n        # whether text embedding is used for conditioning depends on whether text encodings are available for attention (for classifier free guidance, even though it seems from the paper it was not used in the prior ddpm, as the objective is different)\n        # but let's just do it right\n        if self.continuous_embedded_time:"
        },
        {
            "comment": "The code defines a DiffusionPrior class that takes in various inputs such as text encodings, timesteps, and image embeddings. It applies causal transformer to learn the learned_query, which predicts the image embedding per DDPM timestep. The text_cond_drop_prob parameter is optional and if provided, will dropout the text conditioning with a specified probability.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1134-1173",
            "content": "            diffusion_timesteps = diffusion_timesteps.type(dtype)\n        time_embed = self.to_time_embeds(diffusion_timesteps)\n        learned_queries = repeat(self.learned_query, 'd -> b 1 d', b = batch)\n        if self.self_cond:\n            learned_queries = torch.cat((self_cond, learned_queries), dim = -2)\n        tokens = torch.cat((\n            text_encodings,\n            text_embed,\n            time_embed,\n            image_embed,\n            learned_queries\n        ), dim = -2)\n        # attend\n        tokens = self.causal_transformer(tokens)\n        # get learned query, which should predict the image embedding (per DDPM timestep)\n        pred_image_embed = tokens[..., -1, :]\n        return pred_image_embed\nclass DiffusionPrior(nn.Module):\n    def __init__(\n        self,\n        net,\n        *,\n        clip = None,\n        image_embed_dim = None,\n        image_size = None,\n        image_channels = 3,\n        timesteps = 1000,\n        sample_timesteps = None,\n        cond_drop_prob = 0.,\n        text_cond_drop_prob = None,"
        },
        {
            "comment": "This code snippet initializes a DALLE2 model with various optional parameters for training and sampling. These include loss type, conditioning on text encodings, clamping of image embeddings, scaling the L2-normed image embedding, and adapter overrides for CLIP adapter integration.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1174-1187",
            "content": "        image_cond_drop_prob = None,\n        loss_type = \"l2\",\n        predict_x_start = True,\n        predict_v = False,\n        beta_schedule = \"cosine\",\n        condition_on_text_encodings = True,  # the paper suggests this is needed, but you can turn it off for your CLIP preprocessed text embed -> image embed training\n        sampling_clamp_l2norm = False,       # whether to l2norm clamp the image embed at each denoising iteration (analogous to -1 to 1 clipping for usual DDPMs)\n        sampling_final_clamp_l2norm = False, # whether to l2norm the final image embedding output (this is also done for images in ddpm)\n        training_clamp_l2norm = False,\n        init_image_embed_l2norm = False,\n        image_embed_scale = None,            # this is for scaling the l2-normed image embedding, so it is more suitable for gaussian diffusion, as outlined by Katherine (@crowsonkb) https://github.com/lucidrains/DALLE2-pytorch/issues/60#issue-1226116132\n        clip_adapter_overrides = dict()\n    ):\n        super().__init__()"
        },
        {
            "comment": "The code is initializing an instance of a model. It sets the sample_timesteps, creates a NoiseScheduler object with specified parameters, checks if CLIP is provided and adapts it if necessary, sets the image_embed_dim if not given, and assigns the network architecture (net) to be used.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1189-1213",
            "content": "        self.sample_timesteps = sample_timesteps\n        self.noise_scheduler = NoiseScheduler(\n            beta_schedule = beta_schedule,\n            timesteps = timesteps,\n            loss_type = loss_type\n        )\n        if exists(clip):\n            assert image_channels == clip.image_channels, f'channels of image ({image_channels}) should be equal to the channels that CLIP accepts ({clip.image_channels})'\n            if isinstance(clip, CLIP):\n                clip = XClipAdapter(clip, **clip_adapter_overrides)\n            elif isinstance(clip, CoCa):\n                clip = CoCaAdapter(clip, **clip_adapter_overrides)\n            assert isinstance(clip, BaseClipAdapter)\n            freeze_model_and_make_eval_(clip)\n            self.clip = clip\n        else:\n            assert exists(image_embed_dim), 'latent dimension must be given, if training prior network without CLIP given'\n            self.clip = None\n        self.net = net\n        self.image_embed_dim = default(image_embed_dim, lambda: clip.dim_latent)"
        },
        {
            "comment": "The code asserts that the diffusion prior network dimension and the image embedding dimension are consistent, and checks if a CLIP is passed in with correct latent dimensions. It also sets channels, text conditional drop probability, image conditional drop probability, enables classifier guidance if probabilities are greater than 0, and conditions on text encodings. It offers both options to predict noise or x0 directly for image embedding as per the paper's claim of better results.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1215-1226",
            "content": "        assert net.dim == self.image_embed_dim, f'your diffusion prior network has a dimension of {net.dim}, but you set your image embedding dimension (keyword image_embed_dim) on DiffusionPrior to {self.image_embed_dim}'\n        assert not exists(clip) or clip.dim_latent == self.image_embed_dim, f'you passed in a CLIP to the diffusion prior with latent dimensions of {clip.dim_latent}, but your image embedding dimension (keyword image_embed_dim) for the DiffusionPrior was set to {self.image_embed_dim}'\n        self.channels = default(image_channels, lambda: clip.image_channels)\n        self.text_cond_drop_prob = default(text_cond_drop_prob, cond_drop_prob)\n        self.image_cond_drop_prob = default(image_cond_drop_prob, cond_drop_prob)\n        self.can_classifier_guidance = self.text_cond_drop_prob > 0. and self.image_cond_drop_prob > 0.\n        self.condition_on_text_encodings = condition_on_text_encodings\n        # in paper, they do not predict the noise, but predict x0 directly for image embedding, claiming empirically better results. I'll just offer both."
        },
        {
            "comment": "The code sets various parameters and properties for an object, including predict_x_start, image_embed_scale, sampling_clamp_l2norm, etc. It also defines the l2norm_clamp_embed function and p_mean_variance function. The device property retrieves the device used by the object, and there's a register_buffer for tracking device usage.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1228-1254",
            "content": "        self.predict_x_start = predict_x_start\n        self.predict_v = predict_v # takes precedence over predict_x_start\n        # @crowsonkb 's suggestion - https://github.com/lucidrains/DALLE2-pytorch/issues/60#issue-1226116132\n        self.image_embed_scale = default(image_embed_scale, self.image_embed_dim ** 0.5)\n        # whether to force an l2norm, similar to clipping denoised, when sampling\n        self.sampling_clamp_l2norm = sampling_clamp_l2norm\n        self.sampling_final_clamp_l2norm = sampling_final_clamp_l2norm\n        self.training_clamp_l2norm = training_clamp_l2norm\n        self.init_image_embed_l2norm = init_image_embed_l2norm\n        # device tracker\n        self.register_buffer('_dummy', torch.tensor([True]), persistent = False)\n    @property\n    def device(self):\n        return self._dummy.device\n    def l2norm_clamp_embed(self, image_embed):\n        return l2norm(image_embed) * self.image_embed_scale\n    def p_mean_variance(self, x, t, text_cond, self_cond = None, clip_denoised = False, cond_scale = 1.):"
        },
        {
            "comment": "This code asserts that the model was not trained with conditional dropout, preventing classifier free guidance if cond_scale is anything other than 1. It then calculates and returns the model mean, posterior variance, posterior log variance, and x_start depending on different conditions.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1255-1273",
            "content": "        assert not (cond_scale != 1. and not self.can_classifier_guidance), 'the model was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n        pred = self.net.forward_with_cond_scale(x, t, cond_scale = cond_scale, self_cond = self_cond, **text_cond)\n        if self.predict_v:\n            x_start = self.noise_scheduler.predict_start_from_v(x, t = t, v = pred)\n        elif self.predict_x_start:\n            x_start = pred\n        else:\n            x_start = self.noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n        if clip_denoised and not self.predict_x_start:\n            x_start.clamp_(-1., 1.)\n        if self.predict_x_start and self.sampling_clamp_l2norm:\n            x_start = l2norm(x_start) * self.image_embed_scale\n        model_mean, posterior_variance, posterior_log_variance = self.noise_scheduler.q_posterior(x_start=x_start, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance, x_start"
        },
        {
            "comment": "This code defines the `p_sample` and `p_sample_loop_ddpm` functions. `p_sample` takes input, generates a model mean and log variance, applies noise based on whether t is zero or not, and returns the prediction and x_start. `p_sample_loop_ddpm` initializes an image embedding, optionally normalizes it, and iterates through a reversed range to perform some unspecified operation for each iteration. The code uses PyTorch's `@torch.no_grad()` decorator to disable gradient computation during these functions' execution.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1275-1295",
            "content": "    @torch.no_grad()\n    def p_sample(self, x, t, text_cond = None, self_cond = None, clip_denoised = True, cond_scale = 1.):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = t, text_cond = text_cond, self_cond = self_cond, clip_denoised = clip_denoised, cond_scale = cond_scale)\n        noise = torch.randn_like(x)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n        return pred, x_start\n    @torch.no_grad()\n    def p_sample_loop_ddpm(self, shape, text_cond, cond_scale = 1.):\n        batch, device = shape[0], self.device\n        image_embed = torch.randn(shape, device = device)\n        x_start = None # for self-conditioning\n        if self.init_image_embed_l2norm:\n            image_embed = l2norm(image_embed) * self.image_embed_scale\n        for i in tqdm(reversed(range("
        },
        {
            "comment": "The code defines the `p_sample` function which samples images and their corresponding embeddings using a loop over time steps. It also includes an optional L2-norm clamping for final image embedding. The `p_sample_loop_ddim` function is a helper method to define shape, times, and time pairs for the sampling loop in DDIM style.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1295-1315",
            "content": "0, self.noise_scheduler.num_timesteps)), desc='sampling loop time step', total=self.noise_scheduler.num_timesteps):\n            times = torch.full((batch,), i, device = device, dtype = torch.long)\n            self_cond = x_start if self.net.self_cond else None\n            image_embed, x_start = self.p_sample(image_embed, times, text_cond = text_cond, self_cond = self_cond, cond_scale = cond_scale)\n        if self.sampling_final_clamp_l2norm and self.predict_x_start:\n            image_embed = self.l2norm_clamp_embed(image_embed)\n        return image_embed\n    @torch.no_grad()\n    def p_sample_loop_ddim(self, shape, text_cond, *, timesteps, eta = 1., cond_scale = 1.):\n        batch, device, alphas, total_timesteps = shape[0], self.device, self.noise_scheduler.alphas_cumprod_prev, self.noise_scheduler.num_timesteps\n        times = torch.linspace(-1., total_timesteps, steps = timesteps + 1)[:-1]\n        times = list(reversed(times.int().tolist()))\n        time_pairs = list(zip(times[:-1], times[1:]))\n        image_embed = torch.randn(shape, device = device)"
        },
        {
            "comment": "The code is iterating through time pairs, calculating alpha values and performing a forward pass in the neural network. It also adjusts x_start based on prediction methods and performs noise scheduling. The purpose seems to be generating an image using conditional sampling with self-conditioning and considering different prediction methods for x_start.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1317-1341",
            "content": "        x_start = None # for self-conditioning\n        if self.init_image_embed_l2norm:\n            image_embed = l2norm(image_embed) * self.image_embed_scale\n        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n            alpha = alphas[time]\n            alpha_next = alphas[time_next]\n            time_cond = torch.full((batch,), time, device = device, dtype = torch.long)\n            self_cond = x_start if self.net.self_cond else None\n            pred = self.net.forward_with_cond_scale(image_embed, time_cond, self_cond = self_cond, cond_scale = cond_scale, **text_cond)\n            # derive x0\n            if self.predict_v:\n                x_start = self.noise_scheduler.predict_start_from_v(image_embed, t = time_cond, v = pred)\n            elif self.predict_x_start:\n                x_start = pred\n            else:\n                x_start = self.noise_scheduler.predict_start_from_noise(image_embed, t = time_cond, noise = pred)\n            # clip x0 before maybe predicting noise"
        },
        {
            "comment": "In this code segment, it checks if predicting x_start is enabled and performs L2-norm clamping if necessary. It then predicts noise using the noise scheduler based on image embeddings, time condition, and x_start. If time_next is less than 0, it sets image_embed to x_start. Calculates coefficients c1 and c2 for RNN sampling and generates noise accordingly. Combines these elements to generate the final image_embed which is then optionally L2-norm clamped if enabled.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1343-1371",
            "content": "            if not self.predict_x_start:\n                x_start.clamp_(-1., 1.)\n            if self.predict_x_start and self.sampling_clamp_l2norm:\n                x_start = self.l2norm_clamp_embed(x_start)\n            # predict noise\n            pred_noise = self.noise_scheduler.predict_noise_from_start(image_embed, t = time_cond, x0 = x_start)\n            if time_next < 0:\n                image_embed = x_start\n                continue\n            c1 = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n            c2 = ((1 - alpha_next) - torch.square(c1)).sqrt()\n            noise = torch.randn_like(image_embed) if time_next > 0 else 0.\n            image_embed = x_start * alpha_next.sqrt() + \\\n                          c1 * noise + \\\n                          c2 * pred_noise\n        if self.predict_x_start and self.sampling_final_clamp_l2norm:\n            image_embed = self.l2norm_clamp_embed(image_embed)\n        return image_embed\n    @torch.no_grad()\n    def p_sample_loop(self, *args, timesteps = None, **kwargs):"
        },
        {
            "comment": "This code is from the DALLE2-pytorch model. It first determines if the timesteps are less than the number of timesteps in the noise scheduler. If so, it uses the p_sample_loop_ddim function to get the normalized image embeddings, otherwise it uses the p_sample_loop_ddpm function. The code then scales the normalized image embeddings by the image_embed_scale and returns the scaled embeddings. The p_losses function generates a noisy version of the input image embedding using the noise scheduler, and optionally conditions the model with self-conditioning if the condition is met. Finally, it passes the noisy embedding to the network for prediction.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1372-1395",
            "content": "        timesteps = default(timesteps, self.noise_scheduler.num_timesteps)\n        assert timesteps <= self.noise_scheduler.num_timesteps\n        is_ddim = timesteps < self.noise_scheduler.num_timesteps\n        if not is_ddim:\n            normalized_image_embed = self.p_sample_loop_ddpm(*args, **kwargs)\n        else:\n            normalized_image_embed = self.p_sample_loop_ddim(*args, **kwargs, timesteps = timesteps)\n        image_embed = normalized_image_embed / self.image_embed_scale\n        return image_embed\n    def p_losses(self, image_embed, times, text_cond, noise = None):\n        noise = default(noise, lambda: torch.randn_like(image_embed))\n        image_embed_noisy = self.noise_scheduler.q_sample(x_start = image_embed, t = times, noise = noise)\n        self_cond = None\n        if self.net.self_cond and random.random() < 0.5:\n            with torch.no_grad():\n                self_cond = self.net(image_embed_noisy, times, **text_cond).detach()\n        pred = self.net(\n            image_embed_noisy,"
        },
        {
            "comment": "The code defines a method for predicting and calculating loss. It takes in parameters such as times, self_cond, text_cond_drop_prob, image_cond_drop_prob, and text_cond. If certain conditions are met, it performs l2norm clamping on the prediction, sets the target based on whether to predict x or v, then calculates the loss using the noise scheduler's loss function. The code also includes a sample_batch_size method that samples an image batch and iterates over time steps in reverse order for some processing.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1396-1424",
            "content": "            times,\n            self_cond = self_cond,\n            text_cond_drop_prob = self.text_cond_drop_prob,\n            image_cond_drop_prob = self.image_cond_drop_prob,\n            **text_cond\n        )\n        if self.predict_x_start and self.training_clamp_l2norm:\n            pred = self.l2norm_clamp_embed(pred)\n        if self.predict_v:\n            target = self.noise_scheduler.calculate_v(image_embed, times, noise)\n        elif self.predict_x_start:\n            target = image_embed\n        else:\n            target = noise\n        loss = self.noise_scheduler.loss_fn(pred, target)\n        return loss\n    @torch.no_grad()\n    @eval_decorator\n    def sample_batch_size(self, batch_size, text_cond, cond_scale = 1.):\n        device = self.betas.device\n        shape = (batch_size, self.image_embed_dim)\n        img = torch.randn(shape, device = device)\n        for i in tqdm(reversed(range(0, self.noise_scheduler.num_timesteps)), desc = 'sampling loop time step', total = self.noise_scheduler.num_timesteps):"
        },
        {
            "comment": "This code is part of a DALL-E 2 model implementation in PyTorch. The sample function generates multiple image embeddings based on provided text, then chooses the most similar one according to CLIP's similarity judgment. The function uses a p_sample_loop method which takes timesteps as input and returns a batch of images with the specified size.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1425-1453",
            "content": "            img = self.p_sample(img, torch.full((batch_size,), i, device = device, dtype = torch.long), text_cond = text_cond, cond_scale = cond_scale)\n        return img\n    @torch.no_grad()\n    @eval_decorator\n    def sample(\n        self,\n        text,\n        num_samples_per_batch = 2,\n        cond_scale = 1.,\n        timesteps = None\n    ):\n        timesteps = default(timesteps, self.sample_timesteps)\n        # in the paper, what they did was\n        # sample 2 image embeddings, choose the top 1 similarity, as judged by CLIP\n        text = repeat(text, 'b ... -> (b r) ...', r = num_samples_per_batch)\n        batch_size = text.shape[0]\n        image_embed_dim = self.image_embed_dim\n        text_embed, text_encodings = self.clip.embed_text(text)\n        text_cond = dict(text_embed = text_embed)\n        if self.condition_on_text_encodings:\n            text_cond = {**text_cond, 'text_encodings': text_encodings}\n        image_embeds = self.p_sample_loop((batch_size, image_embed_dim), text_cond = text_cond, cond_scale = cond_scale, timesteps = timesteps)"
        },
        {
            "comment": "This function retrieves the original unscaled image embeddings from the input, rearranges them based on the number of samples per batch, calculates text-image similarities using Euclidean distance, gets the top indices and gathers corresponding embeddings. It allows for training on preprocessed CLIP text and image embeddings or CLIP text encodings. If neither text nor text embedding is supplied, an assertion error will be raised.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1455-1480",
            "content": "        # retrieve original unscaled image embed\n        text_embeds = text_cond['text_embed']\n        text_embeds = rearrange(text_embeds, '(b r) d -> b r d', r = num_samples_per_batch)\n        image_embeds = rearrange(image_embeds, '(b r) d -> b r d', r = num_samples_per_batch)\n        text_image_sims = einsum('b r d, b r d -> b r', l2norm(text_embeds), l2norm(image_embeds))\n        top_sim_indices = text_image_sims.topk(k = 1).indices\n        top_sim_indices = repeat(top_sim_indices, 'b 1 -> b 1 d', d = image_embed_dim)\n        top_image_embeds = image_embeds.gather(1, top_sim_indices)\n        return rearrange(top_image_embeds, 'b 1 d -> b d')\n    def forward(\n        self,\n        text = None,\n        image = None,\n        text_embed = None,      # allow for training on preprocessed CLIP text and image embeddings\n        image_embed = None,\n        text_encodings = None,  # as well as CLIP text encodings\n        *args,\n        **kwargs\n    ):\n        assert exists(text) ^ exists(text_embed), 'either text or text embedding must be supplied'"
        },
        {
            "comment": "The code snippet checks if an image or image embedding is supplied and throws an error if neither exists. It also verifies the presence of text encodings or text based on the specified conditioning during initialization. The code then calculates the text embeddings from the given text using the clip model. If conditioned on text encodings, it includes them in the text_cond dictionary. It samples random times for timestep conditioning from the noise scheduler and scales the image embed (by Katherine).",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1481-1503",
            "content": "        assert exists(image) ^ exists(image_embed), 'either image or image embedding must be supplied'\n        assert not (self.condition_on_text_encodings and (not exists(text_encodings) and not exists(text))), 'text encodings must be present if you specified you wish to condition on it on initialization'\n        if exists(image):\n            image_embed, _ = self.clip.embed_image(image)\n        # calculate text conditionings, based on what is passed in\n        if exists(text):\n            text_embed, text_encodings = self.clip.embed_text(text)\n        text_cond = dict(text_embed = text_embed)\n        if self.condition_on_text_encodings:\n            assert exists(text_encodings), 'text encodings must be present for diffusion prior if specified'\n            text_cond = {**text_cond, 'text_encodings': text_encodings}\n        # timestep conditioning from ddpm\n        batch, device = image_embed.shape[0], image_embed.device\n        times = self.noise_scheduler.sample_random_times(batch)\n        # scale image embed (Katherine)"
        },
        {
            "comment": "This code contains two classes, `NearestUpsample` and `PixelShuffleUpsample`. `NearestUpsample` performs nearest neighbor upsampling followed by a convolution operation. `PixelShuffleUpsample` applies pixel shuffling after a 1x1 convolution to reduce checkerboard artifacts. Both classes can be used for image upsampling tasks.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1505-1542",
            "content": "        image_embed *= self.image_embed_scale\n        # calculate forward loss\n        return self.p_losses(image_embed, times, text_cond = text_cond, *args, **kwargs)\n# decoder\ndef NearestUpsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n        self.init_conv_(conv)\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)"
        },
        {
            "comment": "The code defines a class called WeightStandardizedConv2d that extends nn.Conv2d and implements weight standardization for improving synergy with group normalization. It also includes a function named Downsample to downsample the input using pixel unshuffle technique, which is optimal according to a reference paper. The forward method in WeightStandardizedConv2d calculates mean and variance of flattened weights and performs weight standardization before applying convolution operations.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1543-1573",
            "content": "        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n    def forward(self, x):\n        return self.net(x)\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\nclass WeightStandardizedConv2d(nn.Conv2d):\n    \"\"\"\n    https://arxiv.org/abs/1903.10520\n    weight standardization purportedly works synergistically with group normalization\n    \"\"\"\n    def forward(self, x):\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        weight = self.weight\n        flattened_weights = rearrange(weight, 'o ... -> o (...)')\n        mean = reduce(weight, 'o ... -> o 1 1 1', 'mean')\n        var = torch.var(flattened_weights, dim = -1, unbiased = False)"
        },
        {
            "comment": "This code snippet contains the definition of three classes: `rearrange`, `SinusoidalPosEmb`, and `Block`. The `rearrange` function is used to reshape tensors, `SinusoidalPosEmb` class computes sinusoidal positional embeddings, and `Block` class defines a convolutional block with an option for weight standardization.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1574-1604",
            "content": "        var = rearrange(var, 'o -> o 1 1 1')\n        weight = (weight - mean) * (var + eps).rsqrt()\n        return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n    def forward(self, x):\n        dtype, device = x.dtype, x.device\n        assert is_float_dtype(dtype), 'input to sinusoidal pos emb must be a float type'\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = device, dtype = dtype) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1).type(dtype)\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        weight_standardization = False\n    ):\n        super().__init__()\n        conv_klass = nn.Conv2d if not weight_standardization else WeightStandardizedConv2d"
        },
        {
            "comment": "This code snippet defines a ResnetBlock class that takes in dimensions and other parameters for its initialization. It includes a project layer, normalization layer, activation function, and optional scale-shift operation. The forward method performs the computation steps involving these layers. Additionally, it checks if time_cond_dim is given to initialize a time MLP and if cond_dim exists to initialize a cross-attention layer.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1606-1648",
            "content": "        self.project = conv_klass(dim, dim_out, 3, padding = 1)\n        self.norm = nn.GroupNorm(groups, dim_out)\n        self.act = nn.SiLU()\n    def forward(self, x, scale_shift = None):\n        x = self.project(x)\n        x = self.norm(x)\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n        x = self.act(x)\n        return x\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        weight_standardization = False,\n        cosine_sim_cross_attn = False\n    ):\n        super().__init__()\n        self.time_mlp = None\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n        self.cross_attn = None\n        if exists(cond_dim):\n            self.cross_attn = CrossAttention(\n                dim = dim_out,\n                context_dim = cond_dim,"
        },
        {
            "comment": "This code defines a class for an encoder-decoder architecture with residual connections and cross-attention. It includes blocks, convolutions, time MLP, and optional cross-attention with conditional input. The forward method processes the input, applies blocks, optionally performs time embedding and cross-attention, and returns the output.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1649-1675",
            "content": "                cosine_sim = cosine_sim_cross_attn\n            )\n        self.block1 = Block(dim, dim_out, groups = groups, weight_standardization = weight_standardization)\n        self.block2 = Block(dim_out, dim_out, groups = groups, weight_standardization = weight_standardization)\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n    def forward(self, x, time_emb = None, cond = None):\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n        h = self.block1(x, scale_shift = scale_shift)\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = rearrange(h, 'b c ... -> b ... c')\n            h, ps = pack([h], 'b * c')\n            h = self.cross_attn(h, context = cond) + h\n            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b ... c -> b c ...')"
        },
        {
            "comment": "The code defines a CrossAttention class with parameters for dimensionality, context dimension, number of heads, dropout rate, and normalization options. It initializes the necessary layers including linear transformations and layer norms. The cosine similarity scale and null keys are also defined.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1677-1710",
            "content": "        h = self.block2(h)\n        return h + self.res_conv(x)\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        dropout = 0.,\n        norm_context = False,\n        cosine_sim = False,\n        cosine_sim_scale = 16\n    ):\n        super().__init__()\n        self.cosine_sim = cosine_sim\n        self.scale = cosine_sim_scale if cosine_sim else (dim_head ** -0.5)\n        self.heads = heads\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, dim)\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else nn.Identity()\n        self.dropout = nn.Dropout(dropout)\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),"
        },
        {
            "comment": "This function defines a multi-head attention layer. It normalizes input x and context, splits them into queries (q), keys (k), and values (v). It also includes null key/value pairs for classifier free guidance in the prior net. If cosine_sim is set, it normalizes q and k again. It then computes the attention scores (sim) between q and k, and applies a mask if available, replacing negative values with max_neg_value.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1711-1742",
            "content": "            LayerNorm(dim)\n        )\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n        x = self.norm(x)\n        context = self.norm_context(context)\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), (q, k, v))\n        # add null key / value for classifier free guidance in prior net\n        nk, nv = map(lambda t: repeat(t, 'd -> b h 1 d', h = self.heads,  b = b), self.null_kv.unbind(dim = -2))\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n        if self.cosine_sim:\n            q, k = map(l2norm, (q, k))\n        q, k = map(lambda t: t * math.sqrt(self.scale), (q, k))\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n        max_neg_value = -torch.finfo(sim.dtype).max\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)"
        },
        {
            "comment": "This code defines a LinearAttention module that performs multi-head attention. It normalizes the input, applies convolutions to split input into queries (Q), keys (K), and values (V), then computes attention weights, rearranges output dimensions for efficiency, and finally passes the result through another set of convolutions before returning it.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1744-1779",
            "content": "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.type(sim.dtype)\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n        self.nonlin = nn.GELU()\n        self.to_qkv = nn.Conv2d(dim, inner_dim * 3, 1, bias = False)\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n    def forward(self, fmap):\n        h, x, y = self.heads, *fmap.shape[-2:]\n        seq_len = x * y\n        fmap = self.norm(fmap)\n        q, k, v = self.to_qkv(fmap).chunk(3, dim = 1)\n        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h = h), (q, k, v))"
        },
        {
            "comment": "The code calculates and applies attention weights to query (q) and key (k) tensors, normalizes them, scales the vectors, and performs element-wise multiplication. It then applies a linear transformation (nonlin) on the result and rearranges the dimensions of the output tensor using the 'rearrange' function. The code also defines a CrossEmbedLayer class that initializes convolutional layers for feature extraction at multiple scales.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1781-1815",
            "content": "        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n        q = q * self.scale\n        v = l2norm(v)\n        k, v = map(lambda t: t / math.sqrt(seq_len), (k, v))\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n        out = self.nonlin(out)\n        return self.to_out(out)\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n        self.convs = nn.ModuleList([])"
        },
        {
            "comment": "The code defines a convolutional network with adjustable kernel sizes and applies an upsampling combiner to combine feature maps. The enabled flag controls whether the upsampling combiner is active, and it can be customized with different input/output dimensions.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1816-1848",
            "content": "        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\nclass UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()\n        assert len(dim_ins) == len(dim_outs)\n        self.enabled = enabled\n        if not self.enabled:\n            self.dim_out = dim\n            return\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n    def forward(self, x, fmaps = None):\n        target_size = x.shape[-1]\n        fmaps = default(fmaps, tuple())\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:"
        },
        {
            "comment": "This code defines a Unet model with multiple components including fmaps, convolutions, image and text embeddings, dimensions, conditional parameters, and attention mechanisms. It also includes options for lowres_cond, self_attn, lowres_noise_cond, sparse_attn, and cosine_sim_cross_attn.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1849-1876",
            "content": "            return x\n        fmaps = [resize_image_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n        return torch.cat((x, *outs), dim = 1)\nclass Unet(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        image_embed_dim = None,\n        text_embed_dim = None,\n        cond_dim = None,\n        num_image_tokens = 4,\n        num_time_tokens = 2,\n        out_dim = None,\n        dim_mults=(1, 2, 4, 8),\n        channels = 3,\n        channels_out = None,\n        self_attn = False,\n        attn_dim_head = 32,\n        attn_heads = 16,\n        lowres_cond = False,             # for cascading diffusion - https://cascaded-diffusion.github.io/\n        lowres_noise_cond = False,       # for conditioning on low resolution noising, based on Imagen\n        self_cond = False,               # set this to True to use the self-conditioning technique from - https://arxiv.org/abs/2208.04202\n        sparse_attn = False,\n        cosine_sim_cross_attn = False,"
        },
        {
            "comment": "The code defines various settings for the DALLE2 model, including whether to use cosine similarity self-attention, if a layer of attention should be at the bottleneck, and whether to condition on text or image embeddings. It also includes options for initializing embeddings, resnet blocks, cross embeddings, and more. These settings allow for customization and optimization in the DALLE2 model's architecture.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1877-1894",
            "content": "        cosine_sim_self_attn = False,\n        attend_at_middle = True,         # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n        cond_on_text_encodings = False,\n        max_text_len = 256,\n        cond_on_image_embeds = False,\n        add_image_embeds_to_time = True, # alerted by @mhh0318 to a phrase in the paper - \"Specifically, we modify the architecture described in Nichol et al. (2021) by projecting and adding CLIP embeddings to the existing timestep embedding\"\n        init_dim = None,\n        init_conv_kernel_size = 7,\n        resnet_groups = 8,\n        resnet_weight_standardization = False,\n        num_resnet_blocks = 2,\n        init_cross_embed = True,\n        init_cross_embed_kernel_sizes = (3, 7, 15),\n        cross_embed_downsample = False,\n        cross_embed_downsample_kernel_sizes = (2, 4),\n        memory_efficient = False,\n        scale_skip_connection = False,\n        pixel_shuffle_upsample = True,"
        },
        {
            "comment": "The code initializes a DDPM model with specified parameters such as number of channels, output channels, low resolution conditioning and self-conditioning. It determines the dimensions and initial number of channels based on these inputs and saves the hyperparameters for possible cascading DDPM in the future.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1895-1926",
            "content": "        final_conv_kernel_size = 1,\n        combine_upsample_fmaps = False, # whether to combine the outputs of all upsample blocks, as in unet squared paper\n        checkpoint_during_training = False,\n        **kwargs\n    ):\n        super().__init__()\n        # save locals to take care of some hyperparameters for cascading DDPM\n        self._locals = locals()\n        del self._locals['self']\n        del self._locals['__class__']\n        # for eventual cascading diffusion\n        self.lowres_cond = lowres_cond\n        # whether to do self conditioning\n        self.self_cond = self_cond\n        # determine dimensions\n        self.channels = channels\n        self.channels_out = default(channels_out, channels)\n         # initial number of channels depends on\n         # (1) low resolution conditioning from cascading ddpm paper, conditioned on previous unet output in the cascade\n         # (2) self conditioning (bit diffusion paper)\n        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n        init_dim = default(init_dim, dim)"
        },
        {
            "comment": "This code initializes layers for processing time and image inputs. It creates a CrossEmbedLayer or Conv2d layer for the initial input, sets the dimensions for subsequent stages, defines layers to transform time-based data into conditioning tokens, and initializes an image-to-tokens sequence of layers. These layers will be used in a DALL-E 2 model for processing text, image, and time-based inputs for generating images.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1928-1955",
            "content": "        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else nn.Conv2d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n        num_stages = len(in_out)\n        # time, image embeddings, and optional text encoding\n        cond_dim = default(cond_dim, dim)\n        time_cond_dim = dim * 4\n        self.to_time_hiddens = nn.Sequential(\n            SinusoidalPosEmb(dim),\n            nn.Linear(dim, time_cond_dim),\n            nn.GELU()\n        )\n        self.to_time_tokens = nn.Sequential(\n            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n        )\n        self.to_time_cond = nn.Sequential(\n            nn.Linear(time_cond_dim, time_cond_dim)\n        )\n        self.image_to_tokens = nn.Sequential("
        },
        {
            "comment": "The code defines the architecture of a model. It includes linear layers, layer normalization, GELU activation function, and conditioning options for image embeddings, text encodings, and low resolution noise. These components are used to transform inputs and generate conditions based on optional parameters.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1956-1980",
            "content": "            nn.Linear(image_embed_dim, cond_dim * num_image_tokens),\n            Rearrange('b (n d) -> b n d', n = num_image_tokens)\n        ) if cond_on_image_embeds and image_embed_dim != cond_dim else nn.Identity()\n        self.to_image_hiddens = nn.Sequential(\n            nn.Linear(image_embed_dim, time_cond_dim),\n            nn.GELU()\n        ) if cond_on_image_embeds and add_image_embeds_to_time else None\n        self.norm_cond = nn.LayerNorm(cond_dim)\n        self.norm_mid_cond = nn.LayerNorm(cond_dim)\n        # text encoding conditioning (optional)\n        self.text_to_cond = None\n        self.text_embed_dim = None\n        if cond_on_text_encodings:\n            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text_encodings is True'\n            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n            self.text_embed_dim = text_embed_dim\n        # low resolution noise conditiong, based on Imagen's upsampler training technique\n        self.lowres_noise_cond = lowres_noise_cond"
        },
        {
            "comment": "This code initializes various components of a model. It creates an optional sequential layer for low-res noise conditioning based on a flag, allows fine control over whether to condition on image embeddings and text encodings, and sets up parameters for classifier-free guidance. The skip connection scale is set either to 1 or scaled as per Imagen's approach.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":1982-2005",
            "content": "        self.to_lowres_noise_cond = nn.Sequential(\n            SinusoidalPosEmb(dim),\n            nn.Linear(dim, time_cond_dim),\n            nn.GELU(),\n            nn.Linear(time_cond_dim, time_cond_dim)\n        ) if lowres_noise_cond else None\n        # finer control over whether to condition on image embeddings and text encodings\n        # so one can have the latter unets in the cascading DDPMs only focus on super-resoluting\n        self.cond_on_text_encodings = cond_on_text_encodings\n        self.cond_on_image_embeds = cond_on_image_embeds\n        # for classifier free guidance\n        self.null_image_embed = nn.Parameter(torch.randn(1, num_image_tokens, cond_dim))\n        self.null_image_hiddens = nn.Parameter(torch.randn(1, time_cond_dim))\n        self.max_text_len = max_text_len\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        # whether to scale skip connection, adopted in Imagen\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)"
        },
        {
            "comment": "This code initializes various parameters and classes for the DALL-E 2 model. It sets up attention, resnet block, downsampling, and upsampling functions based on user inputs. The code uses partial function applications to customize the resnet blocks and other components according to specific settings.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2007-2034",
            "content": "        # attention related params\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head, cosine_sim = cosine_sim_self_attn)\n        self_attn = cast_tuple(self_attn, num_stages)\n        create_self_attn = lambda dim: RearrangeToSequence(Residual(Attention(dim, **attn_kwargs)))\n        # resnet block klass\n        resnet_groups = cast_tuple(resnet_groups, num_stages)\n        top_level_resnet_group = first(resnet_groups)\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_stages)\n        # downsample klass\n        downsample_klass = Downsample\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n        # upsample klass\n        upsample_klass = NearestUpsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n        # prepare resnet klass\n        resnet_block = partial(ResnetBlock, cosine_sim_cross_attn = cosine_sim_cross_attn, weight_standardization = resnet_weight_standardization)"
        },
        {
            "comment": "The code initializes the memory efficient UNet with an initial resnet block, and creates two lists for downsampling and upsampling layers. It also keeps track of skip connection dimensions and dimensions for final upsample feature map combiner. The code iterates over different layer configurations, including whether to use self-attention or not.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2036-2058",
            "content": "        # give memory efficient unet an initial resnet block\n        self.init_resnet_block = resnet_block(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = top_level_resnet_group) if memory_efficient else None\n        # layers\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n        skip_connect_dims = []          # keeping track of skip connection dimensions\n        upsample_combiner_dims = []     # keeping track of dimensions for final upsample feature map combiner\n        for ind, ((dim_in, dim_out), groups, layer_num_resnet_blocks, layer_self_attn) in enumerate(zip(in_out, resnet_groups, num_resnet_blocks, self_attn)):\n            is_first = ind == 0\n            is_last = ind >= (num_resolutions - 1)\n            layer_cond_dim = cond_dim if not is_first else None\n            dim_layer = dim_out if memory_efficient else dim_in\n            skip_connect_dims.append(dim_layer)\n            attention = nn.Identity()\n            if layer_self_attn:"
        },
        {
            "comment": "This code initializes a module for a neural network. It adds downsampling modules, resnet blocks, attention layers, and convolutional layers based on the given parameters. The last block of the code initializes two additional blocks and an attention layer for further processing.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2059-2075",
            "content": "                attention = create_self_attn(dim_layer)\n            elif sparse_attn:\n                attention = Residual(LinearAttention(dim_layer, **attn_kwargs))\n            self.downs.append(nn.ModuleList([\n                downsample_klass(dim_in, dim_out = dim_out) if memory_efficient else None,\n                resnet_block(dim_layer, dim_layer, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([resnet_block(dim_layer, dim_layer, cond_dim = layer_cond_dim, time_cond_dim = time_cond_dim, groups = groups) for _ in range(layer_num_resnet_blocks)]),\n                attention,\n                downsample_klass(dim_layer, dim_out = dim_out) if not is_last and not memory_efficient else nn.Conv2d(dim_layer, dim_out, 1)\n            ]))\n        mid_dim = dims[-1]\n        self.mid_block1 = resnet_block(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = create_self_attn(mid_dim)\n        self.mid_block2 = re"
        },
        {
            "comment": "The code is defining a ResNet-based architecture with optional self-attention layers. It iterates through the input and output dimensions, groups, number of resnet blocks, and self-attention usage to create a series of resnet blocks, optionally including an identity or linear attention layer after each block.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2075-2093",
            "content": "snet_block(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        for ind, ((dim_in, dim_out), groups, layer_num_resnet_blocks, layer_self_attn) in enumerate(zip(reversed(in_out), reversed(resnet_groups), reversed(num_resnet_blocks), reversed(self_attn))):\n            is_last = ind >= (len(in_out) - 1)\n            layer_cond_dim = cond_dim if not is_last else None\n            skip_connect_dim = skip_connect_dims.pop()\n            attention = nn.Identity()\n            if layer_self_attn:\n                attention = create_self_attn(dim_out)\n            elif sparse_attn:\n                attention = Residual(LinearAttention(dim_out, **attn_kwargs))\n            upsample_combiner_dims.append(dim_out)\n            self.ups.append(nn.ModuleList([\n                resnet_block(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([resnet_block(dim_out + skip_connect_dim,"
        },
        {
            "comment": "This code defines a DALL\u00b7E 2 model architecture. It includes multiple resnet blocks, an upsampling sequence, and a final convolution layer. The number of resnet blocks is determined by the `layer_num_resnet_blocks` parameter. The upsample sequence combines outputs from all upsample blocks if `combine_upsample_fmaps` is set to True. The final resnet block takes in the combined output and the model's channels, with time conditioning (`time_cond_dim`) and top-level resnet grouping (`top_level_resnet_group`). Finally, a convolution layer converts the output to the desired channel size (`channels_out`). The `zero_init_` function initializes the final convolution layer with zero values.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2093-2115",
            "content": " dim_out, cond_dim = layer_cond_dim, time_cond_dim = time_cond_dim, groups = groups)  for _ in range(layer_num_resnet_blocks)]),\n                attention,\n                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else nn.Identity()\n            ]))\n        # whether to combine outputs from all upsample blocks for final resnet block\n        self.upsample_combiner = UpsampleCombiner(\n            dim = dim,\n            enabled = combine_upsample_fmaps,\n            dim_ins = upsample_combiner_dims,\n            dim_outs = (dim,) * len(upsample_combiner_dims)\n        )\n        # a final resnet block\n        self.final_resnet_block = resnet_block(self.upsample_combiner.dim_out + dim, dim, time_cond_dim = time_cond_dim, groups = top_level_resnet_group)\n        out_dim_in = dim + (channels if lowres_cond else 0)\n        self.to_out = nn.Conv2d(out_dim_in, self.channels_out, kernel_size = final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n        zero_init_(self.to_out) # since both OpenAI and @crowsonkb are doing it"
        },
        {
            "comment": "This code function checks if the current unet model parameters are correct for cascading DDPM. If not, it reinitializes the unet with the new settings. The parameters being checked include lowres_cond, channels, cond_on_image_embeds, and cond_on_text_encodings.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2117-2145",
            "content": "        # whether to checkpoint during training\n        self.checkpoint_during_training = checkpoint_during_training\n    # if the current settings for the unet are not correct\n    # for cascading DDPM, then reinit the unet with the right settings\n    def cast_model_parameters(\n        self,\n        *,\n        lowres_cond,\n        lowres_noise_cond,\n        channels,\n        channels_out,\n        cond_on_image_embeds,\n        cond_on_text_encodings,\n    ):\n        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_image_embeds == self.cond_on_image_embeds and \\\n            cond_on_text_encodings == self.cond_on_text_encodings and \\\n            lowres_noise_cond == self.lowres_noise_cond and \\\n            channels_out == self.channels_out:\n            return self\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_image_embeds = cond_on_image_embeds,"
        },
        {
            "comment": "This code defines a class with forward, forward_with_cond_scale methods that take various parameters and perform image processing operations. The forward method calculates logits based on input images, time, image embeddings, and other optional parameters. The forward_with_cond_scale method applies conditional scaling to the logits calculated by the forward method.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2146-2184",
            "content": "            cond_on_text_encodings = cond_on_text_encodings,\n            lowres_noise_cond = lowres_noise_cond\n        )\n        return self.__class__(**{**self._locals, **updated_kwargs})\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n        if cond_scale == 1:\n            return logits\n        null_logits = self.forward(*args, text_cond_drop_prob = 1., image_cond_drop_prob = 1., **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n    def forward(\n        self,\n        x,\n        time,\n        *,\n        image_embed,\n        lowres_cond_img = None,\n        lowres_noise_level = None,\n        text_encodings = None,\n        image_cond_drop_prob = 0.,\n        text_cond_drop_prob = 0.,\n        blur_sigma = None,\n        blur_kernel_size = None,\n        disable_checkpoint = False,\n        self_cond = None\n    ):\n        batch_size, device = x.shape[0], x.device\n        # add low resolution conditioning, if present"
        },
        {
            "comment": "The code checks if low resolution conditioning image exists and appends it to the input. It then concatenates self-conditioning, initializes a convolution, clones the input for residual calculations, performs time conditioning, and applies low resolution noise conditioning (if enabled).",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2186-2215",
            "content": "        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n        # concat self conditioning, if needed\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x, self_cond), dim = 1)\n        # concat low resolution conditioning\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n        # initial convolution\n        x = self.init_conv(x)\n        r = x.clone() # final residual\n        # time conditioning\n        time = time.type_as(x)\n        time_hiddens = self.to_time_hiddens(time)\n        time_tokens = self.to_time_tokens(time_hiddens)\n        t = self.to_time_cond(time_hiddens)\n        # low res noise conditioning (similar to time above)\n        if exists(lowres_noise_level):\n            assert exists(self.to_lowres_noise_cond), 'lowres_noise_cond must be set to True on instantiation of the unet in order to conditiong on lowres noise'"
        },
        {
            "comment": "This code performs conditional dropout by maintaining image and text masks, checks if an image embedding exists, applies a conditional dropout to the image embedding based on the masks, and adds it to the time embedding.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2216-2242",
            "content": "            lowres_noise_level = lowres_noise_level.type_as(x)\n            t = t + self.to_lowres_noise_cond(lowres_noise_level)\n        # conditional dropout\n        image_keep_mask = prob_mask_like((batch_size,), 1 - image_cond_drop_prob, device = device)\n        text_keep_mask = prob_mask_like((batch_size,), 1 - text_cond_drop_prob, device = device)\n        text_keep_mask = rearrange(text_keep_mask, 'b -> b 1 1')\n        # image embedding to be summed to time embedding\n        # discovered by @mhh0318 in the paper\n        if exists(image_embed) and exists(self.to_image_hiddens):\n            image_hiddens = self.to_image_hiddens(image_embed)\n            image_keep_mask_hidden = rearrange(image_keep_mask, 'b -> b 1')\n            null_image_hiddens = self.null_image_hiddens.to(image_hiddens.dtype)\n            image_hiddens = torch.where(\n                image_keep_mask_hidden,\n                image_hiddens,\n                null_image_hiddens\n            )\n            t = t + image_hiddens\n        # mask out image embedding depending on condition dropout"
        },
        {
            "comment": "This code chunk is setting up the input for a classifier-free guidance model. It checks if the image and text encodings are provided, and if so, prepares them for the model's input. If both the image embeddings and text encodings are present, it applies conditional guidance by masking the image tokens with the image_keep_mask and nullifying where needed. It asserts that the text encodings match the batch size and the expected embedding dimension of the model.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2243-2264",
            "content": "        # for classifier free guidance\n        image_tokens = None\n        if self.cond_on_image_embeds:\n            image_keep_mask_embed = rearrange(image_keep_mask, 'b -> b 1 1')\n            image_tokens = self.image_to_tokens(image_embed)\n            null_image_embed = self.null_image_embed.to(image_tokens.dtype) # for some reason pytorch AMP not working\n            image_tokens = torch.where(\n                image_keep_mask_embed,\n                image_tokens,\n                null_image_embed\n            )\n        # take care of text encodings (optional)\n        text_tokens = None\n        if exists(text_encodings) and self.cond_on_text_encodings:\n            assert text_encodings.shape[0] == batch_size, f'the text encodings being passed into the unet does not have the proper batch size - text encoding shape {text_encodings.shape} - required batch size is {batch_size}'\n            assert self.text_embed_dim == text_encodings.shape[-1], f'the text encodings you are passing in have a dimension of {text_encodings.shape[-1]}, but the unet was created with text_embed_dim of {self.text_embed_dim}.'"
        },
        {
            "comment": "This code snippet is preparing text_tokens for the model by applying padding and ensuring correct shape. It creates a binary mask (text_mask) from the non-zero elements in text_encodings to indicate which tokens are present, then applies this mask to both text_tokens and text_keep_mask. The code also checks if there's remaining space in the max_text_len and pads text_tokens accordingly. Lastly, it asserts that the shapes of text_mask and text_keep_mask match before combining them using a logical AND operation.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2266-2287",
            "content": "            text_mask = torch.any(text_encodings != 0., dim = -1)\n            text_tokens = self.text_to_cond(text_encodings)\n            text_tokens = text_tokens[:, :self.max_text_len]\n            text_mask = text_mask[:, :self.max_text_len]\n            text_tokens_len = text_tokens.shape[1]\n            remainder = self.max_text_len - text_tokens_len\n            if remainder > 0:\n                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n                text_mask = F.pad(text_mask, (0, remainder), value = False)\n            text_mask = rearrange(text_mask, 'b n -> b n 1')\n            assert text_mask.shape[0] == text_keep_mask.shape[0], f'text_mask has shape of {text_mask.shape} while text_keep_mask has shape {text_keep_mask.shape}. text encoding is of shape {text_encodings.shape}'\n            text_keep_mask = text_mask & text_keep_mask\n            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n            text_tokens = torch.where("
        },
        {
            "comment": "This code snippet is part of the DALLE2-pytorch model, responsible for handling conditioning tokens (main and auxiliary) for image and text inputs. The code normalizes these tokens using `self.norm_cond` and `self.norm_mid_cond`, applies gradient checkpointing, and makes certain modules (e.g., `self.init_resnet_block`) checkpointable based on training parameters. This helps to optimize the model's computation during inference and improve its performance.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2288-2317",
            "content": "                text_keep_mask,\n                text_tokens,\n                null_text_embed\n            )\n        # main conditioning tokens (c)\n        c = time_tokens\n        if exists(image_tokens):\n            c = torch.cat((c, image_tokens), dim = -2)\n        # text and image conditioning tokens (mid_c)\n        # to save on compute, only do cross attention based conditioning on the inner most layers of the Unet\n        mid_c = c if not exists(text_tokens) else torch.cat((c, text_tokens), dim = -2)\n        # normalize conditioning tokens\n        c = self.norm_cond(c)\n        mid_c = self.norm_mid_cond(mid_c)\n        # gradient checkpointing\n        can_checkpoint = self.training and self.checkpoint_during_training and not disable_checkpoint\n        apply_checkpoint_fn = make_checkpointable if can_checkpoint else identity\n        # make checkpointable modules\n        init_resnet_block, mid_block1, mid_attn, mid_block2, final_resnet_block = [maybe(apply_checkpoint_fn)(module) for module in (self.init_resnet_block, self.mid_block1, self.mid_attn, self.mid_block2, self.final_resnet_block)]"
        },
        {
            "comment": "This code initializes a U-Net model by iterating over its components. It applies pre-downsample, initial block, and resnet blocks to the input x. Then, it adds hidden representations of down and up stages into separate lists. After that, it passes x through an attention module and potentially post-downsample. Finally, it processes x with two more blocks, possibly applies mid-attention, and returns the final result.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2319-2355",
            "content": "        can_checkpoint_cond = lambda m: isinstance(m, ResnetBlock)\n        downs, ups = [maybe(apply_checkpoint_fn)(m, condition = can_checkpoint_cond) for m in (self.downs, self.ups)]\n        # initial resnet block\n        if exists(init_resnet_block):\n            x = init_resnet_block(x, t)\n        # go through the layers of the unet, down and up\n        down_hiddens = []\n        up_hiddens = []\n        for pre_downsample, init_block, resnet_blocks, attn, post_downsample in downs:\n            if exists(pre_downsample):\n                x = pre_downsample(x)\n            x = init_block(x, t, c)\n            for resnet_block in resnet_blocks:\n                x = resnet_block(x, t, c)\n                down_hiddens.append(x.contiguous())\n            x = attn(x)\n            down_hiddens.append(x.contiguous())\n            if exists(post_downsample):\n                x = post_downsample(x)\n        x = mid_block1(x, t, mid_c)\n        if exists(mid_attn):\n            x = mid_attn(x)\n        x = mid_block2(x, t, mid_c)\n "
        },
        {
            "comment": "This code defines a class for processing input images, which consists of an upscaling network and a low-resolution conditioner. The upscaling network takes in a low-resolution image and upscales it using skip connections and residual blocks. The low-resolution conditioner can optionally take a low-resolution version of the input image as additional input. The final output is passed through an activation function before being returned.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2355-2392",
            "content": "       connect_skip = lambda fmap: torch.cat((fmap, down_hiddens.pop() * self.skip_connect_scale), dim = 1)\n        for init_block, resnet_blocks, attn, upsample in ups:\n            x = connect_skip(x)\n            x = init_block(x, t, c)\n            for resnet_block in resnet_blocks:\n                x = connect_skip(x)\n                x = resnet_block(x, t, c)\n            x = attn(x)\n            up_hiddens.append(x.contiguous())\n            x = upsample(x)\n        x = self.upsample_combiner(x, up_hiddens)\n        x = torch.cat((x, r), dim = 1)\n        x = final_resnet_block(x, t)\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n        return self.to_out(x)\nclass LowresConditioner(nn.Module):\n    def __init__(\n        self,\n        downsample_first = True,\n        use_blur = True,\n        blur_prob = 0.5,\n        blur_sigma = 0.6,\n        blur_kernel_size = 3,\n        use_noise = False,\n        input_image_range = None,\n        normalize_img_fn = identity,\n        unnormalize_img_fn = identity"
        },
        {
            "comment": "This code initializes an object with various parameters, including downsampling, image range, and noise-related options. It also includes methods for generating noise images based on the given parameters. The class utilizes normalization and denormalization functions as well as a NoiseScheduler instance to apply noise to the input condition maps.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2393-2417",
            "content": "    ):\n        super().__init__()\n        self.downsample_first = downsample_first\n        self.input_image_range = input_image_range\n        self.use_blur = use_blur\n        self.blur_prob = blur_prob\n        self.blur_sigma = blur_sigma\n        self.blur_kernel_size = blur_kernel_size\n        self.use_noise = use_noise\n        self.normalize_img = normalize_img_fn\n        self.unnormalize_img = unnormalize_img_fn\n        self.noise_scheduler = NoiseScheduler(beta_schedule = 'linear', timesteps = 1000, loss_type = 'l2') if use_noise else None\n    def noise_image(self, cond_fmap, noise_levels = None):\n        assert exists(self.noise_scheduler)\n        batch = cond_fmap.shape[0]\n        cond_fmap = self.normalize_img(cond_fmap)\n        random_noise_levels = default(noise_levels, lambda: self.noise_scheduler.sample_random_times(batch))\n        cond_fmap = self.noise_scheduler.q_sample(cond_fmap, t = random_noise_levels, noise = torch.randn_like(cond_fmap))\n        cond_fmap = self.unnormalize_img(cond_fmap)"
        },
        {
            "comment": "This function takes a conditional feature map and optional parameters to resize, blur, and downsample the image. The code checks if downsampling is needed first, then decides whether to apply blurring based on a probability setting. Blur sigma and kernel size are also set based on default values or user input.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2418-2446",
            "content": "        return cond_fmap, random_noise_levels\n    def forward(\n        self,\n        cond_fmap,\n        *,\n        target_image_size,\n        downsample_image_size = None,\n        should_blur = True,\n        blur_sigma = None,\n        blur_kernel_size = None\n    ):\n        if self.downsample_first and exists(downsample_image_size):\n            cond_fmap = resize_image_to(cond_fmap, downsample_image_size, clamp_range = self.input_image_range, nearest = True)\n        # blur is only applied 50% of the time\n        # section 3.1 in https://arxiv.org/abs/2106.15282\n        if self.use_blur and should_blur and random.random() < self.blur_prob:\n            # when training, blur the low resolution conditional image\n            blur_sigma = default(blur_sigma, self.blur_sigma)\n            blur_kernel_size = default(blur_kernel_size, self.blur_kernel_size)\n            # allow for drawing a random sigma between lo and hi float values\n            if isinstance(blur_sigma, tuple):\n                blur_sigma = tuple(map(float, blur_sigma))"
        },
        {
            "comment": "This code performs image conditioning by applying Gaussian blur and noise addition, then resizes the image to a target size. The blurring and noise addition are optional depending on the use_noise flag, and the final result is returned along with any applied random noise levels.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2447-2470",
            "content": "                blur_sigma = random.uniform(*blur_sigma)\n            # allow for drawing a random kernel size between lo and hi int values\n            if isinstance(blur_kernel_size, tuple):\n                blur_kernel_size = tuple(map(int, blur_kernel_size))\n                kernel_size_lo, kernel_size_hi = blur_kernel_size\n                blur_kernel_size = random.randrange(kernel_size_lo, kernel_size_hi + 1)\n            cond_fmap = gaussian_blur2d(cond_fmap, cast_tuple(blur_kernel_size, 2), cast_tuple(blur_sigma, 2))\n        # resize to target image size\n        cond_fmap = resize_image_to(cond_fmap, target_image_size, clamp_range = self.input_image_range, nearest = True)\n        # noise conditioning, as done in Imagen\n        # as a replacement for the BSR noising, and potentially replace blurring for first stage too\n        random_noise_levels = None\n        if self.use_noise:\n            cond_fmap, random_noise_levels = self.noise_image(cond_fmap)\n        # return conditioning feature map, as well as the augmentation noise levels"
        },
        {
            "comment": "The code defines a Decoder class that takes various parameters like unet, clip, image_size, channels, vae, timesteps, sample_timesteps, image_cond_drop_prob, text_cond_drop_prob, loss_type, beta_schedule, predict_x_start, predict_v, predict_x_start_for_latent_diffusion, image_sizes, random_crop_sizes, use_noise_for_lowres_cond, and use_blur_for_lowres_cond. It returns cond_fmap and random_noise_levels.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2472-2495",
            "content": "        return cond_fmap, random_noise_levels\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        unet,\n        *,\n        clip = None,\n        image_size = None,\n        channels = 3,\n        vae = tuple(),\n        timesteps = 1000,\n        sample_timesteps = None,\n        image_cond_drop_prob = 0.1,\n        text_cond_drop_prob = 0.5,\n        loss_type = 'l2',\n        beta_schedule = None,\n        predict_x_start = False,\n        predict_v = False,\n        predict_x_start_for_latent_diffusion = False,\n        image_sizes = None,                         # for cascading ddpm, image size at each stage\n        random_crop_sizes = None,                   # whether to random crop the image at that stage in the cascade (super resoluting convolutions at the end may be able to generalize on smaller crops)\n        use_noise_for_lowres_cond = False,          # whether to use Imagen-like noising for low resolution conditioning  \n        use_blur_for_lowres_cond = True,            # whether to use the blur conditioning used in the original cascading ddpm paper, as well as DALL-E2"
        },
        {
            "comment": "This code snippet is responsible for configuring the settings for a denoising diffusion probabilistic model (DDPM) in the DALLE2-pytorch project. The settings include cascading DDPM parameters, noise level at sample time, clip options, learned variance configuration, and unconditional image generation toggles.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2496-2508",
            "content": "        lowres_downsample_first = True,             # cascading ddpm - resizes to lower resolution, then to next conditional resolution + blur\n        blur_prob = 0.5,                            # cascading ddpm - when training, the gaussian blur is only applied 50% of the time\n        blur_sigma = 0.6,                           # cascading ddpm - blur sigma\n        blur_kernel_size = 3,                       # cascading ddpm - blur kernel size\n        lowres_noise_sample_level = 0.2,            # in imagen paper, they use a 0.2 noise level at sample time for low resolution conditioning\n        clip_denoised = True,\n        clip_x_start = True,\n        clip_adapter_overrides = dict(),\n        learned_variance = True,\n        learned_variance_constrain_frac = False,\n        vb_loss_weight = 0.001,\n        unconditional = False,                      # set to True for generating images without conditioning\n        auto_normalize_img = True,                  # whether to take care of normalizing the i"
        },
        {
            "comment": "The code initializes an object with various parameters such as use_dynamic_thres, dynamic_thres_percentile, p2_loss_weight_gamma, p2_loss_weight_k, ddim_sampling_eta, and clip. It also checks if the 'clip' parameter is given and performs necessary assertions. If 'clip' exists and unconditional image training is not being done, it ensures the channels match with CLIP's accepted channels. It also uses XClipAdapter for compatibility with additional overrides.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2508-2525",
            "content": "mage from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        use_dynamic_thres = False,                  # from the Imagen paper\n        dynamic_thres_percentile = 0.95,\n        p2_loss_weight_gamma = 0.,                  # p2 loss weight, from https://arxiv.org/abs/2204.00227 - 0 is equivalent to weight of 1 across time - 1. is recommended\n        p2_loss_weight_k = 1,\n        ddim_sampling_eta = 0.                      # can be set to 0. for deterministic sampling afaict\n    ):\n        super().__init__()\n        # clip\n        self.clip = None\n        if exists(clip):\n            assert not unconditional, 'clip must not be given if doing unconditional image training'\n            assert channels == clip.image_channels, f'channels of image ({channels}) should be equal to the channels that CLIP accepts ({clip.image_channels})'\n            if isinstance(clip, CLIP):\n                clip = XClipAdapter(clip, **clip_adapter_overrides)"
        },
        {
            "comment": "The code checks the input 'clip' type and applies the CoCaAdapter if it's an instance of CoCa. It then freezes the model for evaluation, ensures 'clip' is a BaseClipAdapter instance, and assigns it to self.clip. The image_size is determined from either 'image_size', 'image_sizes', or 'clip'. It sets the 'channels', 'normalize_img', and 'unnormalize_img' based on given parameters.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2526-2554",
            "content": "            elif isinstance(clip, CoCa):\n                clip = CoCaAdapter(clip, **clip_adapter_overrides)\n            freeze_model_and_make_eval_(clip)\n            assert isinstance(clip, BaseClipAdapter)\n            self.clip = clip\n        # determine image size, with image_size and image_sizes taking precedence\n        if exists(image_size) or exists(image_sizes):\n            assert exists(image_size) ^ exists(image_sizes), 'only one of image_size or image_sizes must be given'\n            image_size = default(image_size, lambda: image_sizes[-1])\n        elif exists(clip):\n            image_size = clip.image_size\n        else:\n            raise Error('either image_size, image_sizes, or clip must be given to decoder')\n        # channels\n        self.channels = channels\n        # normalize and unnormalize image functions\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        # verify conditioning method"
        },
        {
            "comment": "This code initializes the U-Nets and VAEs for a DALL-E 2 model. It sets the number of unets, whether they are unconditional or conditioned on previous unets, and their learned variance. It also sets default parameters for conditioning with noise and constrains the output of the network from 0 to 1.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2556-2576",
            "content": "        unets = cast_tuple(unet)\n        num_unets = len(unets)\n        self.num_unets = num_unets\n        self.unconditional = unconditional\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n        vaes = pad_tuple_to_length(cast_tuple(vae), len(unets), fillvalue = NullVQGanVAE(channels = self.channels))\n        # whether to use learned variance, defaults to True for the first unet in the cascade, as in paper\n        learned_variance = pad_tuple_to_length(cast_tuple(learned_variance), len(unets), fillvalue = False)\n        self.learned_variance = learned_variance\n        self.learned_variance_constrain_frac = learned_variance_constrain_frac # whether to constrain the output of the network (the interpolation fraction) from 0 to 1\n        self.vb_loss_weight = vb_loss_weight\n        # default and validate conditioning parameters\n        use_noise_for_lowres_cond = cast_tuple(use_noise_for_lowres_cond, num_unets - 1, validate = False)"
        },
        {
            "comment": "This code is setting up Unets and Vaes for a model. It ensures that the lists of noise conditions and blur conditions are long enough to correspond to each Unet, adds the Unets and Vaes to module lists, and asserts that at least one Unet will not need low res noise or blur conditioning.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2577-2596",
            "content": "        use_blur_for_lowres_cond = cast_tuple(use_blur_for_lowres_cond, num_unets - 1, validate = False)\n        if len(use_noise_for_lowres_cond) < num_unets:\n            use_noise_for_lowres_cond = (False, *use_noise_for_lowres_cond)\n        if len(use_blur_for_lowres_cond) < num_unets:\n            use_blur_for_lowres_cond = (False, *use_blur_for_lowres_cond)\n        assert not use_noise_for_lowres_cond[0], 'first unet will never need low res noise conditioning'\n        assert not use_blur_for_lowres_cond[0], 'first unet will never need low res blur conditioning'\n        assert num_unets == 1 or all((use_noise or use_blur) for use_noise, use_blur in zip(use_noise_for_lowres_cond[1:], use_blur_for_lowres_cond[1:]))\n        # construct unets and vaes\n        self.unets = nn.ModuleList([])\n        self.vaes = nn.ModuleList([])\n        for ind, (one_unet, one_vae, one_unet_learned_var, lowres_noise_cond) in enumerate(zip(unets, vaes, learned_variance, use_noise_for_lowres_cond)):\n            assert isinstance(one_unet, Unet)"
        },
        {
            "comment": "This code block appends a new VAE instance to the list of VAEs and a copied evaluation version of that VAE to the VAEs list. The code also sets the sampling timesteps and ddim_sampling_eta based on the input parameters.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2597-2620",
            "content": "            assert isinstance(one_vae, (VQGanVAE, NullVQGanVAE))\n            is_first = ind == 0\n            latent_dim = one_vae.encoded_dim if exists(one_vae) else None\n            unet_channels = default(latent_dim, self.channels)\n            unet_channels_out = unet_channels * (1 if not one_unet_learned_var else 2)\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                lowres_noise_cond = lowres_noise_cond,\n                cond_on_image_embeds = not unconditional and is_first,\n                cond_on_text_encodings = not unconditional and one_unet.cond_on_text_encodings,\n                channels = unet_channels,\n                channels_out = unet_channels_out\n            )\n            self.unets.append(one_unet)\n            self.vaes.append(one_vae.copy_for_eval())\n        # sampling timesteps, defaults to non-ddim with full timesteps sampling\n        self.sample_timesteps = cast_tuple(sample_timesteps, num_unets)\n        self.ddim_sampling_eta = ddim_sampling_eta"
        },
        {
            "comment": "This code creates noise schedulers for each unet, based on the provided beta schedule and loss weight gamma. It asserts that sampling timesteps must be less than or equal to the number of training timesteps, and initializes a NoiseScheduler object with the specified parameters for each unet.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2622-2640",
            "content": "        # create noise schedulers per unet\n        if not exists(beta_schedule):\n            beta_schedule = ('cosine', *(('cosine',) * max(num_unets - 2, 0)), *(('linear',) * int(num_unets > 1)))\n        beta_schedule = cast_tuple(beta_schedule, num_unets)\n        p2_loss_weight_gamma = cast_tuple(p2_loss_weight_gamma, num_unets)\n        self.noise_schedulers = nn.ModuleList([])\n        for ind, (unet_beta_schedule, unet_p2_loss_weight_gamma, sample_timesteps) in enumerate(zip(beta_schedule, p2_loss_weight_gamma, self.sample_timesteps)):\n            assert not exists(sample_timesteps) or sample_timesteps <= timesteps, f'sampling timesteps {sample_timesteps} must be less than or equal to the number of training timesteps {timesteps} for unet {ind + 1}'\n            noise_scheduler = NoiseScheduler(\n                beta_schedule = unet_beta_schedule,\n                timesteps = timesteps,\n                loss_type = loss_type,\n                p2_loss_weight_gamma = unet_p2_loss_weight_gamma,\n                p2_loss_weight_k = p2_loss_weight_k"
        },
        {
            "comment": "This code is setting up the parameters for a model. It creates noise schedulers, defines image sizes and crop sizes for different resolutions, and configures predicting x0 and v values. These settings will be used to train or use the model. The code also performs assertions to ensure that the correct number of unets and vaes are provided for each resolution.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2641-2665",
            "content": "            )\n            self.noise_schedulers.append(noise_scheduler)\n        # unet image sizes\n        image_sizes = default(image_sizes, (image_size,))\n        image_sizes = tuple(sorted(set(image_sizes)))\n        assert self.num_unets == len(image_sizes), f'you did not supply the correct number of u-nets ({self.num_unets}) for resolutions {image_sizes}'\n        self.image_sizes = image_sizes\n        self.sample_channels = cast_tuple(self.channels, len(image_sizes))\n        # random crop sizes (for super-resoluting unets at the end of cascade?)\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, len(image_sizes))\n        assert not exists(self.random_crop_sizes[0]), 'you would not need to randomly crop the image for the base unet'\n        # predict x0 config\n        self.predict_x_start = cast_tuple(predict_x_start, len(unets)) if not predict_x_start_for_latent_diffusion else tuple(map(lambda t: isinstance(t, VQGanVAE), self.vaes))\n        # predict v\n        self.predict_v = cast_tuple(predict_v, len(unets))"
        },
        {
            "comment": "The code initializes the input image range and handles lowres_cond for each unet in the model. It ensures that the first unet is unconditioned, while the rest have `lowres_cond` set to True. The `LowresConditioner` class is used with specified parameters for downsampling, blurring, and input image range.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2667-2690",
            "content": "        # input image range\n        self.input_image_range = (-1. if not auto_normalize_img else 0., 1.)\n        # cascading ddpm related stuff\n        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n        self.lowres_conds = nn.ModuleList([])\n        for unet_index, use_noise, use_blur in zip(range(num_unets), use_noise_for_lowres_cond, use_blur_for_lowres_cond):\n            if unet_index == 0:\n                self.lowres_conds.append(None)\n                continue\n            lowres_cond = LowresConditioner(\n                downsample_first = lowres_downsample_first,\n                use_blur = use_blur,\n                use_noise = use_noise,\n                blur_prob = blur_prob,\n                blur_sigma = blur_sigma,\n                blur_kernel_size = blur_kernel_size,\n                input_image_range = self.input_image_range,"
        },
        {
            "comment": "This code is setting up parameters and functions for an image generation model. It includes normalization and unnormalization functions, lowres noise sample level, classifier free guidance settings, clipping options during sampling, dynamic thresholding settings, and device management. The model can condition on text encodings and uses a device tracker to keep track of device information.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2691-2724",
            "content": "                normalize_img_fn = self.normalize_img,\n                unnormalize_img_fn = self.unnormalize_img\n            )\n            self.lowres_conds.append(lowres_cond)\n        self.lowres_noise_sample_level = lowres_noise_sample_level\n        # classifier free guidance\n        self.image_cond_drop_prob = image_cond_drop_prob\n        self.text_cond_drop_prob = text_cond_drop_prob\n        self.can_classifier_guidance = image_cond_drop_prob > 0. or text_cond_drop_prob > 0.\n        # whether to clip when sampling\n        self.clip_denoised = clip_denoised\n        self.clip_x_start = clip_x_start\n        # dynamic thresholding settings, if clipping denoised during sampling\n        self.use_dynamic_thres = use_dynamic_thres\n        self.dynamic_thres_percentile = dynamic_thres_percentile\n        # device tracker\n        self.register_buffer('_dummy', torch.Tensor([True]), persistent = False)\n    @property\n    def device(self):\n        return self._dummy.device\n    @property\n    def condition_on_text_encodings(self):"
        },
        {
            "comment": "This code defines methods for working with a collection of UNET models. The `get_unet` method retrieves a specific UNET based on its number, ensuring it is within the valid range. `parse_unet_output` parses the output of a UNET, interpreting learned variance if present. The `one_unet_in_gpu` context manager allows running inference for one UNET on the GPU while keeping other UNETs on the CPU.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2725-2761",
            "content": "        return any([unet.cond_on_text_encodings for unet in self.unets if isinstance(unet, Unet)])\n    def get_unet(self, unet_number):\n        assert 0 < unet_number <= self.num_unets\n        index = unet_number - 1\n        return self.unets[index]\n    def parse_unet_output(self, learned_variance, output):\n        var_interp_frac_unnormalized = None\n        if learned_variance:\n            output, var_interp_frac_unnormalized = output.chunk(2, dim = 1)\n        return UnetOutput(output, var_interp_frac_unnormalized)\n    @contextmanager\n    def one_unet_in_gpu(self, unet_number = None, unet = None):\n        assert exists(unet_number) ^ exists(unet)\n        if exists(unet_number):\n            unet = self.get_unet(unet_number)\n        # devices\n        cuda, cpu = torch.device('cuda'), torch.device('cpu')\n        self.cuda()\n        devices = [module_device(unet) for unet in self.unets]\n        self.unets.to(cpu)\n        unet.to(cuda)\n        yield\n        for unet, device in zip(self.unets, devices):\n            unet.to(device)"
        },
        {
            "comment": "This code snippet defines a function `dynamic_threshold` and `p_mean_variance`. The `dynamic_threshold` function adjusts the threshold for clamping based on the input's quantile values. It uses static thresholding (s=1) by default, but can be set to dynamic thresholding if `self.use_dynamic_thres` is true. The `p_mean_variance` function performs classifier-free guidance for image generation and includes options for mean/variance prediction, conditioning, noise scheduling, and more.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2763-2784",
            "content": "    def dynamic_threshold(self, x):\n        \"\"\" proposed in https://arxiv.org/abs/2205.11487 as an improved clamping in the setting of classifier free guidance \"\"\"\n        # s is the threshold amount\n        # static thresholding would just be s = 1\n        s = 1.\n        if self.use_dynamic_thres:\n            s = torch.quantile(\n                rearrange(x, 'b ... -> b (...)').abs(),\n                self.dynamic_thres_percentile,\n                dim = -1\n            )\n            s.clamp_(min = 1.)\n            s = s.view(-1, *((1,) * (x.ndim - 1)))\n        # clip by threshold, depending on whether static or dynamic\n        x = x.clamp(-s, s) / s\n        return x\n    def p_mean_variance(self, unet, x, t, image_embed, noise_scheduler, text_encodings = None, lowres_cond_img = None, self_cond = None, clip_denoised = True, predict_x_start = False, predict_v = False, learned_variance = False, cond_scale = 1., model_output = None, lowres_noise_level = None):\n        assert not (cond_scale != 1. and not self."
        },
        {
            "comment": "This code block is responsible for decoding an input image using a pre-trained unet model. It applies classifier free guidance if enabled, and then calculates the mean and variance of the posterior distribution to perform denoising diffusion probability.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2784-2802",
            "content": "can_classifier_guidance), 'the decoder was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n        model_output = default(model_output, lambda: unet.forward_with_cond_scale(x, t, image_embed = image_embed, text_encodings = text_encodings, cond_scale = cond_scale, lowres_cond_img = lowres_cond_img, self_cond = self_cond, lowres_noise_level = lowres_noise_level))\n        pred, var_interp_frac_unnormalized = self.parse_unet_output(learned_variance, model_output)\n        if predict_v:\n            x_start = noise_scheduler.predict_start_from_v(x, t = t, v = pred)\n        elif predict_x_start:\n            x_start = pred\n        else:\n            x_start = noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n        if clip_denoised:\n            x_start = self.dynamic_threshold(x_start)\n        model_mean, posterior_variance, posterior_log_variance = noise_scheduler.q_posterior(x_start=x_start, x_t=x, t=t)\n        if learned_variance:"
        },
        {
            "comment": "This code calculates the posterior variance and log variance for a model based on the maximum and minimum log beta values, as described in Equation 15 from arXiv paper. It also applies a learned constraint factor and uses sigmoid activation if required. The function returns the model mean, posterior variance, posterior log variance, and x_start.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2803-2819",
            "content": "            # if learned variance, posterio variance and posterior log variance are predicted by the network\n            # by an interpolation of the max and min log beta values\n            # eq 15 - https://arxiv.org/abs/2102.09672\n            min_log = extract(noise_scheduler.posterior_log_variance_clipped, t, x.shape)\n            max_log = extract(torch.log(noise_scheduler.betas), t, x.shape)\n            var_interp_frac = unnormalize_zero_to_one(var_interp_frac_unnormalized)\n            if self.learned_variance_constrain_frac:\n                var_interp_frac = var_interp_frac.sigmoid()\n            posterior_log_variance = var_interp_frac * max_log + (1 - var_interp_frac) * min_log\n            posterior_variance = posterior_log_variance.exp()\n        return model_mean, posterior_variance, posterior_log_variance, x_start\n    @torch.no_grad()\n    def p_sample(self, unet, x, t, image_embed, noise_scheduler, text_encodings = None, cond_scale = 1., lowres_cond_img = None, self_cond = None, predict_x_"
        },
        {
            "comment": "This function takes input x and returns the predicted values pred and x_start. It uses a p_mean_variance method from self to calculate model_mean, model_log_variance, and x_start. Noise is added to the input x, except when t == 0. The result is the sum of model_mean and nonzero_mask * (0.5 * model_log_variance).exp() * noise. This is a part of the DDPM (Denoising Diffusion Probabilistic Models) framework for generating images.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2819-2835",
            "content": "start = False, predict_v = False, learned_variance = False, clip_denoised = True, lowres_noise_level = None):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance, x_start = self.p_mean_variance(unet, x = x, t = t, image_embed = image_embed, text_encodings = text_encodings, cond_scale = cond_scale, lowres_cond_img = lowres_cond_img, self_cond = self_cond, clip_denoised = clip_denoised, predict_x_start = predict_x_start, predict_v = predict_v, noise_scheduler = noise_scheduler, learned_variance = learned_variance, lowres_noise_level = lowres_noise_level)\n        noise = torch.randn_like(x)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n        return pred, x_start\n    @torch.no_grad()\n    def p_sample_loop_ddpm(\n        self,\n        unet,\n        shape,\n        image_embed,\n        noise_scheduler,\n        predict_x_start = False,"
        },
        {
            "comment": "This function initializes image and related variables. If inpainting is present, it normalizes and resizes the image, mask, and sets their dimensions accordingly. The function also determines if the model is performing latent diffusion by checking for provided parameters. It then proceeds to an if-not condition where it assumes that the model is not performing latent diffusion.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2836-2865",
            "content": "        predict_v = False,\n        learned_variance = False,\n        clip_denoised = True,\n        lowres_cond_img = None,\n        text_encodings = None,\n        cond_scale = 1,\n        is_latent_diffusion = False,\n        lowres_noise_level = None,\n        inpaint_image = None,\n        inpaint_mask = None,\n        inpaint_resample_times = 5\n    ):\n        device = self.device\n        b = shape[0]\n        img = torch.randn(shape, device = device)\n        x_start = None # for self-conditioning\n        is_inpaint = exists(inpaint_image)\n        resample_times = inpaint_resample_times if is_inpaint else 1\n        if is_inpaint:\n            inpaint_image = self.normalize_img(inpaint_image)\n            inpaint_image = resize_image_to(inpaint_image, shape[-1], nearest = True)\n            inpaint_mask = rearrange(inpaint_mask, 'b h w -> b 1 h w').float()\n            inpaint_mask = resize_image_to(inpaint_mask, shape[-1], nearest = True)\n            inpaint_mask = inpaint_mask.bool()\n        if not is_latent_diffusion:"
        },
        {
            "comment": "This code performs progressive growing of an image using a diffusion model, such as DALLE 2. It iterates over timesteps in reverse order and resamples each timestep to produce a final output image. It also includes the option for inpainting by following the Repaint paper's approach. The self-conditioning and U-Net are utilized within the p_sample function, which takes care of the actual sampling process.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2866-2889",
            "content": "            lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n        for time in tqdm(reversed(range(0, noise_scheduler.num_timesteps)), desc = 'sampling loop time step', total = noise_scheduler.num_timesteps):\n            is_last_timestep = time == 0\n            for r in reversed(range(0, resample_times)):\n                is_last_resample_step = r == 0\n                times = torch.full((b,), time, device = device, dtype = torch.long)\n                if is_inpaint:\n                    # following the repaint paper\n                    # https://arxiv.org/abs/2201.09865\n                    noised_inpaint_image = noise_scheduler.q_sample(inpaint_image, t = times)\n                    img = (img * ~inpaint_mask) + (noised_inpaint_image * inpaint_mask)\n                self_cond = x_start if unet.self_cond else None\n                img, x_start = self.p_sample(\n                    unet,\n                    img,\n                    times,\n                    image_embed = image_embed,\n                    text_encodings = text_encodings,"
        },
        {
            "comment": "This code is part of a model that performs image denoising using diffusion models. It samples images at different timesteps, applies noise scheduling for resampling, and handles inpainting by combining input mask and image embeddings. The output is then unnormalized for the final result.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2890-2916",
            "content": "                    cond_scale = cond_scale,\n                    self_cond = self_cond,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_level = lowres_noise_level,\n                    predict_x_start = predict_x_start,\n                    predict_v = predict_v,\n                    noise_scheduler = noise_scheduler,\n                    learned_variance = learned_variance,\n                    clip_denoised = clip_denoised\n                )\n                if is_inpaint and not (is_last_timestep or is_last_resample_step):\n                    # in repaint, you renoise and resample up to 10 times every step\n                    img = noise_scheduler.q_sample_from_to(img, times - 1, times)\n        if is_inpaint:\n            img = (img * ~inpaint_mask) + (inpaint_image * inpaint_mask)\n        unnormalize_img = self.unnormalize_img(img)\n        return unnormalize_img\n    @torch.no_grad()\n    def p_sample_loop_ddim(\n        self,\n        unet,\n        shape,\n        image_embed,"
        },
        {
            "comment": "This function takes multiple parameters including noise_scheduler, timesteps, eta, and more. It extracts necessary information like batch size, device, total timesteps, alphas, and other parameters to perform DDIM sampling. It also checks if inpainting is required and resamples times accordingly.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2917-2945",
            "content": "        noise_scheduler,\n        timesteps,\n        eta = 1.,\n        predict_x_start = False,\n        predict_v = False,\n        learned_variance = False,\n        clip_denoised = True,\n        lowres_cond_img = None,\n        text_encodings = None,\n        cond_scale = 1,\n        is_latent_diffusion = False,\n        lowres_noise_level = None,\n        inpaint_image = None,\n        inpaint_mask = None,\n        inpaint_resample_times = 5\n    ):\n        batch, device, total_timesteps, alphas, eta = shape[0], self.device, noise_scheduler.num_timesteps, noise_scheduler.alphas_cumprod, self.ddim_sampling_eta\n        times = torch.linspace(0., total_timesteps, steps = timesteps + 2)[:-1]\n        times = list(reversed(times.int().tolist()))\n        time_pairs = list(zip(times[:-1], times[1:]))\n        time_pairs = list(filter(lambda t: t[0] > t[1], time_pairs))\n        is_inpaint = exists(inpaint_image)\n        resample_times = inpaint_resample_times if is_inpaint else 1\n        if is_inpaint:\n            inpaint_image = self.normalize_img(inpaint_image)"
        },
        {
            "comment": "The code is sampling from a diffusion model and applying inpainting. It resizes images, prepares masks for inpainting, sets up variables for time steps, and conditions the model based on inpainting or not. The code follows the process described in the Repaint paper (https://arxiv.org/abs/2201.09865).",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2946-2971",
            "content": "            inpaint_image = resize_image_to(inpaint_image, shape[-1], nearest = True)\n            inpaint_mask = rearrange(inpaint_mask, 'b h w -> b 1 h w').float()\n            inpaint_mask = resize_image_to(inpaint_mask, shape[-1], nearest = True)\n            inpaint_mask = inpaint_mask.bool()\n        img = torch.randn(shape, device = device)\n        x_start = None # for self-conditioning\n        if not is_latent_diffusion:\n            lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n            is_last_timestep = time_next == 0\n            for r in reversed(range(0, resample_times)):\n                is_last_resample_step = r == 0\n                alpha = alphas[time]\n                alpha_next = alphas[time_next]\n                time_cond = torch.full((batch,), time, device = device, dtype = torch.long)\n                if is_inpaint:\n                    # following the repaint paper\n                    # https://arxiv.org/abs/2201.09865"
        },
        {
            "comment": "This code is using a conditional image generation model to generate an output image based on the input image, conditioning factors (time_cond, image_embed, text_encodings), and possibly predicting x0 values for further processing or clipping.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2972-2993",
            "content": "                    noised_inpaint_image = noise_scheduler.q_sample(inpaint_image, t = time_cond)\n                    img = (img * ~inpaint_mask) + (noised_inpaint_image * inpaint_mask)\n                self_cond = x_start if unet.self_cond else None\n                unet_output = unet.forward_with_cond_scale(img, time_cond, image_embed = image_embed, text_encodings = text_encodings, cond_scale = cond_scale, self_cond = self_cond, lowres_cond_img = lowres_cond_img, lowres_noise_level = lowres_noise_level)\n                pred, _ = self.parse_unet_output(learned_variance, unet_output)\n                # predict x0\n                if predict_v:\n                    x_start = noise_scheduler.predict_start_from_v(img, t = time_cond, v = pred)\n                elif predict_x_start:\n                    x_start = pred\n                else:\n                    x_start = noise_scheduler.predict_start_from_noise(img, t = time_cond, noise = pred)\n                # maybe clip x0\n                if clip_denoised:\n                    x_start = self.dynamic_threshold(x_start)"
        },
        {
            "comment": "Predicts noise based on the current state and time, applies coefficients to noise and image, performs inpainting if necessary, and unnormalizes the image.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":2995-3016",
            "content": "                # predict noise\n                pred_noise = noise_scheduler.predict_noise_from_start(img, t = time_cond, x0 = x_start)\n                c1 = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n                c2 = ((1 - alpha_next) - torch.square(c1)).sqrt()\n                noise = torch.randn_like(img) if not is_last_timestep else 0.\n                img = x_start * alpha_next.sqrt() + \\\n                      c1 * noise + \\\n                      c2 * pred_noise\n                if is_inpaint and not (is_last_timestep or is_last_resample_step):\n                    # in repaint, you renoise and resample up to 10 times every step\n                    time_next_cond = torch.full((batch,), time_next, device = device, dtype = torch.long)\n                    img = noise_scheduler.q_sample_from_to(img, time_next_cond, time_cond)\n        if exists(inpaint_image):\n            img = (img * ~inpaint_mask) + (inpaint_image * inpaint_mask)\n        img = self.unnormalize_img(img)\n        return img"
        },
        {
            "comment": "Function `p_sample_loop` takes in arguments, determines if DDPM or DDIM should be used for sampling, and calls respective function.\nIn `p_losses`, noise is defaulted if not provided, and images are normalized before processing if not latent diffusion.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3018-3038",
            "content": "    @torch.no_grad()\n    def p_sample_loop(self, *args, noise_scheduler, timesteps = None, **kwargs):\n        num_timesteps = noise_scheduler.num_timesteps\n        timesteps = default(timesteps, num_timesteps)\n        assert timesteps <= num_timesteps\n        is_ddim = timesteps < num_timesteps\n        if not is_ddim:\n            return self.p_sample_loop_ddpm(*args, noise_scheduler = noise_scheduler, **kwargs)\n        return self.p_sample_loop_ddim(*args, noise_scheduler = noise_scheduler, timesteps = timesteps, **kwargs)\n    def p_losses(self, unet, x_start, times, *, image_embed, noise_scheduler, lowres_cond_img = None, text_encodings = None, predict_x_start = False, predict_v = False, noise = None, learned_variance = False, clip_denoised = False, is_latent_diffusion = False, lowres_noise_level = None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        # normalize to [-1, 1]\n        if not is_latent_diffusion:\n            x_start = self.normalize_img(x_start)\n            lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)"
        },
        {
            "comment": "Code snippet is from the DALLE2-pytorch model. It samples noisy images and uses them to conditionally generate unet outputs for self-conditioning and prediction, with optional dropout probabilities for image and text conditions.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3040-3074",
            "content": "        # get x_t\n        x_noisy = noise_scheduler.q_sample(x_start = x_start, t = times, noise = noise)\n        # unet kwargs\n        unet_kwargs = dict(\n            image_embed = image_embed,\n            text_encodings = text_encodings,\n            lowres_cond_img = lowres_cond_img,\n            lowres_noise_level = lowres_noise_level,\n        )\n        # self conditioning\n        self_cond = None\n        if unet.self_cond and random.random() < 0.5:\n            with torch.no_grad():\n                unet_output = unet(x_noisy, times, **unet_kwargs)\n                self_cond, _ = self.parse_unet_output(learned_variance, unet_output)\n                self_cond = self_cond.detach()\n        # forward to get model prediction\n        unet_output = unet(\n            x_noisy,\n            times,\n            **unet_kwargs,\n            self_cond = self_cond,\n            image_cond_drop_prob = self.image_cond_drop_prob,\n            text_cond_drop_prob = self.text_cond_drop_prob,\n        )\n        pred, _ = self.parse_unet_output(learned_variance, unet_output)"
        },
        {
            "comment": "The code calculates the loss in a specific manner depending on the input parameters. If predict_v is true, it calculates the target value for v. If predict_x_start is true, it uses x_start as the target. Otherwise, it uses noise as the target. Then, it applies the loss function, reduces the loss, reweighs the loss based on times, and finally calculates the mean of the loss. If learned_variance is not used, it returns the simple loss.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3076-3097",
            "content": "        if predict_v:\n            target = noise_scheduler.calculate_v(x_start, times, noise)\n        elif predict_x_start:\n            target = x_start\n        else:\n            target = noise\n        loss = noise_scheduler.loss_fn(pred, target, reduction = 'none')\n        loss = reduce(loss, 'b ... -> b (...)', 'mean')\n        loss = noise_scheduler.p2_reweigh_loss(loss, times)\n        loss = loss.mean()\n        if not learned_variance:\n            # return simple loss if not using learned variance\n            return loss\n        # most of the code below is transcribed from\n        # https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/diffusion_utils_2.py\n        # the Improved DDPM paper then further modified it so that the mean is detached (shown a couple lines before), and weighted to be smaller than the l1 or l2 \"simple\" loss\n        # it is questionable whether this is really needed, looking at some of the figures in the paper, but may as well stay faithful to their implementation"
        },
        {
            "comment": "This code calculates the KL divergence between true and model predicted posterior distributions, and decoder negative log likelihood. It uses detached model predictions for stability reasons as per the paper. The loss at the first timestep is the decoder NLL, otherwise it's the KL divergence.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3099-3114",
            "content": "        # if learning the variance, also include the extra weight kl loss\n        true_mean, _, true_log_variance_clipped = noise_scheduler.q_posterior(x_start = x_start, x_t = x_noisy, t = times)\n        model_mean, _, model_log_variance, _ = self.p_mean_variance(unet, x = x_noisy, t = times, image_embed = image_embed, noise_scheduler = noise_scheduler, clip_denoised = clip_denoised, learned_variance = True, model_output = unet_output)\n        # kl loss with detached model predicted mean, for stability reasons as in paper\n        detached_model_mean = model_mean.detach()\n        kl = normal_kl(true_mean, true_log_variance_clipped, detached_model_mean, model_log_variance)\n        kl = meanflat(kl) * NAT\n        decoder_nll = -discretized_gaussian_log_likelihood(x_start, means = detached_model_mean, log_scales = 0.5 * model_log_variance)\n        decoder_nll = meanflat(decoder_nll) * NAT\n        # at the first timestep return the decoder NLL, otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))"
        },
        {
            "comment": "This function calculates the variational Bayes loss and adds it to the main loss. It then samples from the model given input parameters such as image, text, batch size, etc., with option for conditional or unconditional sampling. The function also performs some assertions on the inputs to ensure proper usage.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3116-3148",
            "content": "        vb_losses = torch.where(times == 0, decoder_nll, kl)\n        # weight the vb loss smaller, for stability, as in the paper (recommended 0.001)\n        vb_loss = vb_losses.mean() * self.vb_loss_weight\n        return loss + vb_loss\n    @torch.no_grad()\n    @eval_decorator\n    def sample(\n        self,\n        image = None,\n        image_embed = None,\n        text = None,\n        text_encodings = None,\n        batch_size = 1,\n        cond_scale = 1.,\n        start_at_unet_number = 1,\n        stop_at_unet_number = None,\n        distributed = False,\n        inpaint_image = None,\n        inpaint_mask = None,\n        inpaint_resample_times = 5,\n        one_unet_in_gpu_at_time = True\n    ):\n        assert self.unconditional or exists(image_embed), 'image embed must be present on sampling from decoder unless if trained unconditionally'\n        if not self.unconditional:\n            batch_size = image_embed.shape[0]\n        if exists(text) and not exists(text_encodings) and not self.unconditional:\n            assert exists(self.clip)"
        },
        {
            "comment": "This code checks for valid inputs and asserts whether text, text encodings, or inpaint_image and mask are present based on the condition specified. It also ensures that the image input has the correct batch size when starting at a specific unet number. If necessary, it resizes the image using nearest-neighbor interpolation.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3149-3162",
            "content": "            _, text_encodings = self.clip.embed_text(text)\n        assert not (self.condition_on_text_encodings and not exists(text_encodings)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text_encodings and exists(text_encodings)), 'decoder specified not to be conditioned on text, yet it is presented'\n        assert not (exists(inpaint_image) ^ exists(inpaint_mask)), 'inpaint_image and inpaint_mask (boolean mask of [batch, height, width]) must be both given for inpainting'\n        img = None\n        if start_at_unet_number > 1:\n            # Then we are not generating the first image and one must have been passed in\n            assert exists(image), 'image must be passed in if starting at unet number > 1'\n            assert image.shape[0] == batch_size, 'image must have batch size of {} if starting at unet number > 1'.format(batch_size)\n            prev_unet_output_size = self.image_sizes[start_at_unet_number - 2]\n            img = resize_image_to(image, prev_unet_output_size, nearest = True)"
        },
        {
            "comment": "This code is iterating through each unet in the model, skipping the first X unets based on a given parameter. It checks if the current unet should be processed based on its position, and then prepares low resolution conditioning for upsamplers if required. The code also handles CUDA processing and uses context managers to ensure efficient resource usage.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3164-3181",
            "content": "        is_cuda = next(self.parameters()).is_cuda\n        num_unets = self.num_unets\n        cond_scale = cast_tuple(cond_scale, num_unets)\n        for unet_number, unet, vae, channel, image_size, predict_x_start, predict_v, learned_variance, noise_scheduler, lowres_cond, sample_timesteps, unet_cond_scale in tqdm(zip(range(1, num_unets + 1), self.unets, self.vaes, self.sample_channels, self.image_sizes, self.predict_x_start, self.predict_v, self.learned_variance, self.noise_schedulers, self.lowres_conds, self.sample_timesteps, cond_scale)):\n            if unet_number < start_at_unet_number:\n                continue  # It's the easiest way to do it\n            context = self.one_unet_in_gpu(unet = unet) if is_cuda and one_unet_in_gpu_at_time else null_context()\n            with context:\n                # prepare low resolution conditioning for upsamplers\n                lowres_cond_img = lowres_noise_level = None\n                shape = (batch_size, channel, image_size, image_size)\n                if unet.lowres_cond:"
        },
        {
            "comment": "This code is part of a denoising diffusion model. It first resizes the input image to a target size and applies noise if needed. Then, it checks if the VAE (Variational Autoencoder) is used for latent diffusion and adjusts the image size accordingly. Finally, it encodes the low-resolution image using the VAE and enters a denoising loop with a UNet model to generate the final output image.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3182-3203",
            "content": "                    lowres_cond_img = resize_image_to(img, target_image_size = image_size, clamp_range = self.input_image_range, nearest = True)\n                    if lowres_cond.use_noise:\n                        lowres_noise_level = torch.full((batch_size,), int(self.lowres_noise_sample_level * 1000), dtype = torch.long, device = self.device)\n                        lowres_cond_img, _ = lowres_cond.noise_image(lowres_cond_img, lowres_noise_level)\n                # latent diffusion\n                is_latent_diffusion = isinstance(vae, VQGanVAE)\n                image_size = vae.get_encoded_fmap_size(image_size)\n                shape = (batch_size, vae.encoded_dim, image_size, image_size)\n                lowres_cond_img = maybe(vae.encode)(lowres_cond_img)\n                # denoising loop for image\n                img = self.p_sample_loop(\n                    unet,\n                    shape,\n                    image_embed = image_embed,\n                    text_encodings = text_encodings,\n                    cond_scale = unet_cond_scale,"
        },
        {
            "comment": "The function takes an image and optionally text, generates images at different UNet resolutions based on input parameters, and returns the generated image. It includes options for low-resolution output, inpainting, and stopping at a specific UNet resolution.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3204-3232",
            "content": "                    predict_x_start = predict_x_start,\n                    predict_v = predict_v,\n                    learned_variance = learned_variance,\n                    clip_denoised = not is_latent_diffusion,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_level = lowres_noise_level,\n                    is_latent_diffusion = is_latent_diffusion,\n                    noise_scheduler = noise_scheduler,\n                    timesteps = sample_timesteps,\n                    inpaint_image = inpaint_image,\n                    inpaint_mask = inpaint_mask,\n                    inpaint_resample_times = inpaint_resample_times\n                )\n                img = vae.decode(img)\n            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n                break\n        return img\n    def forward(\n        self,\n        image,\n        text = None,\n        image_embed = None,\n        text_encodings = None,\n        unet_number = None,\n        return_lowres_"
        },
        {
            "comment": "This function is initializing variables for a specific U-Net in the model, based on the provided unet_number. It assigns the corresponding U-Net, VAE, noise scheduler, lowres conditioner, target image size, predict x start, predict v, random crop size, and learned variance from predefined lists for that U-Net index. It also ensures the image shape aligns with the expected number of channels.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3232-3250",
            "content": "cond_image = False # whether to return the low resolution conditioning images, for debugging upsampler purposes\n    ):\n        assert not (self.num_unets > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {self.num_unets}, if you are training cascading DDPM (multiple unets)'\n        unet_number = default(unet_number, 1)\n        unet_index = unet_number - 1\n        unet = self.get_unet(unet_number)\n        vae                 = self.vaes[unet_index]\n        noise_scheduler     = self.noise_schedulers[unet_index]\n        lowres_conditioner  = self.lowres_conds[unet_index]\n        target_image_size   = self.image_sizes[unet_index]\n        predict_x_start     = self.predict_x_start[unet_index]\n        predict_v           = self.predict_v[unet_index]\n        random_crop_size    = self.random_crop_sizes[unet_index]\n        learned_variance    = self.learned_variance[unet_index]\n        b, c, h, w, device, = *image.shape, image.device\n        assert image.shape[1] == self.channels"
        },
        {
            "comment": "The code checks if the image and/or text inputs exist, ensuring that either the CLIP model or the necessary inputs are present. It asserts that if the decoder is supposed to be conditioned on text encodings, then the text encodings must be provided, and vice versa. This helps prevent errors in the input data for generating image embeddings.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3251-3266",
            "content": "        assert h >= target_image_size and w >= target_image_size\n        times = torch.randint(0, noise_scheduler.num_timesteps, (b,), device = device, dtype = torch.long)\n        if not exists(image_embed) and not self.unconditional:\n            assert exists(self.clip), 'if you want to derive CLIP image embeddings automatically, you must supply `clip` to the decoder on init'\n            image_embed, _ = self.clip.embed_image(image)\n        if exists(text) and not exists(text_encodings) and not self.unconditional:\n            assert exists(self.clip), 'if you are passing in raw text, you need to supply `clip` to the decoder'\n            _, text_encodings = self.clip.embed_text(text)\n        assert not (self.condition_on_text_encodings and not exists(text_encodings)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text_encodings and exists(text_encodings)), 'decoder specified not to be conditioned on text, yet it is presented'\n        "
        },
        {
            "comment": "This code snippet is conditioning a low-resolution image using the lowres_conditioner and performing data augmentation via Kornia's RandomCrop. It also encodes both the image and the conditioned image using a VAE (Variational Autoencoder) and calculates loss from p_losses for further processing in the U-net model.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3266-3284",
            "content": "lowres_cond_img, lowres_noise_level = lowres_conditioner(image, target_image_size = target_image_size, downsample_image_size = self.image_sizes[unet_index - 1]) if exists(lowres_conditioner) else (None, None)\n        image = resize_image_to(image, target_image_size, nearest = True)\n        if exists(random_crop_size):\n            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n            # make sure low res conditioner and image both get augmented the same way\n            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n            image = aug(image)\n            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n        is_latent_diffusion = not isinstance(vae, NullVQGanVAE)\n        vae.eval()\n        with torch.no_grad():\n            image = vae.encode(image)\n            lowres_cond_img = maybe(vae.encode)(lowres_cond_img)\n        losses = self.p_losses(unet, image, times, image_embed = image"
        },
        {
            "comment": "This code defines a DALLE2 class with prior and decoder modules. It takes text input, performs diffusion, and returns losses or lowres_cond_img based on the return flag. If not returning the lowres conditional image, it returns only losses.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3284-3318",
            "content": "_embed, text_encodings = text_encodings, lowres_cond_img = lowres_cond_img, predict_x_start = predict_x_start, predict_v = predict_v, learned_variance = learned_variance, is_latent_diffusion = is_latent_diffusion, noise_scheduler = noise_scheduler, lowres_noise_level = lowres_noise_level)\n        if not return_lowres_cond_image:\n            return losses\n        return losses, lowres_cond_img\n# main class\nclass DALLE2(nn.Module):\n    def __init__(\n        self,\n        *,\n        prior,\n        decoder,\n        prior_num_samples = 2\n    ):\n        super().__init__()\n        assert isinstance(prior, DiffusionPrior)\n        assert isinstance(decoder, Decoder)\n        self.prior = prior\n        self.decoder = decoder\n        self.prior_num_samples = prior_num_samples\n        self.decoder_need_text_cond = self.decoder.condition_on_text_encodings\n        self.to_pil = T.ToPILImage()\n    @torch.no_grad()\n    @eval_decorator\n    def forward(\n        self,\n        text,\n        cond_scale = 1.,\n        prior_cond_scale = 1.,"
        },
        {
            "comment": "This function takes text as input, tokenizes it if necessary, and uses a prior model to generate image embeddings. It then passes these embeddings along with the text (if required) to a decoder model to generate images. Optionally, it converts the images to PIL format and returns them. If only one text is given, it returns the first generated image.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/dalle2_pytorch.py\":3319-3339",
            "content": "        return_pil_images = False\n    ):\n        device = module_device(self)\n        one_text = isinstance(text, str) or (not is_list_str(text) and text.shape[0] == 1)\n        if isinstance(text, str) or is_list_str(text):\n            text = [text] if not isinstance(text, (list, tuple)) else text\n            text = tokenizer.tokenize(text).to(device)\n        image_embed = self.prior.sample(text, num_samples_per_batch = self.prior_num_samples, cond_scale = prior_cond_scale)\n        text_cond = text if self.decoder_need_text_cond else None\n        images = self.decoder.sample(image_embed = image_embed, text = text_cond, cond_scale = cond_scale)\n        if return_pil_images:\n            images = list(map(self.to_pil, images.unbind(dim = 0)))\n        if one_text:\n            return first(images)\n        return images"
        }
    ]
}