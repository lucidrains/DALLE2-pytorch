{
    "summary": "The code simplifies DALL-E2 text tokenization by offering a PyTorch BPE tokenizer implementation with features for whitespace cleanup, formatting fixes, human-readable conversion, and handling context length limitations.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines functions for tokenization, specifically for the DALL-E2 model. It uses OpenAI's simple tokenizer, a byte-to-unicode conversion, and a function to generate character pairs from a given word. The code is meant to provide users with an easy way to start training DALL-E without implementing BPE (Byte Pair Encoding).",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/tokenizer.py\":0-41",
            "content": "# take from https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py\n# to give users a quick easy start to training DALL-E without doing BPE\nimport torch\nimport html\nimport os\nimport ftfy\nimport regex as re\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom dalle2_pytorch.utils import import_or_print_error\n# OpenAI simple tokenizer\n@lru_cache()\ndef default_bpe():\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data/bpe_simple_vocab_16e6.txt\")\n@lru_cache()\ndef bytes_to_unicode():\n    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"\u00a1\"), ord(\"\u00ac\") + 1)) + list(range(ord(\"\u00ae\"), ord(\"\u00ff\") + 1))\n    cs = bs[:]\n    n = 0\n    for b in range(2 ** 8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2 ** 8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\ndef get_pairs(word):\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\ndef basic_clean(text):"
        },
        {
            "comment": "This code is a Python class for a tokenizer that utilizes byte encoding and decoding, along with byte-pair encoding (BPE) to convert text into tokens. The class also includes methods for cleaning whitespace and fixing text formatting issues. The BPE merges are loaded from a specified file path, and the vocabulary is expanded by adding special tokens like \"<|startoftext|>\" and \"<|endoftext|>\".",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/tokenizer.py\":42-68",
            "content": "    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\ndef whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path = default_bpe()):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = Path(bpe_path).read_text(encoding='utf8').split('\\n')\n        merges = merges[1:49152 - 256 - 2 + 1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v + '</w>' for v in vocab]\n        for merge in merges:\n            vocab.append(''.join(merge))\n        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n        self.vocab_size = 49408\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))"
        },
        {
            "comment": "The code defines a tokenizer that uses byte-pair encoding (BPE) for text. It compiles a regular expression pattern to match words and special tokens like \"<|startoftext|>\" and \"<|endoftext|>\". The `bpe` method takes a token, checks if it's in the cache, and if not, processes it using BPE by splitting it into smaller parts until no more splits are possible.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/tokenizer.py\":69-97",
            "content": "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n        self.pat = re.compile(\n            r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\",\n            re.IGNORECASE)\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + (token[-1] + '</w>',)\n        pairs = get_pairs(word)\n        if not pairs:\n            return token + '</w>'\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break"
        },
        {
            "comment": "Code snippet is from a byte-pair encoding (BPE) tokenizer implementation in PyTorch. The code encodes input text into BPE tokens, performs wordpiece tokenization, and caches the mapping between tokens and words for decoding. The encode() function processes the input text by applying preprocessing steps, performing BPE, and extending tokens list with BPE tokens. The decode() function allows decoding of encoded tokens back to words using cached mappings.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/tokenizer.py\":99-125",
            "content": "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = ' '.join(word)\n        self.cache[token] = word\n        return word\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n        return bpe_tokens\n    def decode(self, tokens, remove_start_end = True, pad_tokens = set()):\n        if torch.is_tensor(tokens):\n            tokens = tokens.tolist()"
        },
        {
            "comment": "The code defines a SimpleTokenizer class that tokenizes input texts using an encoding scheme and provides a method to convert encoded tokens into human-readable text. It also includes a tokenize function to process multiple input texts, considering context length limitations and handling truncation. The provided code snippet focuses on the process of converting encoded tokens into text.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/tokenizer.py\":127-150",
            "content": "        if remove_start_end:\n            tokens = [token for token in tokens if token not in (49406, 40407, 0)]\n        text = ''.join([self.decoder[token] for token in tokens if token not in pad_tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n        return text\n    def tokenize(self, texts, context_length = 256, truncate_text = False):\n        if isinstance(texts, str):\n            texts = [texts]\n        all_tokens = [self.encode(text) for text in texts]\n        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n        for i, tokens in enumerate(all_tokens):\n            if len(tokens) > context_length:\n                if truncate_text:\n                    tokens = tokens[:context_length]\n                else:\n                    raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n            result[i, :len(tokens)] = torch.tensor(tokens)\n        return result\ntokenizer = SimpleTokenizer()"
        },
        {
            "comment": "This code defines a YTTM tokenizer class in PyTorch. The constructor loads the BPE model from the specified path and initializes the tokenizer instance, which can decode and encode text sequences. The decode function converts tokenized lists to human-readable strings, while the encode function transforms input texts into tokenized lists. The tokenize method takes a list of texts, encodes them, and returns a tensor of shape (number_of_texts, context_length) for further processing.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/tokenizer.py\":152-181",
            "content": "# YTTM tokenizer\nclass YttmTokenizer:\n    def __init__(self, bpe_path = None):\n        bpe_path = Path(bpe_path)\n        assert bpe_path.exists(), f'BPE json path {str(bpe_path)} does not exist'\n        self.yttm = import_or_print_error('youtokentome', 'you need to install youtokentome by `pip install youtokentome`')\n        tokenizer = self.yttm.BPE(model = str(bpe_path))\n        self.tokenizer = tokenizer\n        self.vocab_size = tokenizer.vocab_size()\n    def decode(self, tokens, pad_tokens = set()):\n        if torch.is_tensor(tokens):\n            tokens = tokens.tolist()\n        return self.tokenizer.decode(tokens, ignore_ids = pad_tokens.union({0}))\n    def encode(self, texts):\n        encoded = self.tokenizer.encode(texts, output_type = self.yttm.OutputType.ID)\n        return list(map(torch.tensor, encoded))\n    def tokenize(self, texts, context_length = 256, truncate_text = False):\n        if isinstance(texts, str):\n            texts = [texts]\n        all_tokens = self.encode(texts)\n        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)"
        },
        {
            "comment": "This code segment iterates through all tokens in a list, truncating any token sequence longer than the specified context length. If truncation is not allowed and an input text is too long, it raises a RuntimeError. The truncated or original tokens are then converted to torch tensors and stored in a result array.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/tokenizer.py\":182-190",
            "content": "        for i, tokens in enumerate(all_tokens):\n            if len(tokens) > context_length:\n                if truncate_text:\n                    tokens = tokens[:context_length]\n                else:\n                    raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n            result[i, :len(tokens)] = torch.tensor(tokens)\n        return result"
        }
    ]
}