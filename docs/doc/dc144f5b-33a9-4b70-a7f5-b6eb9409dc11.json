{
    "summary": "Code describes VQGAN-VAE and Vision Transformer architectures for image generation models, including convolutional layers, self-attention mechanisms, layer normalization, initializes model, calculates losses, determines adaptive weight, applies clamp function, calculates combined loss, returns reconstructed feature maps if required.",
    "details": [
        {
            "comment": "This code imports various libraries and defines several constants, helper functions, and decorators for use in a deep learning model. It also sets up a class for a Vector Quantize module using PyTorch, with functionality to evaluate the model and remove the VGG feature if present.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":0-50",
            "content": "import copy\nimport math\nfrom math import sqrt\nfrom functools import partial, wraps\nfrom vector_quantize_pytorch import VectorQuantize as VQ\nimport torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\nfrom torch.autograd import grad as torch_grad\nimport torchvision\nfrom einops import rearrange, reduce, repeat, pack, unpack\nfrom einops.layers.torch import Rearrange\n# constants\nMList = nn.ModuleList\n# helper functions\ndef exists(val):\n    return val is not None\ndef default(val, d):\n    return val if exists(val) else d\n# decorators\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\ndef remove_vgg(fn):\n    @wraps(fn)\n    def inner(self, *args, **kwargs):\n        has_vgg = hasattr(self, 'vgg')\n        if has_vgg:\n            vgg = self.vgg\n            delattr(self, 'vgg')\n        out = fn(self, *args, **kwargs)\n        if has_vgg:\n            self.vgg = vgg"
        },
        {
            "comment": "This code contains various utility functions. \"pick_and_pop\" removes and returns keys from a dictionary, \"group_dict_by_key\" groups dictionary items by key condition, \"string_begins_with\" checks if a string begins with a given prefix, \"group_by_key_prefix\" groups dictionary items based on a key prefix, and \"groupby_prefix_and_trim\" trims key prefixes before grouping. Lastly, the \"log\" function calculates the natural logarithm of an input tensor, and the \"gradient_penalty\" function is used to calculate a gradient penalty for image generation tasks.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":52-86",
            "content": "        return out\n    return inner\n# keyword argument helpers\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\ndef string_begins_with(prefix, string_input):\n    return string_input.startswith(prefix)\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n# tensor helper functions\ndef log(t, eps = 1e-10):\n    return torch.log(t + eps)\ndef gradient_penalty(images, output, weight = 10):\n    batch_size = images.shape[0]"
        },
        {
            "comment": "This code contains several utility functions and loss functions used in the VQ-VAE-GAN model. It includes functions for gradient calculations, normalization, activation functions, and various GAN losses. The functions are defined to be reusable throughout the codebase.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":87-120",
            "content": "    gradients = torch_grad(outputs = output, inputs = images,\n                           grad_outputs = torch.ones(output.size(), device = images.device),\n                           create_graph = True, retain_graph = True, only_inputs = True)[0]\n    gradients = rearrange(gradients, 'b ... -> b (...)')\n    return weight * ((gradients.norm(2, dim = 1) - 1) ** 2).mean()\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\ndef leaky_relu(p = 0.1):\n    return nn.LeakyReLU(0.1)\ndef stable_softmax(t, dim = -1, alpha = 32 ** 2):\n    t = t / alpha\n    t = t - torch.amax(t, dim = dim, keepdim = True).detach()\n    return (t * alpha).softmax(dim = dim)\ndef safe_div(numer, denom, eps = 1e-8):\n    return numer / (denom + eps)\n# gan losses\ndef hinge_discr_loss(fake, real):\n    return (F.relu(1 + fake) + F.relu(1 - real)).mean()\ndef hinge_gen_loss(fake):\n    return -fake.mean()\ndef bce_discr_loss(fake, real):\n    return (-log(1 - torch.sigmoid(fake)) - log(torch.sigmoid(real))).mean()\ndef bce_gen_loss(fake):\n    return -log(torch.sigmoid(fake)).mean()"
        },
        {
            "comment": "The code defines a function to compute gradients of a layer wrt the loss, and introduces two custom modules: LayerNormChan for layer normalization and Discriminator for a convolutional network. The discriminator consists of multiple layers with decreasing kernel sizes, each followed by a leaky ReLU activation function. These components are part of the VQGAN-VAE architecture in DALLE2-pytorch.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":122-162",
            "content": "def grad_layer_wrt_loss(loss, layer):\n    return torch_grad(\n        outputs = loss,\n        inputs = layer,\n        grad_outputs = torch.ones_like(loss),\n        retain_graph = True\n    )[0].detach()\n# vqgan vae\nclass LayerNormChan(nn.Module):\n    def __init__(\n        self,\n        dim,\n        eps = 1e-5\n    ):\n        super().__init__()\n        self.eps = eps\n        self.gamma = nn.Parameter(torch.ones(1, dim, 1, 1))\n    def forward(self, x):\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) / (var + self.eps).sqrt() * self.gamma\n# discriminator\nclass Discriminator(nn.Module):\n    def __init__(\n        self,\n        dims,\n        channels = 3,\n        groups = 16,\n        init_kernel_size = 5\n    ):\n        super().__init__()\n        dim_pairs = zip(dims[:-1], dims[1:])\n        self.layers = MList([nn.Sequential(nn.Conv2d(channels, dims[0], init_kernel_size, padding = init_kernel_size // 2), leaky_relu())])\n        for dim_in, dim_out in dim_pairs:"
        },
        {
            "comment": "The code defines a VQGAN-VAE model. It uses convolutional layers and group normalization for downsampling the input image, followed by linear layers and leaky ReLU activation functions in a sequential manner to generate logits. The `ContinuousPositionBias` class is used for positional encoding in the model.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":163-196",
            "content": "            self.layers.append(nn.Sequential(\n                nn.Conv2d(dim_in, dim_out, 4, stride = 2, padding = 1),\n                nn.GroupNorm(groups, dim_out),\n                leaky_relu()\n            ))\n        dim = dims[-1]\n        self.to_logits = nn.Sequential( # return 5 x 5, for PatchGAN-esque training\n            nn.Conv2d(dim, dim, 1),\n            leaky_relu(),\n            nn.Conv2d(dim, 1, 4)\n        )\n    def forward(self, x):\n        for net in self.layers:\n            x = net(x)\n        return self.to_logits(x)\n# positional encoding\nclass ContinuousPositionBias(nn.Module):\n    \"\"\" from https://arxiv.org/abs/2111.09883 \"\"\"\n    def __init__(self, *, dim, heads, layers = 2):\n        super().__init__()\n        self.net = MList([])\n        self.net.append(nn.Sequential(nn.Linear(2, dim), leaky_relu()))\n        for _ in range(layers - 1):\n            self.net.append(nn.Sequential(nn.Linear(dim, dim), leaky_relu()))\n        self.net.append(nn.Linear(dim, heads))\n        self.register_buffer('rel_pos', None, persistent = False)"
        },
        {
            "comment": "The code defines a VQ-VAE implementation with a resnet encoder/decoder for image generation. The function calculates relative positional embeddings and applies them to the input, then passes the result through a resnet encoder/decoder network before returning the transformed input. The ResnetEncDec class creates an instance of the resnet encoder/decoder with optional parameters such as dimensions, channels, layers, layer_mults, num_resnet_blocks, resnet_groups, first_conv_kernel_size, and use_attn.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":198-231",
            "content": "    def forward(self, x):\n        n, device = x.shape[-1], x.device\n        fmap_size = int(sqrt(n))\n        if not exists(self.rel_pos):\n            pos = torch.arange(fmap_size, device = device)\n            grid = torch.stack(torch.meshgrid(pos, pos, indexing = 'ij'))\n            grid = rearrange(grid, 'c i j -> (i j) c')\n            rel_pos = rearrange(grid, 'i c -> i 1 c') - rearrange(grid, 'j c -> 1 j c')\n            rel_pos = torch.sign(rel_pos) * torch.log(rel_pos.abs() + 1)\n            self.register_buffer('rel_pos', rel_pos, persistent = False)\n        rel_pos = self.rel_pos.float()\n        for layer in self.net:\n            rel_pos = layer(rel_pos)\n        bias = rearrange(rel_pos, 'i j h -> h i j')\n        return x + bias\n# resnet encoder / decoder\nclass ResnetEncDec(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        channels = 3,\n        layers = 4,\n        layer_mults = None,\n        num_resnet_blocks = 1,\n        resnet_groups = 16,\n        first_conv_kernel_size = 5,\n        use_attn = True,"
        },
        {
            "comment": "This code defines a class with specified parameters for layers, encoders, and decoders. It ensures the dimension is divisible by resnet_groups. The layer multipliers are stored in a list and used to determine the dimensions of each layer. num_resnet_blocks and use_attn are checked to make sure they match the designated number of layers.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":232-261",
            "content": "        attn_dim_head = 64,\n        attn_heads = 8,\n        attn_dropout = 0.,\n    ):\n        super().__init__()\n        assert dim % resnet_groups == 0, f'dimension {dim} must be divisible by {resnet_groups} (groups for the groupnorm)'\n        self.layers = layers\n        self.encoders = MList([])\n        self.decoders = MList([])\n        layer_mults = default(layer_mults, list(map(lambda t: 2 ** t, range(layers))))\n        assert len(layer_mults) == layers, 'layer multipliers must be equal to designated number of layers'\n        layer_dims = [dim * mult for mult in layer_mults]\n        dims = (dim, *layer_dims)\n        self.encoded_dim = dims[-1]\n        dim_pairs = zip(dims[:-1], dims[1:])\n        append = lambda arr, t: arr.append(t)\n        prepend = lambda arr, t: arr.insert(0, t)\n        if not isinstance(num_resnet_blocks, tuple):\n            num_resnet_blocks = (*((0,) * (layers - 1)), num_resnet_blocks)\n        if not isinstance(use_attn, tuple):\n            use_attn = (*((False,) * (layers - 1)), use_attn)"
        },
        {
            "comment": "This code creates encoder and decoder blocks for a VQ-VAE model. It asserts that the number of resnet blocks and use_attn match the layers, then iterates over each layer creating convolutional layers, LeakyReLU activation functions, optionally adding attention modules, and repeating a specific number of residual blocks in both encoders and decoders.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":263-278",
            "content": "        assert len(num_resnet_blocks) == layers, 'number of resnet blocks config must be equal to number of layers'\n        assert len(use_attn) == layers\n        for layer_index, (dim_in, dim_out), layer_num_resnet_blocks, layer_use_attn in zip(range(layers), dim_pairs, num_resnet_blocks, use_attn):\n            append(self.encoders, nn.Sequential(nn.Conv2d(dim_in, dim_out, 4, stride = 2, padding = 1), leaky_relu()))\n            prepend(self.decoders, nn.Sequential(nn.ConvTranspose2d(dim_out, dim_in, 4, 2, 1), leaky_relu()))\n            if layer_use_attn:\n                prepend(self.decoders, VQGanAttention(dim = dim_out, heads = attn_heads, dim_head = attn_dim_head, dropout = attn_dropout))\n            for _ in range(layer_num_resnet_blocks):\n                append(self.encoders, ResBlock(dim_out, groups = resnet_groups))\n                prepend(self.decoders, GLUResBlock(dim_out, groups = resnet_groups))\n            if layer_use_attn:\n                append(self.encoders, VQGanAttention(dim = dim_out, heads = attn_heads, dim_head = attn_dim_head, dropout = attn_dropout))"
        },
        {
            "comment": "The code defines a class for a VQGAN-VAE model. It consists of encoder and decoder blocks, along with a GLUResBlock for the residual connections in the decoder. The encoder and decoder are composed of convolutional layers that reduce and increase image size respectively. The encoded image size is defined as the original image size divided by 2 to the power of the number of layers. The model can encode and decode images using the encoder and decoder blocks, and the last decoder layer's weights can be accessed separately.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":280-314",
            "content": "        prepend(self.encoders, nn.Conv2d(channels, dim, first_conv_kernel_size, padding = first_conv_kernel_size // 2))\n        append(self.decoders, nn.Conv2d(dim, channels, 1))\n    def get_encoded_fmap_size(self, image_size):\n        return image_size // (2 ** self.layers)\n    @property\n    def last_dec_layer(self):\n        return self.decoders[-1].weight\n    def encode(self, x):\n        for enc in self.encoders:\n            x = enc(x)\n        return x\n    def decode(self, x):\n        for dec in self.decoders:\n            x = dec(x)\n        return x\nclass GLUResBlock(nn.Module):\n    def __init__(self, chan, groups = 16):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(chan, chan * 2, 3, padding = 1),\n            nn.GLU(dim = 1),\n            nn.GroupNorm(groups, chan),\n            nn.Conv2d(chan, chan * 2, 3, padding = 1),\n            nn.GLU(dim = 1),\n            nn.GroupNorm(groups, chan),\n            nn.Conv2d(chan, chan, 1)\n        )\n    def forward(self, x):\n        return self.net(x) + x"
        },
        {
            "comment": "This code defines a residual block and a VQGAN attention layer for image processing. The ResBlock consists of multiple 2D convolutions and GroupNorm layers, followed by leaky ReLU activation functions. The VQGANAttention class is responsible for self-attention in the VQGAN model, using continuous position bias and multi-head attention with dropout regularization.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":316-353",
            "content": "class ResBlock(nn.Module):\n    def __init__(self, chan, groups = 16):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(chan, chan, 3, padding = 1),\n            nn.GroupNorm(groups, chan),\n            leaky_relu(),\n            nn.Conv2d(chan, chan, 3, padding = 1),\n            nn.GroupNorm(groups, chan),\n            leaky_relu(),\n            nn.Conv2d(chan, chan, 1)\n        )\n    def forward(self, x):\n        return self.net(x) + x\n# vqgan attention layer\nclass VQGanAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        dropout = 0.\n    ):\n        super().__init__()\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        inner_dim = heads * dim_head\n        self.dropout = nn.Dropout(dropout)\n        self.pre_norm = LayerNormChan(dim)\n        self.cpb = ContinuousPositionBias(dim = dim // 4, heads = heads)\n        self.to_qkv = nn.Conv2d(dim, inner_dim * 3, 1, bias = False)\n        self.to_out = nn.Conv2d(inner_dim, dim, 1, bias = False)"
        },
        {
            "comment": "This code defines a class for the Attention module in a ViT (Vision Transformer) model. It performs multi-head attention using key, query, and value tensors, followed by a softmax function to compute attention weights. The output is then passed through a linear layer and layer normalization before being added back to the input with residual connection.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":355-395",
            "content": "    def forward(self, x):\n        h = self.heads\n        height, width, residual = *x.shape[-2:], x.clone()\n        x = self.pre_norm(x)\n        q, k, v = self.to_qkv(x).chunk(3, dim = 1)\n        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = h), (q, k, v))\n        sim = einsum('b h c i, b h c j -> b h i j', q, k) * self.scale\n        sim = self.cpb(sim)\n        attn = stable_softmax(sim, dim = -1)\n        attn = self.dropout(attn)\n        out = einsum('b h i j, b h c j -> b h c i', attn, v)\n        out = rearrange(out, 'b h c (x y) -> b (h c) x y', x = height, y = width)\n        out = self.to_out(out)\n        return out + residual\n# ViT encoder / decoder\nclass RearrangeImage(nn.Module):\n    def forward(self, x):\n        n = x.shape[1]\n        w = h = int(sqrt(n))\n        return rearrange(x, 'b (h w) ... -> b h w ...', h = h, w = w)\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        heads = 8,\n        dim_head = 32\n    ):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)"
        },
        {
            "comment": "This code defines a MultiHeadAttention module for a transformer model. It initializes the attention head count and scale, calculates inner dimension based on head count and input dimension. The forward function performs multi-head attention by splitting input into query, key, value tensors, scaling query tensor, computing similarity between query and key, subtracting maximum similarity to avoid zero gradients, performing softmax on attention scores, and finally producing output tensor through weighted sum of value tensors. The FeedForward function defines a feedforward network for the transformer model, consisting of layer normalization, linear layers with GELU activation function.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":396-432",
            "content": "        self.heads = heads\n        self.scale = dim_head ** -0.5\n        inner_dim = dim_head * heads\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim)\n    def forward(self, x):\n        h = self.heads\n        x = self.norm(x)\n        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n        q = q * self.scale\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n        attn = sim.softmax(dim = -1)\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\ndef FeedForward(dim, mult = 4):\n    return nn.Sequential(\n        nn.LayerNorm(dim),\n        nn.Linear(dim, dim * mult, bias = False),\n        nn.GELU(),\n        nn.Linear(dim * mult, dim, bias = False)\n    )\nclass Transformer(nn.Module):\n    def __init__(\n        self,"
        },
        {
            "comment": "The code defines a class for an encoder-decoder architecture, which is part of the Vision Transformer (ViT) model. It utilizes attention and feedforward layers, and includes layer normalization in its forward pass. The encoder section takes input images, reshapes them into patches, and passes them through multiple attention and feedforward layers.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":433-475",
            "content": "        dim,\n        *,\n        layers,\n        dim_head = 32,\n        heads = 8,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(layers):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n        self.norm = nn.LayerNorm(dim)\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return self.norm(x)\nclass ViTEncDec(nn.Module):\n    def __init__(\n        self,\n        dim,\n        channels = 3,\n        layers = 4,\n        patch_size = 8,\n        dim_head = 32,\n        heads = 8,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.encoded_dim = dim\n        self.patch_size = patch_size\n        input_dim = channels * (patch_size ** 2)\n        self.encoder = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),"
        },
        {
            "comment": "The code defines a VQ-VAE model for image generation, consisting of an encoder and decoder. The encoder processes the input image and outputs a compressed codebook index followed by a positional embedding. The decoder then reconstructs the original image from these inputs using a series of transformers and linear layers. The get_encoded_fmap_size function calculates the encoded feature map size based on the input image size.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":476-509",
            "content": "            nn.Linear(input_dim, dim),\n            Transformer(\n                dim = dim,\n                dim_head = dim_head,\n                heads = heads,\n                ff_mult = ff_mult,\n                layers = layers\n            ),\n            RearrangeImage(),\n            Rearrange('b h w c -> b c h w')\n        )\n        self.decoder = nn.Sequential(\n            Rearrange('b c h w -> b (h w) c'),\n            Transformer(\n                dim = dim,\n                dim_head = dim_head,\n                heads = heads,\n                ff_mult = ff_mult,\n                layers = layers\n            ),\n            nn.Sequential(\n                nn.Linear(dim, dim * 4, bias = False),\n                nn.Tanh(),\n                nn.Linear(dim * 4, input_dim, bias = False),\n            ),\n            RearrangeImage(),\n            Rearrange('b h w (p1 p2 c) -> b c (h p1) (w p2)', p1 = patch_size, p2 = patch_size)\n        )\n    def get_encoded_fmap_size(self, image_size):\n        return image_size // self.patch_size\n    @property"
        },
        {
            "comment": "This code defines two classes: NullVQGanVAE and VQGanVAE. The NullVQGanVAE is a placeholder class without any specific layers or functionality, while the VQGanVAE class represents a variant of the VAE model with optional features like VGG loss, GAN integration, and customizable parameters for codebook dimensions and layers.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":510-561",
            "content": "    def last_dec_layer(self):\n        return self.decoder[-3][-1].weight\n    def encode(self, x):\n        return self.encoder(x)\n    def decode(self, x):\n        return self.decoder(x)\n# main vqgan-vae classes\nclass NullVQGanVAE(nn.Module):\n    def __init__(\n        self,\n        *,\n        channels\n    ):\n        super().__init__()\n        self.encoded_dim = channels\n        self.layers = 0\n    def get_encoded_fmap_size(self, size):\n        return size\n    def copy_for_eval(self):\n        return self\n    def encode(self, x):\n        return x\n    def decode(self, x):\n        return x\nclass VQGanVAE(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        image_size,\n        channels = 3,\n        layers = 4,\n        l2_recon_loss = False,\n        use_hinge_loss = True,\n        vgg = None,\n        vq_codebook_dim = 256,\n        vq_codebook_size = 512,\n        vq_decay = 0.8,\n        vq_commitment_weight = 1.,\n        vq_kmeans_init = True,\n        vq_use_cosine_sim = True,\n        use_vgg_and_gan = True,\n        vae_type = 'resnet',"
        },
        {
            "comment": "This code initializes a VQ-VAE model with given parameters. It uses a specified encoder-decoder network (ResNet or ViT), codebook size, and other VQ-specific options. The VQ module is initialized based on the dimensionality of the encoder-decoder's encoded output, and the codebook size and related options. If an invalid VAE type is given, a ValueError is raised.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":562-595",
            "content": "        discr_layers = 4,\n        **kwargs\n    ):\n        super().__init__()\n        vq_kwargs, kwargs = groupby_prefix_and_trim('vq_', kwargs)\n        encdec_kwargs, kwargs = groupby_prefix_and_trim('encdec_', kwargs)\n        self.image_size = image_size\n        self.channels = channels\n        self.codebook_size = vq_codebook_size\n        if vae_type == 'resnet':\n            enc_dec_klass = ResnetEncDec\n        elif vae_type == 'vit':\n            enc_dec_klass = ViTEncDec\n        else:\n            raise ValueError(f'{vae_type} not valid')\n        self.enc_dec = enc_dec_klass(\n            dim = dim,\n            channels = channels,\n            layers = layers,\n            **encdec_kwargs\n        )\n        self.vq = VQ(\n            dim = self.enc_dec.encoded_dim,\n            codebook_dim = vq_codebook_dim,\n            codebook_size = vq_codebook_size,\n            decay = vq_decay,\n            commitment_weight = vq_commitment_weight,\n            accept_image_fmap = True,\n            kmeans_init = vq_kmeans_init,\n            use_cosine_sim = vq_use_cosine_sim,"
        },
        {
            "comment": "This code defines a VQGAN-VAE model with optional GAN and perceptual loss components. It initializes the VGG model, Discriminator, and sets the reconstruction and generator losses based on provided arguments. The encoded_dim property returns the dimension of the encoded images.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":596-632",
            "content": "            **vq_kwargs\n        )\n        # reconstruction loss\n        self.recon_loss_fn = F.mse_loss if l2_recon_loss else F.l1_loss\n        # turn off GAN and perceptual loss if grayscale\n        self.vgg = None\n        self.discr = None\n        self.use_vgg_and_gan = use_vgg_and_gan\n        if not use_vgg_and_gan:\n            return\n        # preceptual loss\n        if exists(vgg):\n            self.vgg = vgg\n        else:\n            self.vgg = torchvision.models.vgg16(pretrained = True)\n            self.vgg.classifier = nn.Sequential(*self.vgg.classifier[:-2])\n        # gan related losses\n        layer_mults = list(map(lambda t: 2 ** t, range(discr_layers)))\n        layer_dims = [dim * mult for mult in layer_mults]\n        dims = (dim, *layer_dims)\n        self.discr = Discriminator(dims = dims, channels = channels)\n        self.discr_loss = hinge_discr_loss if use_hinge_loss else bce_discr_loss\n        self.gen_loss = hinge_gen_loss if use_hinge_loss else bce_gen_loss\n    @property\n    def encoded_dim(self):"
        },
        {
            "comment": "This code defines a class with methods to get encoded dimensions, calculate encoded frame map size, copy the model for evaluation, save and load state dictionary while removing VGG, encode input frames, and decode encoded frames.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":633-671",
            "content": "        return self.enc_dec.encoded_dim\n    def get_encoded_fmap_size(self, image_size):\n        return self.enc_dec.get_encoded_fmap_size(image_size)\n    def copy_for_eval(self):\n        device = next(self.parameters()).device\n        vae_copy = copy.deepcopy(self.cpu())\n        if vae_copy.use_vgg_and_gan:\n            del vae_copy.discr\n            del vae_copy.vgg\n        vae_copy.eval()\n        return vae_copy.to(device)\n    @remove_vgg\n    def state_dict(self, *args, **kwargs):\n        return super().state_dict(*args, **kwargs)\n    @remove_vgg\n    def load_state_dict(self, *args, **kwargs):\n        return super().load_state_dict(*args, **kwargs)\n    @property\n    def codebook(self):\n        return self.vq.codebook\n    def encode(self, fmap):\n        fmap = self.enc_dec.encode(fmap)\n        return fmap\n    def decode(self, fmap, return_indices_and_loss = False):\n        fmap, indices, commit_loss = self.vq(fmap)\n        fmap = self.enc_dec.decode(fmap)\n        if not return_indices_and_loss:\n            return fmap"
        },
        {
            "comment": "This function encodes an input image, decodes it, and can optionally return autoencoder or discriminator losses. It expects the image to have the specified dimensions and number of channels. The code asserts that the image's height, width, and number of channels match the expected values, and that only one type of loss is returned at a time.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":673-699",
            "content": "        return fmap, indices, commit_loss\n    def forward(\n        self,\n        img,\n        return_loss = False,\n        return_discr_loss = False,\n        return_recons = False,\n        add_gradient_penalty = True\n    ):\n        batch, channels, height, width, device = *img.shape, img.device\n        assert height == self.image_size and width == self.image_size, 'height and width of input image must be equal to {self.image_size}'\n        assert channels == self.channels, 'number of channels on image or sketch is not equal to the channels set on this VQGanVAE'\n        fmap = self.encode(img)\n        fmap, indices, commit_loss = self.decode(fmap, return_indices_and_loss = True)\n        if not return_loss and not return_discr_loss:\n            return fmap\n        assert return_loss ^ return_discr_loss, 'you should either return autoencoder loss or discriminator loss, but not both'\n        # whether to return discriminator loss\n        if return_discr_loss:\n            assert exists(self.discr), 'discriminator must exist to train it'"
        },
        {
            "comment": "The code is calculating the reconstruction and perceptual loss for an image generation model. It also includes gradient penalty for the discriminator loss, and optionally returns the reconstructed feature map.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":701-738",
            "content": "            fmap.detach_()\n            img.requires_grad_()\n            fmap_discr_logits, img_discr_logits = map(self.discr, (fmap, img))\n            discr_loss = self.discr_loss(fmap_discr_logits, img_discr_logits)\n            if add_gradient_penalty:\n                gp = gradient_penalty(img, img_discr_logits)\n                loss = discr_loss + gp\n            if return_recons:\n                return loss, fmap\n            return loss\n        # reconstruction loss\n        recon_loss = self.recon_loss_fn(fmap, img)\n        # early return if training on grayscale\n        if not self.use_vgg_and_gan:\n            if return_recons:\n                return recon_loss, fmap\n            return recon_loss\n        # perceptual loss\n        img_vgg_input = img\n        fmap_vgg_input = fmap\n        if img.shape[1] == 1:\n            # handle grayscale for vgg\n            img_vgg_input, fmap_vgg_input = map(lambda t: repeat(t, 'b 1 ... -> b c ...', c = 3), (img_vgg_input, fmap_vgg_input))\n        img_vgg_feats = self.vgg(img_vgg_input)"
        },
        {
            "comment": "This code calculates a combination of losses, including reconstruction, perceptual, and commitment. The adaptive weight is determined based on the gradients of these losses. A clamp function limits the adaptive weight to prevent extreme values. Finally, the combined loss is calculated and returned. If return_recons is True, fmap is also returned.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/dalle2_pytorch/vqgan_vae.py\":739-763",
            "content": "        recon_vgg_feats = self.vgg(fmap_vgg_input)\n        perceptual_loss = F.mse_loss(img_vgg_feats, recon_vgg_feats)\n        # generator loss\n        gen_loss = self.gen_loss(self.discr(fmap))\n        # calculate adaptive weight\n        last_dec_layer = self.enc_dec.last_dec_layer\n        norm_grad_wrt_gen_loss = grad_layer_wrt_loss(gen_loss, last_dec_layer).norm(p = 2)\n        norm_grad_wrt_perceptual_loss = grad_layer_wrt_loss(perceptual_loss, last_dec_layer).norm(p = 2)\n        adaptive_weight = safe_div(norm_grad_wrt_perceptual_loss, norm_grad_wrt_gen_loss)\n        adaptive_weight.clamp_(max = 1e4)\n        # combine losses\n        loss = recon_loss + perceptual_loss + commit_loss + adaptive_weight * gen_loss\n        if return_recons:\n            return loss, fmap\n        return loss"
        }
    ]
}