{
    "summary": "This code uses diffusion prior and CLIP to generate images from text prompts, implements pre-trained decoders, compares EMA models, checks image embeddings in DALLE2-pytorch, and discusses overfitting and running diffusion model training scripts.",
    "details": [
        {
            "comment": "This code introduces the concept of a diffusion prior, which is a trained model that allows translation between two embedding spaces. It motivates the use case of generating images from text using CLIP and a Decoder when embeddings are not guaranteed to be in the same space. The code loads CLIP and a pre-trained decoder, then retrieves a prompt from the user and encodes it with CLIP for further processing.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":0-20",
            "content": "# Diffusion Prior\nThis readme serves as an introduction to the diffusion prior.\n## Intro\nA properly trained prior will allow you to translate between two embedding spaces. If you know *a priori* that two embeddings are connected some way\u2014then ability the translate between them could extremely helpful.\n### Motivation\nBefore we dive into the model, let\u2019s look at a quick example of where the model may be helpful.\nFor demonstration purposes we will imagine that we wish to generate images from text using CLIP and a Decoder.\n> [CLIP](https://openai.com/blog/clip/) is a contrastive model that learns to maximize the cosine similarity between a given image and caption, however, there is no guarantee that these embeddings are in the same space. While the embeddings generated are ***close*** the image and text embeddings occupy two disjoint sets.\n```python\n# Load Models\nclip_model = clip.load(\"ViT-L/14\")\ndecoder = Decoder(checkpoint=\"best.pth\") # A decoder trained on CLIP Image embeddings\n# Retrieve prompt from user and encode with CLIP"
        },
        {
            "comment": "This code snippet demonstrates the process of generating an image from a text prompt using deep learning models. The decoder model is trained to convert text into embeddings that are in the same space as CLIP image embeddings. First, we load two models: Prior and Decoder. Then, we retrieve a user-inputted prompt, tokenize it, and use the Prior model to sample a text embedding in the same space as images. Finally, we pass this text embedding into the Decoder model to generate an image.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":21-46",
            "content": "prompt = \"A corgi wearing sunglasses\"\ntokenized_text = tokenize(prompt)\ntext_embedding = clip_model.encode_text(tokenized_text)\n# Now, pass the text embedding to the decoder\npredicted_image = decoder.sample(text_embedding)\n```\n> **Question**: *Can you spot the issue here?*\n>\n> **Answer**: *We\u2019re trying to generate an image from a text embedding!*\nUnfortunately, we run into the issue previously mentioned--the image embeddings and the text embeddings are not interchangeable! Now let's look at a better solution\n```python\n# Load Models\nprior= Prior(checkpoint=\"prior.pth\") # A decoder trained to go from: text-> clip text emb -> clip img emb\ndecoder = Decoder(checkpoint=\"decoder.pth\") # A decoder trained on CLIP Image embeddings\n# Retrieve prompt from user and encode with a prior\nprompt = \"A corgi wearing sunglasses\"\ntokenized_text = tokenize(prompt)\ntext_embedding = prior.sample(tokenized_text) # <-- now we get an embedding in the same space as images!\n# Now, pass the predicted image embedding to the decoder\npredicted_image = decoder.sample(text_embedding)"
        },
        {
            "comment": "The code demonstrates how to load a pre-trained prior model for use in generating embeddings within CLIP's image space, enhancing the performance of the decoder. The usage section outlines the necessary steps to load a checkpoint from a specific path using `load_diffusion_model()`.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":47-75",
            "content": "```\nWith the prior we are able to successfully generate embeddings *within* CLIP's image space! For this reason, the decoder will perform much better as it receives input that is much closer to its training data.\n> **You may be asking yourself the following question:**\n>\n> *\"Why don't you just train the decoder on clip text embeddings instead of image embeddings?\"*\n>\n> OpenAI covers this topic in their [DALLE-2 paper](https://arxiv.org/abs/2204.06125). The TL;DR is *\"it doesn't work as well as decoders trained on image embeddings\"*...also...its just an example :smile:\n## Usage\nTo utilize a pre-trained prior, it\u2019s quite simple.\n### Loading Checkpoints\n```python\nimport torch\nfrom dalle2_pytorch import DiffusionPrior, DiffusionPriorNetwork, OpenAIClipAdapter\nfrom dalle2_pytorch.trainer import DiffusionPriorTrainer\ndef load_diffusion_model(dprior_path):\n    prior_network = DiffusionPriorNetwork(\n        dim=768,\n        depth=24,\n        dim_head=64,\n        heads=32,\n        normformer=True,\n        attn_dropout=5e-2,"
        },
        {
            "comment": "Here, a pre-trained model is instantiated and its weights are loaded. This can be done just like any other PyTorch model. To generate embeddings from text, first tokenize the input text using `clip.tokenize()`.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":76-118",
            "content": "        ff_dropout=5e-2,\n        num_time_embeds=1,\n        num_image_embeds=1,\n        num_text_embeds=1,\n        num_timesteps=1000,\n        ff_mult=4\n    )\n    diffusion_prior = DiffusionPrior(\n        net=prior_network,\n        clip=OpenAIClipAdapter(\"ViT-L/14\"),\n        image_embed_dim=768,\n        timesteps=1000,\n        cond_drop_prob=0.1,\n        loss_type=\"l2\",\n        condition_on_text_encodings=True,\n    )\n    trainer = DiffusionPriorTrainer(\n        diffusion_prior=diffusion_prior,\n        lr=1.1e-4,\n        wd=6.02e-2,\n        max_grad_norm=0.5,\n        amp=False,\n        group_wd_params=True,\n        use_ema=True,\n        device=device,\n        accelerator=None,\n    )\n    trainer.load(dprior_path)\n    return trainer\n```\n Here we instantiate a model matches the configuration it was trained with, and then load the weights (*just like any other PyTorch model!*)\n### Sampling\nOnce we have a pre-trained model, generating embeddings is quite simple!\n```python\n# tokenize the text\ntokenized_text = clip.tokenize(\"<your amazing prompt>\")"
        },
        {
            "comment": "The code snippet is predicting an embedding using the prior's sample function, which returns a tensor of the same shape as the training data. The number of embeddings to sample can be specified and conditioning scale can be adjusted for better results. It serves as a replacement for clip.encode_text() in CLIP priors.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":119-129",
            "content": "# predict an embedding\npredicted_embedding = prior.sample(tokenized_text, n_samples_per_batch=2, cond_scale=1.0)\n```\nThe resulting tensor returned from `.sample()` is of the same shape as your training data along the non-batch dimension(s). For example, a prior trained on `ViT-L/14` embeddings will predict an embedding of shape (1, 768).\n> For CLIP priors, this is quite handy as it means that you can use prior.sample(tokenizer_text) as a drop in replacement for clip.encode_text().\n**Some things to note:**\n* It is possible to specify the number of embeddings to sample from (the default suggested by OpenAI is `n=2`). Put simply, the idea here is that you avoid getting unlucky with a bad embedding generation by creating two; and selecting the one with the higher cosine similarity with the prompt.\n* You may specify a higher conditioning scale than the default (`1.0`). It is unclear whether OpenAI uses a higher value for the prior specifically, or only on the decoder. Local testing has shown poor results with anything higher than `1.0` but *ymmv*."
        },
        {
            "comment": "Training the prior involves preparing a dataset in the format expected by EmbeddingReader. Precomputed embeddings for images significantly increase training efficiency and are beneficial for other tasks as well. To obtain precomputed embeddings, you can use img2dataset and clip_retrieval. The configuration file enables tracking and reproducing experiments.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":131-145",
            "content": "---\n## Training\n### Overview\nTraining the prior is a relatively straightforward process thanks to the Trainer base class. The major step that is required of you is preparing a dataset in the format that EmbeddingReader expects. Having pre-computed embeddings massively increases training efficiency and is generally recommended as you will likely benefit from having them on hand for other tasks as well. Once you have a dataset, you are ready to move onto configuration\n## Dataset\nTo train the prior, it is highly recommended to use precomputed embeddings for the images. To obtain these for a custom dataset, you can leverage [img2datset](https://github.com/rom1504/img2dataset) to pull images from a list of URLs and [clip_retrieval](https://github.com/rom1504/clip-retrieval#clip-inference) for generating the actual embeddings that can be used in the prior's dataloader.\n## Configuration\nThe configuration file allows for you to easily track and reproduce experiments. It is a simple JSON file that wil"
        },
        {
            "comment": "This code describes the architecture, dataset, and training parameters for a specific task. It also mentions distributed training using HuggingFace's Accelerate library and various evaluation metrics available during training.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":145-154",
            "content": "l specify the architecture, dataset, and training parameters. For more information and specifics please see the configuration README.\n## Distributed Training\nIf you would like to train in a distributed manner we have opted to leverage huggingface\u2019 new Accelerate library. HFA makes it extremely simple to distribute work across multiple GPU\u2019s and nodes. All that is required of you is to follow the simple CLI configuration tool [more information here](https://huggingface.co/docs/accelerate/accelerator).\n## Evaluation\nThere are a variety of metrics available to you when training the prior. You can read a brief description of each in the table below:\n| Metric                              | Description                                                                                                                                                                                                                                                  | Comments                                                "
        },
        {
            "comment": "This code is for calculating the validation loss associated with the online model validation process. The calculated validation loss will be used to evaluate the performance of the trained model during inference.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":154-156",
            "content": "                                                                                                                                                                                                                                                                                                |\n| ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Online Model Validation             | The validation loss associated "
        },
        {
            "comment": "This code is discussing the usage of an Exponential Moving Average (EMA) model in a machine learning context. The EMA model's performance is compared to the online model, specifically focusing on validation loss as a metric. The lower the validation loss, the better the model's performance, with values around 0.1 achievable after billions of samples. However, the EMA validation loss might lag behind but should outperform in the long term.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":156-157",
            "content": "with your online model.                                                                                                                                                                                                       | Ideally validation loss will be as low as possible. Using L2 loss, values as low as `0.1` and lower are possible after around 1 Billion samples seen.                                                                                                                                                                                                |\n| EMA Validation                      | This metric measures the validation loss associated with your EMA model.                                                                                                                                                                                     | This will likely lag behind your \"online\" model's validation loss, but should outperform in the long-term.                                 "
        },
        {
            "comment": "This code snippet is explaining the concept of baseline similarity in the context of DALLE2-pytorch, where it refers to the similarity between dataset prompts and image embeddings. It also mentions that generally, a cosine similarity value of 0.3 is considered good for caption similarity. Additionally, there's information about another metric - similarity with original image, which measures cosine similarity between the prior's predicted image embedding and the actual image.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":157-159",
            "content": "                                                                                                                                                                                                             |\n| Baseline Similarity                 | Baseline similarity refers to the similarity between your dataset's prompts and associated image embeddings. This will serve as a guide for your prior's performance in cosine similarity.                                                                    | Generally `0.3` is considered a good cosine similarity for caption similarity.                                                                                                                                                                                                                                                                         |\n| Similarity With Original Image      | This metric will measure the cosine similarity between your prior's predicted image embedding and the actual image"
        },
        {
            "comment": "The code provides information about the similarity metric between generated images and captions, as well as the difference from baseline similarity. The values should improve rapidly in early stages of training and plateau over time, while staying around 0 for the difference metric. Values above 0.5/0.6 or climbing to high values may indicate issues with training efficiency or overfitting, respectively.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":159-160",
            "content": " that the caption was associated with. This is useful for determining wether your prior is generating images with the right contents.      | Values around `0.75`+ are obtainable. This metric should improve rapidly in the early stages of training and plateau with diminishing increases over time. If it takes hundreds of millions of samples to reach above `0.5`/`0.6` similarity--then you likely are suffering from some kind of training error or inefficiency (i.e. not using EMA) |\n| Difference From Baseline Similarity | Sometimes its useful to visualize a metric in another light. This metric will show you how your prior's predicted image embeddings match up with the baseline similarity measured in your dataset.                                                           | This value should float around `0.0` with some room for variation. After a billion samples seen, values are within `0.01`+/- of `0.0`. If this climbs to high, (~>`0.02`) then this may be a sign that your model is overfitting "
        },
        {
            "comment": "The code measures the cosine similarity between predicted image embeddings and original captions, as well as with unrelated captions to detect overfitting. Monitoring these metrics is crucial for model performance, as they indicate how well the model is learning from captions and generating valid image embeddings.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":160-162",
            "content": "somehow.                                                                                                       |\n| Similarity With Text                | This metric is your bread and butter cosine similarity between the predicted image embedding and the original caption given to the prior. Monitoring this metric will be on of your main focuses and is probably the second most important behind your loss. | As mentioned, this value should be close to baseline similarity. We have observed early rapid increase with diminishing returns as the prior learns to generate valid image embeddings. If this value increases too far beyond the baseline similarity--it could be an indication that your model is overfitting.                                       |\n| Similarity With Unrelated Caption   | This metric will attempt to exposed an overfit prior by feeding it arbitrary prompts (from your dataset) and then measure the similarity of this predicted embedding with some other image.                     "
        },
        {
            "comment": "The code provides instructions on how to launch the training script for a diffusion model using either distributed training with HuggingFace Accelerate or without it. It also mentions that checkpoints will be saved in the directory specified in the configuration file, and an additional final checkpoint will be saved before running the test split. The prior value should ideally be kept low to avoid fooling CLIP into believing unrelated captions and images have high cosine similarity.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":162-174",
            "content": "                                              | Early on we found that a poorly trained/modeled prior could effectively fool CLIP into believing that the cosine similarity between two images were high (when in fact the caption and image were completely unrelated). With this in mind--a low value is ideal, anything below `0.1` is probably safe.                                              |\n## Launching the script\nNow that you\u2019ve done all the prep it\u2019s time for the easy part! \ud83d\ude80\nTo actually launch the script, you will either use `accelerate launch train_diffusion_prior.py --config_path <path to your config>` to launch with distributed training & huggingface accelerate or `python train_diffusion_prior.py` if you would like to train on your gpu/cpu without huggingface accelerate.\n## Checkpointing\nCheckpoints will be saved to the directory specified in your configuration file.\nAdditionally, a final checkpoint is saved before running the test split. This file will be saved to the same directory and"
        },
        {
            "comment": "This code snippet is providing information about the \"latest.pth\" file and its purpose to avoid potential problems with `save_every` configuration not overlapping with data requirements. It also mentions that the prior network has not been trained for tasks other than traditional CLIP embedding translation, hinting at future experiments applying the prior network to other tasks.",
            "location": "\"/media/root/Toshiba XG3/works/DALLE2-pytorch/docs/src/prior.md\":174-182",
            "content": " titled \u201clatest.pth\u201d. This is to avoid problems where your `save_every` configuration does not overlap with the number of steps required to do a complete pass through the data.\n## Things To Keep In Mind\nThe prior has not been trained for tasks other than the traditional CLIP embedding translation\u2026at least yet.\nAs we finalize the replication of unCLIP, there will almost assuredly be experiments attempting to apply the prior network to other tasks.\nWith that in mind, you are more or less a pioneer in embedding-translation if you are reading this and attempting something you don\u2019t see documentation for!"
        }
    ]
}